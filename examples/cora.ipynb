{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行前请安装dhg: `pip install git+https://github.com/iMoonLab/DeepHypergraph.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(199)\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import dhg\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN,BDLiner\n",
    "from hgp.loss import loss_bs_matrix,loss_bs_matrix_mega\n",
    "from hgp.utils import from_pickle_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 3\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1330, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.4}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.   \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for n in net:\n",
    "            self.layers.append(n.to(DEVICE))   \n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch): \n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    0,    0,  ..., 1328, 1329, 1329]),\n",
       " tensor([ 933,  936,  938,  ..., 1085, 1136, 1137])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hgp.utils import from_pickle_to_adj\n",
    "A = from_pickle_to_adj(\"../data/cora\",unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    1,  ..., 1328, 1329, 1329],\n",
       "                       [ 872,  875,  161,  ..., 1017, 1064, 1065]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(1330, 1413), nnz=4370, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = from_pickle_to_hypergraph(\"../data/cora\")\n",
    "edges, _ = G.e\n",
    "G.num_e, G.num_v\n",
    "G.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(n=G.num_v)\n",
    "embedding_net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "partition_net = BDLiner(hyper[\"l_hyper_prmts\"]).to(DEVICE)\n",
    "\n",
    "hgnn_trainer = Trainer(net=[embedding_net, partition_net], X=X, hg=G).to(DEVICE)\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=4e-5, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.layers.0.theta.weight Parameter containing:\n",
      "tensor([[-0.0138,  0.0145,  0.0074,  ..., -0.0130, -0.0147, -0.0027],\n",
      "        [-0.0272, -0.0073,  0.0124,  ...,  0.0089,  0.0073,  0.0174],\n",
      "        [ 0.0016, -0.0160,  0.0167,  ...,  0.0133, -0.0062,  0.0131],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0022,  0.0170,  ...,  0.0260,  0.0265, -0.0233],\n",
      "        [ 0.0164, -0.0179, -0.0143,  ..., -0.0068,  0.0012, -0.0168],\n",
      "        [ 0.0262, -0.0263, -0.0137,  ...,  0.0023,  0.0186, -0.0157]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.0.theta.bias Parameter containing:\n",
      "tensor([ 0.0102, -0.0036,  0.0125,  ...,  0.0084, -0.0239, -0.0192],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.1.theta.weight Parameter containing:\n",
      "tensor([[-0.0198,  0.0135,  0.0277,  ...,  0.0073, -0.0167,  0.0141],\n",
      "        [-0.0149, -0.0045, -0.0054,  ..., -0.0087,  0.0194,  0.0164],\n",
      "        [ 0.0073,  0.0309, -0.0059,  ...,  0.0087, -0.0113, -0.0213],\n",
      "        ...,\n",
      "        [ 0.0261,  0.0033, -0.0178,  ..., -0.0120, -0.0248,  0.0152],\n",
      "        [-0.0135, -0.0271,  0.0290,  ...,  0.0167, -0.0148,  0.0195],\n",
      "        [ 0.0016, -0.0305, -0.0183,  ...,  0.0205, -0.0302,  0.0185]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.1.theta.bias Parameter containing:\n",
      "tensor([ 3.0144e-02,  2.5472e-02,  3.9547e-03,  2.4255e-02,  6.9995e-03,\n",
      "        -2.8056e-02,  8.9784e-04,  1.7263e-02, -3.0921e-02,  9.4458e-03,\n",
      "         1.2438e-02, -1.8188e-02, -2.6288e-02, -3.6899e-03,  1.7819e-02,\n",
      "         2.8287e-02, -2.6387e-02, -2.1126e-02,  1.3032e-02, -1.1897e-02,\n",
      "         2.7796e-02, -1.6151e-02, -1.0019e-02,  3.3054e-03,  2.8615e-02,\n",
      "         1.1534e-02, -1.0909e-02, -2.3729e-02, -1.6518e-02, -1.4069e-02,\n",
      "        -9.4496e-03,  2.1331e-02, -2.1116e-02,  2.9809e-02, -1.0678e-02,\n",
      "         1.9646e-02,  2.7034e-02,  5.9142e-03,  1.8448e-02, -4.2334e-03,\n",
      "        -1.2766e-02, -7.8658e-03, -1.4644e-03, -1.0824e-02,  1.1367e-02,\n",
      "        -1.0141e-02, -9.8284e-03,  1.2208e-02,  7.2387e-03,  5.4564e-03,\n",
      "         1.8547e-02, -7.4214e-03, -1.8464e-02, -1.5257e-02,  1.9393e-02,\n",
      "        -3.3233e-04, -6.5923e-03,  1.3336e-02,  2.9596e-02, -2.5813e-02,\n",
      "         1.9202e-02, -1.3279e-02,  2.2957e-02,  3.0118e-03, -1.6712e-02,\n",
      "        -8.2700e-03,  5.5691e-03, -9.7878e-03,  1.4277e-02, -1.1282e-02,\n",
      "         2.3024e-02, -2.7540e-02, -2.9228e-02,  2.0960e-03, -1.9334e-03,\n",
      "         2.3984e-02,  2.3042e-02, -5.4260e-03, -1.5124e-02,  2.6629e-02,\n",
      "         3.0049e-02,  3.0180e-02,  2.6950e-02, -7.3040e-03, -2.4848e-02,\n",
      "        -8.5791e-03,  1.7815e-03,  1.3327e-03, -2.8984e-02, -1.6839e-02,\n",
      "         2.1712e-02, -2.9290e-02, -1.8230e-02, -2.1509e-02,  3.9848e-03,\n",
      "         2.2321e-03,  2.6801e-02,  2.9276e-02, -2.9020e-02,  2.7221e-02,\n",
      "         1.9695e-02, -2.7691e-02, -1.8070e-02,  3.0404e-02, -2.8385e-02,\n",
      "         6.6644e-04, -2.8534e-02,  2.7349e-03, -2.9439e-02,  2.7388e-02,\n",
      "        -2.7603e-03, -2.9446e-02, -1.3450e-02, -1.8203e-02,  2.7429e-02,\n",
      "        -2.6927e-02,  2.9756e-02, -6.1480e-03,  2.7828e-02, -2.0189e-02,\n",
      "        -2.0011e-02, -1.9766e-02, -2.4980e-02,  1.6216e-03,  2.8833e-02,\n",
      "        -9.0932e-03, -1.3311e-02, -2.5448e-02, -1.5642e-02,  1.2541e-02,\n",
      "        -1.0933e-04, -2.8392e-02,  1.7619e-02,  1.8320e-02,  5.7863e-04,\n",
      "        -7.3801e-03,  3.4173e-03,  1.9613e-02, -2.0942e-02, -1.9216e-02,\n",
      "         1.3427e-03, -2.3598e-03,  2.6785e-03,  2.9568e-02, -1.3182e-02,\n",
      "         1.3436e-02,  1.8367e-02,  2.8512e-02, -7.1665e-03, -9.8777e-03,\n",
      "         2.8441e-02, -2.8772e-02, -2.7070e-02, -1.5255e-02, -2.7290e-02,\n",
      "         1.1558e-02,  1.9636e-02,  9.9005e-03, -2.1550e-02,  9.2217e-03,\n",
      "        -6.7658e-03,  6.7472e-04,  2.9510e-02, -3.7422e-03, -1.1856e-02,\n",
      "        -1.4941e-02, -2.6807e-05,  2.4582e-02, -2.8936e-02,  2.3284e-02,\n",
      "        -2.7585e-02, -1.6530e-03,  2.1938e-02, -2.1876e-02, -2.1697e-02,\n",
      "         1.8588e-02, -1.0183e-02, -2.0256e-02,  3.1231e-02, -1.5874e-02,\n",
      "        -2.9745e-02,  3.8341e-03,  1.8197e-03, -1.1304e-02,  1.5221e-02,\n",
      "        -2.2926e-02, -7.9017e-03,  2.6801e-02, -5.5164e-03,  3.4171e-03,\n",
      "         9.7348e-04, -2.0201e-02,  7.9060e-03,  1.7167e-03,  2.7718e-02,\n",
      "         2.0427e-02, -5.2754e-03,  1.5468e-02,  2.8871e-02, -2.7809e-02,\n",
      "         1.3037e-02,  2.3384e-02, -2.8209e-02, -5.6732e-04,  7.2451e-03,\n",
      "        -2.0269e-02, -2.7834e-02,  4.3386e-03,  9.7287e-03,  1.5627e-02,\n",
      "        -7.2370e-03,  5.3130e-04, -2.2312e-02, -3.0443e-02, -2.9729e-02,\n",
      "         1.7196e-02,  1.8122e-02, -1.5846e-02, -1.4506e-02, -1.2222e-02,\n",
      "         2.2538e-02,  2.8333e-02,  1.1902e-02, -3.0372e-02,  8.2095e-03,\n",
      "         1.5457e-02,  1.1341e-02, -1.4239e-02, -9.6435e-03,  2.8566e-02,\n",
      "         8.5193e-03,  1.8611e-02, -2.0983e-02,  1.2515e-02, -2.0868e-02,\n",
      "        -1.1409e-02,  1.2045e-02, -2.6215e-02, -2.2638e-02, -2.5447e-02,\n",
      "         1.7413e-02,  2.8280e-03,  9.4281e-03,  5.2106e-05, -1.8558e-02,\n",
      "         1.0380e-02,  1.2091e-02,  1.0861e-02, -2.2262e-02,  1.9853e-02,\n",
      "         1.8426e-02,  1.2085e-02, -2.8187e-02, -8.8119e-04, -7.5395e-03,\n",
      "        -1.0010e-02, -1.4979e-02,  4.1015e-03,  2.4421e-02, -3.9641e-03,\n",
      "         2.7107e-02, -2.0413e-02, -2.9313e-02,  2.5713e-02,  4.0933e-03,\n",
      "         2.3393e-02,  8.5830e-03, -1.9628e-02,  2.0632e-02, -1.4317e-02,\n",
      "        -1.6940e-02,  1.3690e-02, -1.9390e-02,  3.0150e-02, -3.0446e-02,\n",
      "        -7.7433e-03, -1.5302e-02, -3.0354e-02,  1.3698e-03,  4.1109e-03,\n",
      "         1.5328e-02, -1.3183e-03,  6.1611e-03, -2.6045e-02,  3.0665e-02,\n",
      "        -2.5657e-02, -7.5683e-03,  2.9460e-02,  2.0546e-02, -3.0943e-02,\n",
      "        -3.0383e-02, -1.3460e-02, -1.0187e-02, -2.2052e-02, -1.9712e-02,\n",
      "        -1.1576e-02, -1.4911e-02,  1.2375e-02,  2.9254e-02, -2.6464e-02,\n",
      "         1.3885e-02, -2.6756e-02, -7.7521e-03, -9.5896e-04,  4.6535e-03,\n",
      "         7.9847e-03,  6.8275e-03, -4.7354e-03,  1.4286e-02,  3.1725e-03,\n",
      "         2.6926e-02, -3.0424e-02, -8.4755e-03,  3.0193e-02, -2.5837e-02,\n",
      "        -1.9497e-02,  2.9351e-02,  7.8395e-03, -5.3634e-03,  2.1593e-02,\n",
      "        -5.4629e-03, -1.8300e-02,  5.9533e-03,  2.6423e-02, -2.0959e-02,\n",
      "        -3.5928e-03,  1.1550e-02, -2.0309e-02,  5.2529e-03, -4.1929e-03,\n",
      "        -5.8920e-03,  1.3459e-02, -1.2965e-03, -5.0408e-03,  2.8869e-04,\n",
      "        -6.2962e-04, -5.3012e-03,  1.6728e-03, -2.5999e-02, -4.5416e-03,\n",
      "        -2.6279e-02,  1.4561e-02, -1.1244e-02,  2.8039e-02,  1.8418e-02,\n",
      "         2.8810e-02, -1.9706e-02, -5.3972e-03,  2.3908e-02,  1.1666e-02,\n",
      "         2.7649e-02, -1.6201e-02, -3.0594e-02,  2.9500e-02,  1.4688e-02,\n",
      "        -9.1082e-03,  2.7208e-02, -1.2812e-02, -3.0859e-02, -1.1892e-02,\n",
      "        -3.0563e-02, -2.3220e-02, -2.8933e-02,  5.0331e-03,  1.8893e-02,\n",
      "         5.7105e-03, -2.0167e-03,  2.1976e-02, -8.3039e-03,  3.6463e-05,\n",
      "        -1.5993e-02,  1.6929e-02,  1.1264e-02, -1.7202e-03, -6.4802e-03,\n",
      "         2.6494e-02,  1.2163e-02, -3.6355e-03,  2.2487e-02,  2.8389e-02,\n",
      "        -2.4334e-02, -2.9494e-03, -9.4982e-03, -1.4321e-02,  2.1226e-02,\n",
      "         1.5933e-02,  1.9485e-02,  4.5150e-03,  2.1589e-02,  5.0181e-03,\n",
      "         4.7836e-03,  1.1098e-02,  1.3330e-02,  1.3941e-02,  2.4152e-02,\n",
      "        -5.2558e-03,  1.5534e-02, -1.1982e-03, -5.4089e-03,  2.9241e-02,\n",
      "         1.2294e-02,  2.4815e-02,  6.4206e-03, -5.7639e-03, -7.7333e-03,\n",
      "         3.0174e-03, -1.5297e-02,  2.9506e-02,  2.5987e-02, -6.7634e-03,\n",
      "        -5.2968e-03, -3.0285e-02,  2.6744e-02, -3.7987e-04,  2.2307e-03,\n",
      "        -1.3937e-02,  6.4209e-03,  5.3642e-03,  1.7329e-02,  2.4296e-02,\n",
      "        -2.9345e-02, -4.8889e-03, -1.9174e-02, -2.7369e-02,  7.1409e-03,\n",
      "        -1.1514e-02,  2.9337e-02, -1.2703e-02,  9.1016e-03,  5.8289e-03,\n",
      "        -2.5303e-02,  1.7701e-02,  8.2731e-04, -1.0951e-02,  3.0024e-02,\n",
      "         1.7966e-02, -6.9331e-03, -5.1322e-03,  1.0927e-02, -1.4644e-02,\n",
      "        -1.1524e-02,  1.0476e-02,  6.7355e-03,  9.2822e-03,  2.3241e-02,\n",
      "         1.1364e-02,  1.8162e-02, -2.7647e-02,  1.3707e-02,  2.9572e-02,\n",
      "         2.8672e-02, -2.4747e-02, -8.3674e-03,  2.2355e-02,  2.1684e-02,\n",
      "        -2.1079e-02, -2.6974e-02,  2.8118e-02,  3.4354e-03,  1.2873e-02,\n",
      "        -8.5886e-03, -1.2665e-02,  8.4356e-03,  1.1411e-02,  1.7562e-02,\n",
      "         1.1265e-03, -2.7933e-02,  2.8285e-02, -1.7982e-02, -1.4028e-02,\n",
      "         6.0838e-03, -2.4233e-02,  3.3074e-03,  2.0837e-02,  2.6272e-02,\n",
      "        -2.0779e-02, -1.9640e-02, -1.9596e-02, -7.1184e-03, -6.2082e-03,\n",
      "        -1.0598e-02,  4.2644e-03,  2.9702e-03, -9.2594e-03,  6.2399e-03,\n",
      "         5.8890e-03,  1.4606e-02, -2.1811e-02, -6.5749e-03,  8.2795e-03,\n",
      "        -2.6648e-02, -1.8230e-02,  2.3298e-02,  3.0217e-03,  1.0811e-03,\n",
      "        -1.1368e-02,  1.2227e-02, -1.5801e-02,  2.9401e-02, -3.0539e-02,\n",
      "        -9.1755e-03,  1.6613e-02,  6.8491e-03,  1.4698e-02, -4.0610e-03,\n",
      "        -7.2709e-03, -2.9168e-02, -2.1470e-02,  4.4267e-03,  1.0761e-02,\n",
      "         1.7452e-02, -1.7424e-02], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.2.theta.weight Parameter containing:\n",
      "tensor([[ 0.0092,  0.0057,  0.0231,  ...,  0.0023,  0.0146,  0.0267],\n",
      "        [-0.0214,  0.0438,  0.0030,  ...,  0.0436,  0.0441, -0.0314],\n",
      "        [-0.0433,  0.0027, -0.0212,  ..., -0.0421, -0.0397,  0.0376],\n",
      "        ...,\n",
      "        [ 0.0272, -0.0140,  0.0181,  ..., -0.0056,  0.0226,  0.0045],\n",
      "        [ 0.0151, -0.0205,  0.0348,  ..., -0.0397, -0.0225,  0.0275],\n",
      "        [ 0.0228,  0.0380, -0.0212,  ...,  0.0388,  0.0297,  0.0173]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.2.theta.bias Parameter containing:\n",
      "tensor([-0.0140, -0.0023,  0.0161, -0.0347,  0.0415, -0.0054,  0.0345, -0.0248,\n",
      "         0.0381,  0.0207,  0.0215,  0.0232, -0.0067,  0.0210, -0.0231, -0.0253,\n",
      "        -0.0400, -0.0097, -0.0070, -0.0155,  0.0171,  0.0071,  0.0211,  0.0043,\n",
      "        -0.0129, -0.0387, -0.0062,  0.0184,  0.0384, -0.0101, -0.0255, -0.0153,\n",
      "         0.0023, -0.0421, -0.0041,  0.0244,  0.0065,  0.0246, -0.0072,  0.0237,\n",
      "        -0.0190, -0.0181, -0.0034, -0.0405, -0.0288,  0.0393, -0.0294,  0.0404,\n",
      "         0.0002,  0.0340,  0.0073, -0.0238, -0.0042, -0.0094,  0.0045,  0.0123,\n",
      "         0.0090,  0.0163, -0.0145,  0.0101,  0.0440, -0.0396, -0.0281,  0.0181,\n",
      "         0.0218,  0.0226,  0.0348, -0.0295, -0.0198, -0.0243, -0.0107,  0.0352,\n",
      "        -0.0162, -0.0193,  0.0308,  0.0334, -0.0427, -0.0442, -0.0376,  0.0144,\n",
      "        -0.0075, -0.0339, -0.0116, -0.0400, -0.0232, -0.0179, -0.0181,  0.0402,\n",
      "        -0.0007,  0.0082,  0.0338,  0.0154, -0.0102, -0.0298,  0.0090, -0.0109,\n",
      "         0.0329, -0.0021, -0.0169,  0.0346,  0.0355,  0.0225, -0.0148,  0.0198,\n",
      "        -0.0057, -0.0105,  0.0373, -0.0046,  0.0417,  0.0309, -0.0423, -0.0252,\n",
      "        -0.0186, -0.0227,  0.0353,  0.0114, -0.0328,  0.0295,  0.0287, -0.0352,\n",
      "         0.0253,  0.0117, -0.0104, -0.0144, -0.0361, -0.0319, -0.0386, -0.0224,\n",
      "         0.0370,  0.0115,  0.0292, -0.0103,  0.0157,  0.0343, -0.0340,  0.0043,\n",
      "        -0.0225, -0.0056,  0.0327, -0.0398,  0.0245, -0.0074, -0.0142,  0.0360,\n",
      "         0.0122,  0.0170, -0.0188, -0.0204,  0.0230,  0.0421, -0.0327, -0.0192,\n",
      "        -0.0038, -0.0095, -0.0294, -0.0238, -0.0176, -0.0233, -0.0409, -0.0029,\n",
      "         0.0358, -0.0106, -0.0005,  0.0266,  0.0213, -0.0339,  0.0152, -0.0181,\n",
      "        -0.0105, -0.0039, -0.0028,  0.0020, -0.0374, -0.0183, -0.0127, -0.0361,\n",
      "         0.0013, -0.0409, -0.0429, -0.0051, -0.0375,  0.0082,  0.0239, -0.0111,\n",
      "        -0.0178,  0.0075,  0.0104, -0.0213, -0.0236, -0.0023,  0.0079,  0.0081,\n",
      "         0.0411, -0.0068, -0.0315, -0.0228,  0.0100, -0.0247, -0.0041,  0.0160,\n",
      "        -0.0229, -0.0174, -0.0100,  0.0312,  0.0391, -0.0248,  0.0188,  0.0157,\n",
      "        -0.0382,  0.0439, -0.0325,  0.0185,  0.0012, -0.0419, -0.0339, -0.0107,\n",
      "         0.0217,  0.0281, -0.0377,  0.0434,  0.0074,  0.0365, -0.0309,  0.0374,\n",
      "        -0.0316,  0.0346,  0.0019, -0.0080, -0.0226, -0.0248,  0.0089, -0.0089,\n",
      "        -0.0262, -0.0248,  0.0180, -0.0068, -0.0362, -0.0115,  0.0170, -0.0348,\n",
      "        -0.0432, -0.0322,  0.0015, -0.0060,  0.0261,  0.0187, -0.0293, -0.0330,\n",
      "         0.0052,  0.0188,  0.0395, -0.0411,  0.0400, -0.0201,  0.0026,  0.0058,\n",
      "        -0.0240,  0.0032,  0.0159, -0.0266,  0.0023,  0.0061, -0.0180,  0.0269,\n",
      "         0.0125, -0.0404,  0.0101,  0.0210, -0.0245, -0.0116,  0.0124, -0.0397,\n",
      "         0.0017, -0.0075,  0.0097,  0.0085, -0.0226, -0.0384, -0.0337, -0.0442,\n",
      "        -0.0167,  0.0160,  0.0276, -0.0087, -0.0066,  0.0275, -0.0368,  0.0211,\n",
      "         0.0264, -0.0224,  0.0157, -0.0052, -0.0035,  0.0364,  0.0222, -0.0012,\n",
      "        -0.0113, -0.0025, -0.0077,  0.0381,  0.0403,  0.0272,  0.0203, -0.0383,\n",
      "        -0.0083,  0.0415,  0.0372,  0.0349,  0.0297, -0.0386, -0.0437,  0.0171,\n",
      "        -0.0268, -0.0012,  0.0199, -0.0195,  0.0145,  0.0053,  0.0029, -0.0081,\n",
      "        -0.0037, -0.0301, -0.0248, -0.0440, -0.0087, -0.0245,  0.0141,  0.0306,\n",
      "         0.0118, -0.0142,  0.0426,  0.0152, -0.0389,  0.0074, -0.0240,  0.0168,\n",
      "        -0.0353, -0.0003,  0.0099,  0.0372,  0.0393, -0.0112,  0.0189,  0.0026,\n",
      "        -0.0114,  0.0109, -0.0276,  0.0067,  0.0101, -0.0055,  0.0307,  0.0388,\n",
      "         0.0429, -0.0399, -0.0255, -0.0063, -0.0072, -0.0191, -0.0372, -0.0045,\n",
      "        -0.0379,  0.0056, -0.0253, -0.0066,  0.0350, -0.0436, -0.0352, -0.0258,\n",
      "         0.0085,  0.0076,  0.0146, -0.0319,  0.0118, -0.0116,  0.0125, -0.0015,\n",
      "         0.0313,  0.0183,  0.0105, -0.0022,  0.0198, -0.0410,  0.0140,  0.0195,\n",
      "        -0.0414, -0.0357, -0.0335, -0.0272, -0.0360, -0.0327, -0.0358,  0.0179,\n",
      "        -0.0434, -0.0032,  0.0296, -0.0118, -0.0255,  0.0153, -0.0064,  0.0363,\n",
      "         0.0080, -0.0297,  0.0292, -0.0338,  0.0249,  0.0219,  0.0374, -0.0147,\n",
      "         0.0180, -0.0369,  0.0055, -0.0231,  0.0062, -0.0151,  0.0101,  0.0296,\n",
      "         0.0440,  0.0419, -0.0411,  0.0335, -0.0126, -0.0130, -0.0187,  0.0134,\n",
      "        -0.0003,  0.0021,  0.0436,  0.0315, -0.0241, -0.0269, -0.0236, -0.0006,\n",
      "         0.0264, -0.0269, -0.0429, -0.0100,  0.0009, -0.0442, -0.0428, -0.0173,\n",
      "         0.0236, -0.0395, -0.0247,  0.0282,  0.0103, -0.0064,  0.0090, -0.0368,\n",
      "         0.0045,  0.0232,  0.0267,  0.0392, -0.0045,  0.0228,  0.0149, -0.0412,\n",
      "        -0.0030, -0.0106,  0.0284, -0.0026,  0.0337, -0.0415, -0.0116, -0.0382,\n",
      "        -0.0229, -0.0330, -0.0174,  0.0399, -0.0376,  0.0347, -0.0393, -0.0279,\n",
      "         0.0298,  0.0110,  0.0121, -0.0317, -0.0151,  0.0189, -0.0431,  0.0101,\n",
      "        -0.0136,  0.0129,  0.0149,  0.0087,  0.0124, -0.0055,  0.0026, -0.0399,\n",
      "         0.0407,  0.0109,  0.0385,  0.0057,  0.0132, -0.0357, -0.0123, -0.0104,\n",
      "         0.0396, -0.0153,  0.0404,  0.0036,  0.0054,  0.0323,  0.0343, -0.0097,\n",
      "        -0.0138, -0.0175,  0.0097, -0.0323, -0.0424, -0.0330,  0.0253,  0.0331],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.3.theta.weight Parameter containing:\n",
      "tensor([[-3.0715e-02, -4.2792e-02,  3.8794e-02,  ...,  4.3059e-03,\n",
      "         -2.0282e-02,  1.9473e-02],\n",
      "        [-2.5570e-02,  5.5576e-05, -3.6666e-02,  ...,  2.0422e-02,\n",
      "         -2.2053e-02, -3.7959e-02],\n",
      "        [-4.2960e-02,  3.5577e-02,  1.5661e-02,  ..., -7.6194e-03,\n",
      "          2.3080e-02,  2.9961e-02],\n",
      "        ...,\n",
      "        [ 3.0538e-02,  3.4116e-02, -1.6358e-02,  ..., -3.3448e-02,\n",
      "         -2.1987e-02,  1.8606e-03],\n",
      "        [-7.5878e-03, -3.6941e-02,  2.1673e-02,  ..., -3.8048e-02,\n",
      "         -3.4295e-02,  3.5593e-02],\n",
      "        [ 3.3692e-02, -1.6499e-02,  4.1730e-02,  ...,  3.5988e-03,\n",
      "          5.5488e-03, -3.5788e-03]], device='cuda:0', requires_grad=True)\n",
      "layers.0.layers.3.theta.bias Parameter containing:\n",
      "tensor([-2.6747e-02,  2.6964e-03,  4.5773e-03, -7.0073e-03, -7.0667e-04,\n",
      "         9.7812e-03,  4.3161e-02, -7.3315e-03,  2.8274e-02, -2.9053e-03,\n",
      "         2.1797e-02, -3.7546e-02,  2.8724e-02,  8.8816e-03, -4.1800e-03,\n",
      "         2.8298e-02,  3.9206e-03,  3.4954e-02,  4.1274e-02,  1.2755e-02,\n",
      "        -4.0937e-04,  7.2523e-03,  2.0082e-02,  1.9578e-02,  2.0927e-02,\n",
      "        -2.8910e-02,  1.6074e-03,  3.1602e-02,  1.3762e-02,  2.4108e-02,\n",
      "         2.9966e-02, -2.7843e-02, -4.0105e-02,  1.9323e-02,  8.6797e-03,\n",
      "        -3.0461e-02,  2.0730e-04,  1.9645e-02,  1.2098e-02,  3.8491e-03,\n",
      "         1.7748e-02,  3.6420e-03, -3.3492e-02, -3.9372e-03, -1.6326e-02,\n",
      "        -2.9340e-02,  3.4858e-02, -2.5319e-02,  5.1868e-03,  1.8394e-02,\n",
      "         1.7273e-02, -3.2692e-02,  1.9569e-02, -1.8916e-02,  8.7000e-03,\n",
      "        -1.7207e-02, -4.3446e-02, -3.0850e-02,  4.1464e-02,  2.2935e-02,\n",
      "        -2.2939e-02,  2.9183e-02, -1.2922e-03,  1.4133e-02, -1.4006e-02,\n",
      "        -7.3499e-03,  2.7760e-02,  1.3984e-02, -3.6203e-02,  1.6710e-02,\n",
      "        -3.8869e-02,  3.4696e-02,  1.9633e-04,  1.9193e-02,  2.1071e-02,\n",
      "        -4.2913e-02,  2.1998e-02, -3.4485e-02,  4.0716e-03,  1.3873e-02,\n",
      "         2.1101e-02,  9.1581e-03, -1.0523e-02, -5.5428e-04, -7.1957e-03,\n",
      "         1.2964e-02,  3.4368e-02, -1.0555e-03,  3.2085e-02, -2.5519e-02,\n",
      "         3.4310e-02,  1.7148e-02,  3.8106e-02,  4.3947e-02,  2.2286e-02,\n",
      "         2.3681e-02, -5.8751e-03, -3.0385e-04,  2.2102e-02, -3.6657e-02,\n",
      "         2.3184e-03,  3.7512e-02, -3.0207e-02, -2.6930e-03, -1.3142e-02,\n",
      "        -7.8095e-03,  1.1119e-02, -2.6202e-02, -1.7447e-02,  3.6083e-02,\n",
      "         3.6610e-03, -2.7698e-02,  3.6783e-02,  1.6802e-02, -4.1344e-02,\n",
      "        -6.5922e-03,  5.1430e-03,  9.4115e-03, -2.0542e-02, -2.0370e-03,\n",
      "         6.8013e-03, -2.3552e-02,  1.6022e-02, -1.7977e-02, -1.3303e-02,\n",
      "        -3.0740e-02,  6.3201e-04,  1.4965e-02,  3.2776e-02,  1.7234e-03,\n",
      "        -2.8154e-02, -2.6108e-02,  2.0996e-03,  2.1312e-02,  3.5583e-02,\n",
      "        -2.8106e-02,  2.4418e-02,  1.0701e-02, -3.5776e-02, -2.9826e-02,\n",
      "         3.8713e-02,  1.9647e-02,  2.4894e-02,  3.6747e-02, -4.0347e-02,\n",
      "        -3.0449e-03,  5.5985e-03,  2.6355e-02, -1.5210e-02,  4.2198e-02,\n",
      "         3.2168e-02, -3.6577e-02, -4.1672e-02, -3.6684e-04, -4.1960e-02,\n",
      "         2.9907e-02,  2.5786e-04,  4.2767e-02,  4.2758e-02,  3.8249e-02,\n",
      "         1.5138e-02, -3.1288e-02, -2.1457e-02, -3.1059e-02, -2.8167e-02,\n",
      "        -4.2748e-02, -3.9483e-02, -2.9538e-03,  3.6798e-02,  7.8537e-03,\n",
      "         3.4943e-02, -5.4674e-03, -1.0012e-02,  5.5930e-03, -1.7515e-02,\n",
      "         6.0277e-03,  1.6115e-02,  1.8144e-02,  1.6541e-02, -4.3456e-02,\n",
      "        -1.1030e-02, -3.6274e-02, -2.5277e-02,  2.5616e-02,  3.9424e-02,\n",
      "         1.8279e-02,  5.6063e-03,  2.3962e-02,  1.1901e-02, -2.0753e-02,\n",
      "        -2.0692e-02,  2.2475e-02,  6.0965e-03, -2.0020e-05, -5.2196e-03,\n",
      "         2.9141e-02,  9.8861e-03, -3.6258e-02, -7.7181e-03, -1.6645e-03,\n",
      "        -5.0749e-03, -1.9044e-02, -1.5225e-02,  5.1350e-03, -1.4605e-02,\n",
      "         4.0024e-02, -3.1029e-02,  1.2760e-02, -1.6419e-02, -9.4773e-03,\n",
      "        -3.4204e-02,  3.9840e-02,  1.5981e-02,  3.6849e-02,  2.0405e-02,\n",
      "         3.8890e-02,  4.2138e-02,  3.2054e-02,  2.6847e-02,  5.3015e-04,\n",
      "         3.3897e-02, -7.0153e-03, -4.1511e-03,  2.6144e-02, -2.7846e-02,\n",
      "        -1.0391e-02,  4.4730e-03, -9.1306e-03, -3.8278e-02, -8.7345e-03,\n",
      "        -2.8707e-02,  4.0150e-02, -1.6884e-02, -2.1438e-02,  1.2321e-02,\n",
      "         1.6349e-02,  3.9072e-02, -2.9388e-02, -9.3475e-03, -2.5605e-02,\n",
      "         2.3905e-02, -2.0101e-02, -5.0640e-03,  4.9721e-03,  3.3964e-02,\n",
      "        -3.3233e-02,  1.0525e-02, -4.3588e-02, -3.3237e-02,  2.8572e-04,\n",
      "        -3.5142e-02,  6.8324e-03,  2.0993e-02, -9.8251e-03,  2.9658e-02,\n",
      "         1.0048e-02], device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.0.weight Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.0.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.3.weight Parameter containing:\n",
      "tensor([[-0.0371, -0.0414, -0.0099,  ..., -0.0100,  0.0041,  0.0166],\n",
      "        [ 0.0548,  0.0198,  0.0583,  ...,  0.0258,  0.0143, -0.0514],\n",
      "        [-0.0531, -0.0290, -0.0607,  ...,  0.0324,  0.0555,  0.0516],\n",
      "        ...,\n",
      "        [ 0.0359, -0.0275,  0.0424,  ..., -0.0368,  0.0419,  0.0619],\n",
      "        [ 0.0536,  0.0521,  0.0281,  ...,  0.0189, -0.0121, -0.0278],\n",
      "        [-0.0034,  0.0256,  0.0558,  ...,  0.0424,  0.0341, -0.0483]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.3.bias Parameter containing:\n",
      "tensor([ 5.9551e-03, -5.1510e-02,  3.4988e-02,  5.3554e-02,  3.9303e-02,\n",
      "         5.2144e-02,  5.1707e-02, -1.9370e-02, -7.6394e-03, -7.5129e-03,\n",
      "        -3.2074e-02,  3.3595e-02,  3.0695e-03, -1.6722e-03, -5.5761e-03,\n",
      "         6.0788e-03,  1.1857e-02, -1.5788e-02,  1.1769e-02, -9.5591e-03,\n",
      "         4.3814e-02,  7.2271e-03, -7.8071e-03,  4.6138e-02,  8.8321e-03,\n",
      "        -2.4170e-02, -1.0279e-02, -2.8212e-02,  4.4326e-02, -4.8411e-02,\n",
      "         4.1911e-02,  4.6113e-02,  2.8681e-02, -5.2377e-02,  2.0294e-02,\n",
      "         2.2167e-02,  3.9407e-02, -5.2103e-02, -1.7908e-02, -5.7586e-02,\n",
      "         2.9867e-02,  4.8771e-02,  2.1397e-02,  4.8281e-02,  2.1563e-02,\n",
      "        -3.4560e-02, -4.7688e-02,  1.4210e-02,  5.8909e-02,  4.8762e-02,\n",
      "         1.1314e-02, -1.5474e-02, -5.1513e-02, -9.9087e-03,  2.0488e-02,\n",
      "        -5.5406e-02,  5.9189e-02,  3.9940e-02, -5.1184e-02, -2.7243e-02,\n",
      "        -3.4436e-02,  3.7612e-02, -2.3003e-02, -2.7905e-02, -1.9929e-02,\n",
      "        -5.0059e-02,  2.5638e-02,  4.8656e-02, -5.5845e-02,  1.5329e-02,\n",
      "        -1.1492e-02,  7.4264e-03, -1.3894e-02,  5.0614e-02, -1.1796e-02,\n",
      "         3.3053e-02, -1.7099e-02,  1.6599e-02,  4.4295e-02, -5.9972e-02,\n",
      "         1.9981e-02, -4.2782e-03,  5.2239e-02,  2.7778e-03, -1.6577e-02,\n",
      "        -4.1060e-03, -6.1944e-02,  3.4669e-02, -2.9408e-02,  3.8974e-02,\n",
      "         1.5431e-02, -5.7812e-02, -6.1082e-02,  2.3131e-02,  5.0802e-02,\n",
      "        -2.6837e-02, -6.1373e-02, -3.1864e-02,  2.5327e-02,  2.7308e-02,\n",
      "        -8.6821e-03, -1.9561e-02, -5.0962e-02, -1.1538e-02, -2.2537e-02,\n",
      "         2.5321e-02, -5.0901e-02,  4.6444e-02,  3.5956e-02, -5.7053e-02,\n",
      "         2.8088e-02, -6.1552e-02, -4.1555e-02,  5.8460e-02, -4.9788e-02,\n",
      "        -2.6316e-02, -8.3102e-03, -9.0547e-05,  1.5752e-02, -1.3219e-02,\n",
      "         1.0882e-02,  4.1369e-02,  5.3053e-02,  1.5300e-02, -3.7336e-02,\n",
      "        -1.8614e-02,  4.4903e-02,  5.2244e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layers.1.layers.4.weight Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.4.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.7.weight Parameter containing:\n",
      "tensor([[ 0.0064,  0.0857, -0.0841,  ..., -0.0330, -0.0176, -0.0615],\n",
      "        [-0.0089,  0.0406,  0.0067,  ..., -0.0049,  0.0020, -0.0600],\n",
      "        [-0.0445,  0.0004, -0.0429,  ..., -0.0693,  0.0367, -0.0204],\n",
      "        ...,\n",
      "        [ 0.0478, -0.0429, -0.0428,  ...,  0.0315,  0.0444, -0.0242],\n",
      "        [ 0.0538, -0.0734, -0.0761,  ..., -0.0012, -0.0009,  0.0506],\n",
      "        [-0.0652,  0.0349,  0.0857,  ..., -0.0291,  0.0256,  0.0094]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.7.bias Parameter containing:\n",
      "tensor([ 0.0440,  0.0396,  0.0763, -0.0301, -0.0036, -0.0002, -0.0717,  0.0448,\n",
      "        -0.0085,  0.0080,  0.0271,  0.0067,  0.0167, -0.0796,  0.0036,  0.0314,\n",
      "        -0.0043, -0.0220,  0.0136,  0.0513,  0.0810, -0.0346, -0.0275, -0.0228,\n",
      "        -0.0707, -0.0153,  0.0837, -0.0023, -0.0472,  0.0846,  0.0613,  0.0830,\n",
      "         0.0344, -0.0818, -0.0140,  0.0638,  0.0248, -0.0842,  0.0300,  0.0167,\n",
      "        -0.0022,  0.0647, -0.0345, -0.0820,  0.0262, -0.0298,  0.0408, -0.0347,\n",
      "        -0.0609, -0.0619,  0.0666, -0.0293,  0.0467,  0.0134, -0.0608,  0.0763,\n",
      "        -0.0632, -0.0243, -0.0613,  0.0333, -0.0811, -0.0370, -0.0197, -0.0457],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.10.weight Parameter containing:\n",
      "tensor([[ 0.0493,  0.0228, -0.1011,  ..., -0.1131, -0.0775,  0.0057],\n",
      "        [-0.0679,  0.1030,  0.0408,  ...,  0.0810,  0.0868, -0.1099],\n",
      "        [ 0.0541, -0.0055, -0.1131,  ..., -0.0896,  0.0820, -0.0011],\n",
      "        ...,\n",
      "        [-0.0125, -0.1193,  0.0640,  ...,  0.0499,  0.1191, -0.0475],\n",
      "        [ 0.0382, -0.0293, -0.0659,  ..., -0.0275, -0.0884,  0.0753],\n",
      "        [-0.0967, -0.0580,  0.0248,  ...,  0.0791, -0.0921, -0.0957]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.10.bias Parameter containing:\n",
      "tensor([-0.1218,  0.0712,  0.0027, -0.0093, -0.0929, -0.1230,  0.0350,  0.0644,\n",
      "         0.0409, -0.0723,  0.0979, -0.0186, -0.0408,  0.0160, -0.0780,  0.1079,\n",
      "        -0.0058, -0.0657,  0.0507, -0.0670, -0.0568, -0.0381, -0.0651,  0.0984,\n",
      "        -0.1106,  0.1166,  0.0159, -0.0596,  0.0935, -0.0620, -0.0713,  0.0632],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.13.weight Parameter containing:\n",
      "tensor([[-0.1577, -0.1537,  0.0373, -0.0005, -0.0017,  0.0644,  0.0315,  0.0141,\n",
      "         -0.0473,  0.0133, -0.0433, -0.0541,  0.0762,  0.0199, -0.0075, -0.1228,\n",
      "          0.1663, -0.0560,  0.0964,  0.1695, -0.1021,  0.0839,  0.0177, -0.0677,\n",
      "          0.0794,  0.0635,  0.1457, -0.0398, -0.0436, -0.1712,  0.0465, -0.1282],\n",
      "        [ 0.0206,  0.1013, -0.0268, -0.0892, -0.0179,  0.0598, -0.1725,  0.0641,\n",
      "         -0.1106, -0.0173,  0.0585, -0.1336, -0.0303, -0.1283, -0.1502, -0.1754,\n",
      "          0.1649,  0.1239,  0.0880, -0.0486,  0.0681,  0.1088, -0.0148, -0.1084,\n",
      "         -0.0589,  0.1576,  0.1515, -0.0669, -0.0738, -0.0097,  0.1757,  0.0314],\n",
      "        [ 0.1438,  0.1365, -0.1719,  0.0730, -0.1063,  0.0013, -0.1213,  0.1756,\n",
      "          0.1273, -0.1607, -0.1145,  0.0257,  0.1170, -0.0890,  0.0377,  0.1008,\n",
      "         -0.1395, -0.0994, -0.1577,  0.1279, -0.0030,  0.0656,  0.0340, -0.1062,\n",
      "         -0.1431,  0.0607,  0.1381, -0.0846, -0.0140,  0.0673, -0.0682, -0.1322]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.layers.13.bias Parameter containing:\n",
      "tensor([-0.1391,  0.1249,  0.0442], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# for name,p in hgnn_trainer.named_parameters():\n",
    "#     print(name,p.shape)\n",
    "# hgnn_trainer.layers\n",
    "for n,p in hgnn_trainer.named_parameters():\n",
    "    print(n,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "# for epoch in range(4500):\n",
    "#     loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "#     # train\n",
    "#     temp_loss_total += loss\n",
    "#     temp_loss1 += loss_1\n",
    "#     temp_loss2 += loss_2\n",
    "#     # validation\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "#         print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "#         print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "#         print(f\"=================================\")\n",
    "#         sys.stdout.flush()\n",
    "#         temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m hgnn_trainer\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4500\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss_1 \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m     17\u001b[0m         best_loss \u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 23\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     25\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition/examples/../hgp/models.py:175\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 175\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1467\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(P, X)\n\u001b[0;32m-> 1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_v_neg_1\u001b[49m, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1469\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(P, X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:900\u001b[0m, in \u001b[0;36mHypergraph.D_v_neg_1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the vertex degree matrix :math:`\\mathbf{D}_v^{-1}` with ``torch.sparse_coo_tensor`` format.\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_v_neg_1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     _mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_v\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    901\u001b[0m     _val \u001b[38;5;241m=\u001b[39m _mat\u001b[38;5;241m.\u001b[39m_values() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    902\u001b[0m     _val[torch\u001b[38;5;241m.\u001b[39misinf(_val)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:863\u001b[0m, in \u001b[0;36mHypergraph.D_v\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the vertex degree matrix :math:`\\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_v\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_of_group(name)\u001b[38;5;241m.\u001b[39m_values()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_names]\n\u001b[1;32m    864\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(_tmp)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_v\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m    866\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    867\u001b[0m         _tmp,\n\u001b[1;32m    868\u001b[0m         torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v]),\n\u001b[1;32m    869\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    870\u001b[0m     )\u001b[38;5;241m.\u001b[39mcoalesce()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:863\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the vertex degree matrix :math:`\\mathbf{D}_v` with ``torch.sparse_coo_tensor`` format.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_v\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 863\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_v_of_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_names]\n\u001b[1;32m    864\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(_tmp)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD_v\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m    866\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    867\u001b[0m         _tmp,\n\u001b[1;32m    868\u001b[0m         torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_v]),\n\u001b[1;32m    869\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    870\u001b[0m     )\u001b[38;5;241m.\u001b[39mcoalesce()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:883\u001b[0m, in \u001b[0;36mHypergraph.D_v_of_group\u001b[0;34m(self, group_name)\u001b[0m\n\u001b[1;32m    881\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_of_group(group_name)\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    882\u001b[0m w_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_e_of_group(group_name)\u001b[38;5;241m.\u001b[39m_values()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 883\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mw_e\u001b[49m\u001b[43m[\u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m H_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(H\u001b[38;5;241m.\u001b[39m_indices(), val, size\u001b[38;5;241m=\u001b[39mH\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mcoalesce()\n\u001b[1;32m    885\u001b[0m _tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39msum(H_, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dense()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "best_loss = 1000\n",
    "seed = 1\n",
    "for i in range(100,200,1):\n",
    "    torch.manual_seed(i)\n",
    "    X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "    X = torch.eye(n=G.num_v)\n",
    "    embedding_net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "    partition_net = BDLiner(hyper[\"l_hyper_prmts\"]).to(DEVICE)\n",
    "    hgnn_trainer = Trainer(net=[embedding_net, partition_net], X=X, hg=G).to(DEVICE)\n",
    "    import torch.optim as optim\n",
    "    optim = optim.Adam(hgnn_trainer.parameters(), lr=4e-5, weight_decay=5e-8)\n",
    "    hgnn_trainer.optimizer = optim\n",
    "    for epoch in range(4500):\n",
    "        loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "        if loss_1 < best_loss:\n",
    "            best_loss = loss_1\n",
    "        # train\n",
    "        temp_loss_total += loss\n",
    "        temp_loss1 += loss_1\n",
    "        temp_loss2 += loss_2\n",
    "        # validation\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "            print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "            print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "            print(f\"=================================\")\n",
    "            sys.stdout.flush()\n",
    "            temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "print(\"=================================\")\n",
    "print(\"best loss:\",best_loss)\n",
    "print(\"seed:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
