{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 4\n",
    "weight = 20\n",
    "limit = 0.5\n",
    "sub = 0.018\n",
    "lr = 4e-3\n",
    "# h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers26\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers22\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers17\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "# h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers26\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers23\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers221\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers222\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers22\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers17\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 128, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":256, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer1w2\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":4, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss =  weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer1: torch.optim.Optimizer = optimizer\n",
    "        self.optimizer2: torch.optim.Optimizer = optimizer\n",
    "        self.convlayers = nn.ModuleList()\n",
    "        self.convlayers.append(net.to(DEVICE))\n",
    "        self.linearlayers = nn.ModuleList()\n",
    "        self.weight = 200\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.convlayers:\n",
    "            X = layer(X, self.hg)\n",
    "        for layer in self.linearlayers:\n",
    "            if isinstance(layer, nn.MultiheadAttention):\n",
    "                X,_ = layer(X, X, X)\n",
    "            else:\n",
    "                X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()\n",
    "        self.optimizer1.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer1.step()\n",
    "        self.optimizer2.step()\n",
    "        \n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 1019)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/citeseer\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.05, inplace=False)\n",
       "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.05, inplace=False)\n",
       "  (7): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU()\n",
       "  (10): Dropout(p=0.05, inplace=False)\n",
       "  (11): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (12): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ReLU()\n",
       "  (14): Dropout(p=0.05, inplace=False)\n",
       "  (15): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (16): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    if str.startswith(k,\"attn\"):\n",
    "        hgnn_trainer.linearlayers.append(nn.MultiheadAttention(embed_dim=v[\"in_channels\"], num_heads=2).to(DEVICE))\n",
    "    else:\n",
    "        hgnn_trainer.linearlayers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "        hgnn_trainer.linearlayers.append(nn.ReLU().to(DEVICE))\n",
    "        if v[\"drop_rate\"] > 0:\n",
    "            hgnn_trainer.linearlayers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "        hgnn_trainer.linearlayers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.linearlayers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 157.5969482421875\n",
      "                , loss1: -2.9138809204101563\n",
      "                , loss2: 215.822119140625\n",
      "                , loss2_weight: 19.982\n",
      "=================================\n",
      "in 10 epoch, average loss: -1783.803125\n",
      "                , loss1: -99.23768920898438\n",
      "                , loss2: 187.2715576171875\n",
      "                , loss2_weight: 19.801999999999992\n",
      "=================================\n",
      "in 20 epoch, average loss: -5039.4234375\n",
      "                , loss1: -257.14013671875\n",
      "                , loss2: 24.413803100585938\n",
      "                , loss2_weight: 19.621999999999986\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 30 epoch, average loss: -8847.07734375\n",
      "                , loss1: -454.051220703125\n",
      "                , loss2: 14.3947509765625\n",
      "                , loss2_weight: 19.44199999999998\n",
      "=================================\n",
      "in 40 epoch, average loss: -11439.603125\n",
      "                , loss1: -591.685205078125\n",
      "                , loss2: 4.290977859497071\n",
      "                , loss2_weight: 19.261999999999972\n",
      "=================================\n",
      "in 50 epoch, average loss: -12108.34140625\n",
      "                , loss1: -632.0677734375\n",
      "                , loss2: 3.7163860321044924\n",
      "                , loss2_weight: 19.081999999999965\n",
      "=================================\n",
      "in 60 epoch, average loss: -12173.16875\n",
      "                , loss1: -641.476123046875\n",
      "                , loss2: 3.915707015991211\n",
      "                , loss2_weight: 18.90199999999996\n",
      "=================================\n",
      "in 70 epoch, average loss: -12107.42734375\n",
      "                , loss1: -644.02490234375\n",
      "                , loss2: 2.146830940246582\n",
      "                , loss2_weight: 18.72199999999995\n",
      "=================================\n",
      "in 80 epoch, average loss: -12042.34140625\n",
      "                , loss1: -646.6974609375\n",
      "                , loss2: 1.0491962432861328\n",
      "                , loss2_weight: 18.541999999999945\n",
      "=================================\n",
      "in 90 epoch, average loss: -11952.9\n",
      "                , loss1: -648.17685546875\n",
      "                , loss2: 1.4194608688354493\n",
      "                , loss2_weight: 18.361999999999938\n",
      "=================================\n",
      "in 100 epoch, average loss: -11845.31796875\n",
      "                , loss1: -648.631298828125\n",
      "                , loss2: 0.6284763813018799\n",
      "                , loss2_weight: 18.18199999999993\n",
      "=================================\n",
      "in 110 epoch, average loss: -11733.5859375\n",
      "                , loss1: -648.9087890625\n",
      "                , loss2: 0.6300149917602539\n",
      "                , loss2_weight: 18.001999999999924\n",
      "=================================\n",
      "in 120 epoch, average loss: -11621.1265625\n",
      "                , loss1: -649.131494140625\n",
      "                , loss2: 0.2705048084259033\n",
      "                , loss2_weight: 17.821999999999917\n",
      "=================================\n",
      "in 130 epoch, average loss: -11507.3765625\n",
      "                , loss1: -649.302392578125\n",
      "                , loss2: 0.20798394680023194\n",
      "                , loss2_weight: 17.64199999999991\n",
      "=================================\n",
      "in 140 epoch, average loss: -11392.8125\n",
      "                , loss1: -649.43583984375\n",
      "                , loss2: 0.2412806510925293\n",
      "                , loss2_weight: 17.461999999999904\n",
      "=================================\n",
      "in 150 epoch, average loss: -11278.31796875\n",
      "                , loss1: -649.573046875\n",
      "                , loss2: 0.21589291095733643\n",
      "                , loss2_weight: 17.281999999999897\n",
      "=================================\n",
      "in 160 epoch, average loss: -11162.71015625\n",
      "                , loss1: -649.6533203125\n",
      "                , loss2: 0.28012354373931886\n",
      "                , loss2_weight: 17.10199999999989\n",
      "=================================\n",
      "in 170 epoch, average loss: -11047.0484375\n",
      "                , loss1: -649.72392578125\n",
      "                , loss2: 0.2060847759246826\n",
      "                , loss2_weight: 16.921999999999883\n",
      "=================================\n",
      "in 180 epoch, average loss: -10931.63359375\n",
      "                , loss1: -649.81396484375\n",
      "                , loss2: 0.1851741075515747\n",
      "                , loss2_weight: 16.741999999999877\n",
      "=================================\n",
      "in 190 epoch, average loss: -10815.571875\n",
      "                , loss1: -649.87099609375\n",
      "                , loss2: 0.22925267219543458\n",
      "                , loss2_weight: 16.56199999999987\n",
      "=================================\n",
      "in 200 epoch, average loss: -10699.67734375\n",
      "                , loss1: -649.943798828125\n",
      "                , loss2: 0.3453786849975586\n",
      "                , loss2_weight: 16.381999999999863\n",
      "=================================\n",
      "in 210 epoch, average loss: -10583.396875\n",
      "                , loss1: -649.98583984375\n",
      "                , loss2: 0.3211680889129639\n",
      "                , loss2_weight: 16.201999999999856\n",
      "=================================\n",
      "in 220 epoch, average loss: -10467.0703125\n",
      "                , loss1: -650.0271484375\n",
      "                , loss2: 0.31550471782684325\n",
      "                , loss2_weight: 16.02199999999985\n",
      "=================================\n",
      "in 230 epoch, average loss: -10350.70625\n",
      "                , loss1: -650.058349609375\n",
      "                , loss2: 0.1710995078086853\n",
      "                , loss2_weight: 15.841999999999842\n",
      "=================================\n",
      "in 240 epoch, average loss: -10234.4765625\n",
      "                , loss1: -650.102490234375\n",
      "                , loss2: 0.08629994988441467\n",
      "                , loss2_weight: 15.661999999999836\n",
      "=================================\n",
      "in 250 epoch, average loss: -10117.9265625\n",
      "                , loss1: -650.135205078125\n",
      "                , loss2: 0.12644563913345336\n",
      "                , loss2_weight: 15.481999999999829\n",
      "=================================\n",
      "in 260 epoch, average loss: -10001.3015625\n",
      "                , loss1: -650.160205078125\n",
      "                , loss2: 0.11224009990692138\n",
      "                , loss2_weight: 15.301999999999822\n",
      "=================================\n",
      "in 270 epoch, average loss: -9884.81328125\n",
      "                , loss1: -650.194384765625\n",
      "                , loss2: 0.09279774427413941\n",
      "                , loss2_weight: 15.121999999999815\n",
      "=================================\n",
      "in 280 epoch, average loss: -9768.26875\n",
      "                , loss1: -650.22685546875\n",
      "                , loss2: 0.08856980204582214\n",
      "                , loss2_weight: 14.941999999999808\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[495], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      8\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m sub\n\u001b[0;32m----> 9\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     11\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[491], line 38\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer1\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer2\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[491], line 26\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvlayers:\n\u001b[0;32m---> 26\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinearlayers:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mMultiheadAttention):\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition/examples/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1656\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e2v_drop_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[0;32m-> 1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2e_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2e_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv2e_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v(X, e2v_aggr, e2v_weight, drop_rate\u001b[38;5;241m=\u001b[39me2v_drop_rate)\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1418\u001b[0m, in \u001b[0;36mHypergraph.v2e\u001b[0;34m(self, X, aggr, v2e_weight, e_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mv2e\u001b[39m(\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1403\u001b[0m     X: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1408\u001b[0m ):\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \n\u001b[1;32m   1411\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1418\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2e_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2e_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1419\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e_update(X, e_weight)\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1267\u001b[0m, in \u001b[0;36mHypergraph.v2e_aggregation\u001b[0;34m(self, X, aggr, v2e_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_T\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1267\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1268\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_e_neg_1, X)\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.convlayers.parameters(), lr=lr, weight_decay=5e-8)\n",
    "optim2 = optim.Adam(hgnn_trainer.linearlayers.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer1 = optim1\n",
    "hgnn_trainer.optimizer2 = optim2\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , loss2_weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([164., 163.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0030581039755351682"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
