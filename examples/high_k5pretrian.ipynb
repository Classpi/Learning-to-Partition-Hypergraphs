{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 5\n",
    "\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers141\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers1233\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer131\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "# l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":1024, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "# l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":327, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":327, \"out_channels\":5, \"use_bn\":True, \"drop_rate\":0.02}\n",
    "# l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":5, \"use_bn\":False, \"drop_rate\":0.01}\n",
    "\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer, label):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.label = label.to(DEVICE)\n",
    "        self.weight = 200\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def pre_train(self, epoch):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss = nn.CrossEntropyLoss()(outs, self.label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(\n",
    "            outs, self.hg, device=DEVICE, weight=self.weight\n",
    "        )\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7818, 327, 327)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "import pickle\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/high\")\n",
    "edges, _ = G.e\n",
    "with open(\"../data/pretrain/label_high.pkl\", \"rb\") as f:\n",
    "    label = pickle.load(f)\n",
    "G.num_e,G.num_v,len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None,label=label).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(torch.nn.GELU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "    \n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 0.15934673547744752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 0.2504544734954834\n",
      "in 20 epoch, average loss: 0.09434515237808228\n",
      "in 30 epoch, average loss: 0.04748322069644928\n",
      "in 40 epoch, average loss: 0.01650959998369217\n",
      "in 50 epoch, average loss: 0.007954175770282745\n",
      "in 60 epoch, average loss: 0.004697287827730179\n",
      "in 70 epoch, average loss: 0.00363367460668087\n",
      "in 80 epoch, average loss: 0.0028901617974042893\n",
      "in 90 epoch, average loss: 0.0024097533896565437\n",
      "in 100 epoch, average loss: 0.002149314060807228\n",
      "in 110 epoch, average loss: 0.0018645396456122398\n",
      "in 120 epoch, average loss: 0.0017013099044561387\n",
      "in 130 epoch, average loss: 0.0015021882951259612\n",
      "in 140 epoch, average loss: 0.0013747300021350385\n",
      "in 150 epoch, average loss: 0.0012483466416597366\n",
      "in 160 epoch, average loss: 0.0011622371152043343\n",
      "in 170 epoch, average loss: 0.0010738635435700416\n",
      "in 180 epoch, average loss: 0.000977269932627678\n",
      "in 190 epoch, average loss: 0.0009106551297008991\n"
     ]
    }
   ],
   "source": [
    "temp_loss_total= torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(200):\n",
    "    loss = hgnn_trainer.pre_train(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total = torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): GELU(approximate='none')\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): GELU(approximate='none')\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=327, bias=True)\n",
       "  (9): BatchNorm1d(327, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): GELU(approximate='none')\n",
       "  (11): Dropout(p=0.02, inplace=False)\n",
       "  (12): Linear(in_features=327, out_features=5, bias=True)\n",
       "  (13): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in hgnn_trainer.layers[1:]:\n",
    "    if isinstance(layer,nn.Linear):\n",
    "        layer.reset_parameters()\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = 0.0015385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 17.866627502441407\n",
      "                , loss1: -29.017706298828124\n",
      "                , loss2: 17.91127166748047\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 54.013922119140624\n",
      "                , loss1: -466.41572265625\n",
      "                , loss2: 54.731500244140626\n",
      "=================================\n",
      "in 20 epoch, average loss: 0.9139400482177734\n",
      "                , loss1: -321.905322265625\n",
      "                , loss2: 1.4091915130615233\n",
      "=================================\n",
      "in 30 epoch, average loss: -0.12720010280609131\n",
      "                , loss1: -322.550146484375\n",
      "                , loss2: 0.36904330253601075\n",
      "=================================\n",
      "in 40 epoch, average loss: -0.09134870171546935\n",
      "                , loss1: -329.3080322265625\n",
      "                , loss2: 0.4152916431427002\n",
      "=================================\n",
      "in 50 epoch, average loss: -0.3867816925048828\n",
      "                , loss1: -335.1601318359375\n",
      "                , loss2: 0.12886215448379518\n",
      "=================================\n",
      "in 60 epoch, average loss: -0.4051405429840088\n",
      "                , loss1: -349.384912109375\n",
      "                , loss2: 0.13238816261291503\n",
      "=================================\n",
      "in 70 epoch, average loss: -0.2228153705596924\n",
      "                , loss1: -374.7165283203125\n",
      "                , loss2: 0.35368597507476807\n",
      "=================================\n",
      "in 80 epoch, average loss: -0.2079464912414551\n",
      "                , loss1: -407.362841796875\n",
      "                , loss2: 0.4187811851501465\n",
      "=================================\n",
      "in 90 epoch, average loss: -0.3914001703262329\n",
      "                , loss1: -446.352001953125\n",
      "                , loss2: 0.2953123331069946\n",
      "=================================\n",
      "in 100 epoch, average loss: -0.34413895606994627\n",
      "                , loss1: -503.818505859375\n",
      "                , loss2: 0.4309857368469238\n",
      "=================================\n",
      "in 110 epoch, average loss: -0.6557831764221191\n",
      "                , loss1: -567.89345703125\n",
      "                , loss2: 0.21792099475860596\n",
      "=================================\n",
      "in 120 epoch, average loss: -0.8078392028808594\n",
      "                , loss1: -663.56298828125\n",
      "                , loss2: 0.21305248737335206\n",
      "=================================\n",
      "in 130 epoch, average loss: -0.899256420135498\n",
      "                , loss1: -737.880224609375\n",
      "                , loss2: 0.23597221374511718\n",
      "=================================\n",
      "in 140 epoch, average loss: -1.095463466644287\n",
      "                , loss1: -803.18974609375\n",
      "                , loss2: 0.14024399518966674\n",
      "=================================\n",
      "in 150 epoch, average loss: -1.2365048408508301\n",
      "                , loss1: -866.784375\n",
      "                , loss2: 0.09704280495643616\n",
      "=================================\n",
      "in 160 epoch, average loss: -1.2518077850341798\n",
      "                , loss1: -922.5005859375\n",
      "                , loss2: 0.1674593687057495\n",
      "=================================\n",
      "in 170 epoch, average loss: -1.1208048820495606\n",
      "                , loss1: -976.57412109375\n",
      "                , loss2: 0.38165435791015623\n",
      "=================================\n",
      "in 180 epoch, average loss: -1.3340794563293457\n",
      "                , loss1: -1022.57646484375\n",
      "                , loss2: 0.23915452957153321\n",
      "=================================\n",
      "in 190 epoch, average loss: -1.460640048980713\n",
      "                , loss1: -1067.5033203125\n",
      "                , loss2: 0.18171361684799195\n",
      "=================================\n",
      "in 200 epoch, average loss: -1.4908044815063477\n",
      "                , loss1: -1112.58359375\n",
      "                , loss2: 0.22090530395507812\n",
      "=================================\n",
      "in 210 epoch, average loss: -1.2713940620422364\n",
      "                , loss1: -1159.01416015625\n",
      "                , loss2: 0.5117491722106934\n",
      "=================================\n",
      "in 220 epoch, average loss: -1.2798783302307128\n",
      "                , loss1: -1179.45478515625\n",
      "                , loss2: 0.5347126960754395\n",
      "=================================\n",
      "in 230 epoch, average loss: -1.499299907684326\n",
      "                , loss1: -1204.5640625\n",
      "                , loss2: 0.3539216756820679\n",
      "=================================\n",
      "in 240 epoch, average loss: -1.652764129638672\n",
      "                , loss1: -1239.88779296875\n",
      "                , loss2: 0.254803466796875\n",
      "=================================\n",
      "in 250 epoch, average loss: -1.8338983535766602\n",
      "                , loss1: -1277.9173828125\n",
      "                , loss2: 0.132177472114563\n",
      "=================================\n",
      "in 260 epoch, average loss: -1.8204151153564454\n",
      "                , loss1: -1324.37548828125\n",
      "                , loss2: 0.21713640689849853\n",
      "=================================\n",
      "in 270 epoch, average loss: -1.7907718658447265\n",
      "                , loss1: -1380.74228515625\n",
      "                , loss2: 0.3334999084472656\n",
      "=================================\n",
      "in 280 epoch, average loss: -1.9704084396362305\n",
      "                , loss1: -1451.88828125\n",
      "                , loss2: 0.26332175731658936\n",
      "=================================\n",
      "in 290 epoch, average loss: -1.9675533294677734\n",
      "                , loss1: -1547.40048828125\n",
      "                , loss2: 0.41312193870544434\n",
      "=================================\n",
      "in 300 epoch, average loss: -2.1461456298828123\n",
      "                , loss1: -1673.0201171875\n",
      "                , loss2: 0.4277955532073975\n",
      "=================================\n",
      "in 310 epoch, average loss: -2.185466194152832\n",
      "                , loss1: -1841.43046875\n",
      "                , loss2: 0.6475746154785156\n",
      "=================================\n",
      "in 320 epoch, average loss: -2.859670639038086\n",
      "                , loss1: -1996.8109375\n",
      "                , loss2: 0.21242289543151854\n",
      "=================================\n",
      "in 330 epoch, average loss: -3.039775276184082\n",
      "                , loss1: -2119.366796875\n",
      "                , loss2: 0.22087109088897705\n",
      "=================================\n",
      "in 340 epoch, average loss: -2.936143493652344\n",
      "                , loss1: -2207.9888671875\n",
      "                , loss2: 0.4608473300933838\n",
      "=================================\n",
      "in 350 epoch, average loss: -3.218408966064453\n",
      "                , loss1: -2249.6107421875\n",
      "                , loss2: 0.24261679649353027\n",
      "=================================\n",
      "in 360 epoch, average loss: -3.3371997833251954\n",
      "                , loss1: -2283.1201171875\n",
      "                , loss2: 0.1753804326057434\n",
      "=================================\n",
      "in 370 epoch, average loss: -2.8611602783203125\n",
      "                , loss1: -2315.5306640625\n",
      "                , loss2: 0.701283311843872\n",
      "=================================\n",
      "in 380 epoch, average loss: -3.1467348098754884\n",
      "                , loss1: -2325.1130859375\n",
      "                , loss2: 0.4304516792297363\n",
      "=================================\n",
      "in 390 epoch, average loss: -3.389822769165039\n",
      "                , loss1: -2339.09296875\n",
      "                , loss2: 0.20887186527252197\n",
      "=================================\n",
      "in 400 epoch, average loss: -3.3292163848876952\n",
      "                , loss1: -2362.473046875\n",
      "                , loss2: 0.3054481506347656\n",
      "=================================\n",
      "in 410 epoch, average loss: -3.391217803955078\n",
      "                , loss1: -2380.1361328125\n",
      "                , loss2: 0.27062153816223145\n",
      "=================================\n",
      "in 420 epoch, average loss: -3.364374542236328\n",
      "                , loss1: -2406.708203125\n",
      "                , loss2: 0.3383459568023682\n",
      "=================================\n",
      "in 430 epoch, average loss: -3.146861457824707\n",
      "                , loss1: -2414.9537109375\n",
      "                , loss2: 0.5685445308685303\n",
      "=================================\n",
      "in 440 epoch, average loss: -3.183014678955078\n",
      "                , loss1: -2432.3232421875\n",
      "                , loss2: 0.5591139793395996\n",
      "=================================\n",
      "in 450 epoch, average loss: -3.3058841705322264\n",
      "                , loss1: -2434.964453125\n",
      "                , loss2: 0.4403089046478271\n",
      "=================================\n",
      "in 460 epoch, average loss: -3.3106502532958983\n",
      "                , loss1: -2456.48203125\n",
      "                , loss2: 0.4686466693878174\n",
      "=================================\n",
      "in 470 epoch, average loss: -3.4789451599121093\n",
      "                , loss1: -2459.571875\n",
      "                , loss2: 0.3051058292388916\n",
      "=================================\n",
      "in 480 epoch, average loss: -3.502567672729492\n",
      "                , loss1: -2475.532421875\n",
      "                , loss2: 0.3060393571853638\n",
      "=================================\n",
      "in 490 epoch, average loss: -3.6925636291503907\n",
      "                , loss1: -2491.487890625\n",
      "                , loss2: 0.1405901551246643\n",
      "=================================\n",
      "in 500 epoch, average loss: -3.6974716186523438\n",
      "                , loss1: -2506.3955078125\n",
      "                , loss2: 0.15861814022064208\n",
      "=================================\n",
      "in 510 epoch, average loss: -3.5667278289794924\n",
      "                , loss1: -2520.7609375\n",
      "                , loss2: 0.31146304607391356\n",
      "=================================\n",
      "in 520 epoch, average loss: -3.4627426147460936\n",
      "                , loss1: -2531.7443359375\n",
      "                , loss2: 0.4323460578918457\n",
      "=================================\n",
      "in 530 epoch, average loss: -2.8504268646240236\n",
      "                , loss1: -2553.27578125\n",
      "                , loss2: 1.0777877807617187\n",
      "=================================\n",
      "in 540 epoch, average loss: -2.141204071044922\n",
      "                , loss1: -2550.121484375\n",
      "                , loss2: 1.7821577072143555\n",
      "=================================\n",
      "in 550 epoch, average loss: -2.4906810760498046\n",
      "                , loss1: -2519.16796875\n",
      "                , loss2: 1.3850587844848632\n",
      "=================================\n",
      "in 560 epoch, average loss: -3.2544837951660157\n",
      "                , loss1: -2528.21015625\n",
      "                , loss2: 0.6351675510406494\n",
      "=================================\n",
      "in 570 epoch, average loss: -3.6493736267089845\n",
      "                , loss1: -2536.059765625\n",
      "                , loss2: 0.25235395431518554\n",
      "=================================\n",
      "in 580 epoch, average loss: -3.684447479248047\n",
      "                , loss1: -2557.5630859375\n",
      "                , loss2: 0.2503634214401245\n",
      "=================================\n",
      "in 590 epoch, average loss: -3.864480972290039\n",
      "                , loss1: -2573.9447265625\n",
      "                , loss2: 0.09553291797637939\n",
      "=================================\n",
      "in 600 epoch, average loss: -3.834092712402344\n",
      "                , loss1: -2593.4845703125\n",
      "                , loss2: 0.1559828281402588\n",
      "=================================\n",
      "in 610 epoch, average loss: -3.6740421295166015\n",
      "                , loss1: -2611.3453125\n",
      "                , loss2: 0.34351229667663574\n",
      "=================================\n",
      "in 620 epoch, average loss: -3.8987064361572266\n",
      "                , loss1: -2621.4798828125\n",
      "                , loss2: 0.13444008827209472\n",
      "=================================\n",
      "in 630 epoch, average loss: -3.835634231567383\n",
      "                , loss1: -2639.383984375\n",
      "                , loss2: 0.22505793571472169\n",
      "=================================\n",
      "in 640 epoch, average loss: -3.9490047454833985\n",
      "                , loss1: -2651.02109375\n",
      "                , loss2: 0.1295904517173767\n",
      "=================================\n",
      "in 650 epoch, average loss: -3.8761112213134767\n",
      "                , loss1: -2670.194921875\n",
      "                , loss2: 0.2319835901260376\n",
      "=================================\n",
      "in 660 epoch, average loss: -3.880260467529297\n",
      "                , loss1: -2678.3373046875\n",
      "                , loss2: 0.24036142826080323\n",
      "=================================\n",
      "in 670 epoch, average loss: -3.918875885009766\n",
      "                , loss1: -2696.53125\n",
      "                , loss2: 0.2297372341156006\n",
      "=================================\n",
      "in 680 epoch, average loss: -3.9679946899414062\n",
      "                , loss1: -2707.462109375\n",
      "                , loss2: 0.19743598699569703\n",
      "=================================\n",
      "in 690 epoch, average loss: -3.895492172241211\n",
      "                , loss1: -2725.47578125\n",
      "                , loss2: 0.29765193462371825\n",
      "=================================\n",
      "in 700 epoch, average loss: -3.982962417602539\n",
      "                , loss1: -2736.78671875\n",
      "                , loss2: 0.22758424282073975\n",
      "=================================\n",
      "in 710 epoch, average loss: -4.008150100708008\n",
      "                , loss1: -2751.459375\n",
      "                , loss2: 0.22496984004974366\n",
      "=================================\n",
      "in 720 epoch, average loss: -4.045526504516602\n",
      "                , loss1: -2760.330078125\n",
      "                , loss2: 0.2012409448623657\n",
      "=================================\n",
      "in 730 epoch, average loss: -4.049446105957031\n",
      "                , loss1: -2775.40078125\n",
      "                , loss2: 0.22050795555114747\n",
      "=================================\n",
      "in 740 epoch, average loss: -4.172825622558594\n",
      "                , loss1: -2784.7400390625\n",
      "                , loss2: 0.11149686574935913\n",
      "=================================\n",
      "in 750 epoch, average loss: -4.127688980102539\n",
      "                , loss1: -2795.1986328125\n",
      "                , loss2: 0.1727236270904541\n",
      "=================================\n",
      "in 760 epoch, average loss: -4.112695693969727\n",
      "                , loss1: -2805.839453125\n",
      "                , loss2: 0.20408830642700196\n",
      "=================================\n",
      "in 770 epoch, average loss: -4.0292213439941404\n",
      "                , loss1: -2815.15\n",
      "                , loss2: 0.3018867015838623\n",
      "=================================\n",
      "in 780 epoch, average loss: -3.9815040588378907\n",
      "                , loss1: -2820.76640625\n",
      "                , loss2: 0.3582445621490479\n",
      "=================================\n",
      "in 790 epoch, average loss: -4.237355422973633\n",
      "                , loss1: -2829.6291015625\n",
      "                , loss2: 0.11602874994277954\n",
      "=================================\n",
      "in 800 epoch, average loss: -4.15839729309082\n",
      "                , loss1: -2834.24375\n",
      "                , loss2: 0.2020864248275757\n",
      "=================================\n",
      "in 810 epoch, average loss: -4.243439483642578\n",
      "                , loss1: -2845.251953125\n",
      "                , loss2: 0.13398002386093139\n",
      "=================================\n",
      "in 820 epoch, average loss: -4.247129440307617\n",
      "                , loss1: -2852.501953125\n",
      "                , loss2: 0.14144520759582518\n",
      "=================================\n",
      "in 830 epoch, average loss: -4.261883926391602\n",
      "                , loss1: -2860.1345703125\n",
      "                , loss2: 0.13843324184417724\n",
      "=================================\n",
      "in 840 epoch, average loss: -4.137407302856445\n",
      "                , loss1: -2870.831640625\n",
      "                , loss2: 0.2793670892715454\n",
      "=================================\n",
      "in 850 epoch, average loss: -4.1466514587402346\n",
      "                , loss1: -2872.73046875\n",
      "                , loss2: 0.27304465770721437\n",
      "=================================\n",
      "in 860 epoch, average loss: -4.264812088012695\n",
      "                , loss1: -2882.45078125\n",
      "                , loss2: 0.1698380708694458\n",
      "=================================\n",
      "in 870 epoch, average loss: -4.23111572265625\n",
      "                , loss1: -2883.95\n",
      "                , loss2: 0.2058410406112671\n",
      "=================================\n",
      "in 880 epoch, average loss: -4.309242248535156\n",
      "                , loss1: -2887.088671875\n",
      "                , loss2: 0.13254377841949463\n",
      "=================================\n",
      "in 890 epoch, average loss: -4.329240036010742\n",
      "                , loss1: -2894.130859375\n",
      "                , loss2: 0.12337970733642578\n",
      "=================================\n",
      "in 900 epoch, average loss: -4.273776626586914\n",
      "                , loss1: -2902.721484375\n",
      "                , loss2: 0.19206029176712036\n",
      "=================================\n",
      "in 910 epoch, average loss: -4.2453155517578125\n",
      "                , loss1: -2907.077734375\n",
      "                , loss2: 0.22722322940826417\n",
      "=================================\n",
      "in 920 epoch, average loss: -4.193780517578125\n",
      "                , loss1: -2906.503515625\n",
      "                , loss2: 0.27787489891052247\n",
      "=================================\n",
      "in 930 epoch, average loss: -4.2924339294433596\n",
      "                , loss1: -2910.8982421875\n",
      "                , loss2: 0.18598322868347167\n",
      "=================================\n",
      "in 940 epoch, average loss: -4.208840179443359\n",
      "                , loss1: -2917.226953125\n",
      "                , loss2: 0.27931392192840576\n",
      "=================================\n",
      "in 950 epoch, average loss: -4.100749206542969\n",
      "                , loss1: -2918.264453125\n",
      "                , loss2: 0.3890004396438599\n",
      "=================================\n",
      "in 960 epoch, average loss: -4.04362678527832\n",
      "                , loss1: -2926.2044921875\n",
      "                , loss2: 0.45833897590637207\n",
      "=================================\n",
      "in 970 epoch, average loss: -4.24629020690918\n",
      "                , loss1: -2919.7814453125\n",
      "                , loss2: 0.24579365253448487\n",
      "=================================\n",
      "in 980 epoch, average loss: -4.293296813964844\n",
      "                , loss1: -2923.044921875\n",
      "                , loss2: 0.20380823612213134\n",
      "=================================\n",
      "in 990 epoch, average loss: -4.301006317138672\n",
      "                , loss1: -2926.8712890625\n",
      "                , loss2: 0.20198535919189453\n",
      "=================================\n",
      "in 1000 epoch, average loss: -4.368508148193359\n",
      "                , loss1: -2933.63671875\n",
      "                , loss2: 0.1448918342590332\n",
      "=================================\n",
      "in 1010 epoch, average loss: -4.407634353637695\n",
      "                , loss1: -2940.5107421875\n",
      "                , loss2: 0.11634154319763183\n",
      "=================================\n",
      "in 1020 epoch, average loss: -4.35345458984375\n",
      "                , loss1: -2947.265625\n",
      "                , loss2: 0.18091310262680055\n",
      "=================================\n",
      "in 1030 epoch, average loss: -4.314239883422852\n",
      "                , loss1: -2951.2134765625\n",
      "                , loss2: 0.22620182037353515\n",
      "=================================\n",
      "in 1040 epoch, average loss: -4.276305389404297\n",
      "                , loss1: -2954.6388671875\n",
      "                , loss2: 0.26940605640411375\n",
      "=================================\n",
      "in 1050 epoch, average loss: -4.351930618286133\n",
      "                , loss1: -2956.590234375\n",
      "                , loss2: 0.19678300619125366\n",
      "=================================\n",
      "in 1060 epoch, average loss: -4.468880462646484\n",
      "                , loss1: -2958.0958984375\n",
      "                , loss2: 0.08214995861053467\n",
      "=================================\n",
      "in 1070 epoch, average loss: -4.457077407836914\n",
      "                , loss1: -2963.82578125\n",
      "                , loss2: 0.10276830196380615\n",
      "=================================\n",
      "in 1080 epoch, average loss: -4.4770751953125\n",
      "                , loss1: -2971.2990234375\n",
      "                , loss2: 0.09426767826080322\n",
      "=================================\n",
      "in 1090 epoch, average loss: -4.419260406494141\n",
      "                , loss1: -2975.437109375\n",
      "                , loss2: 0.15844922065734862\n",
      "=================================\n",
      "in 1100 epoch, average loss: -4.3323310852050785\n",
      "                , loss1: -2980.6482421875\n",
      "                , loss2: 0.2533962965011597\n",
      "=================================\n",
      "in 1110 epoch, average loss: -4.416350555419922\n",
      "                , loss1: -2982.6076171875\n",
      "                , loss2: 0.17239147424697876\n",
      "=================================\n",
      "in 1120 epoch, average loss: -4.463368225097656\n",
      "                , loss1: -2984.184375\n",
      "                , loss2: 0.1277987003326416\n",
      "=================================\n",
      "in 1130 epoch, average loss: -4.398614120483399\n",
      "                , loss1: -2991.090234375\n",
      "                , loss2: 0.20317840576171875\n",
      "=================================\n",
      "in 1140 epoch, average loss: -4.428996658325195\n",
      "                , loss1: -3001.6763671875\n",
      "                , loss2: 0.1890822172164917\n",
      "=================================\n",
      "in 1150 epoch, average loss: -4.256388854980469\n",
      "                , loss1: -3005.3390625\n",
      "                , loss2: 0.3673251152038574\n",
      "=================================\n",
      "in 1160 epoch, average loss: -4.372953414916992\n",
      "                , loss1: -3006.019921875\n",
      "                , loss2: 0.25180811882019044\n",
      "=================================\n",
      "in 1170 epoch, average loss: -4.4476158142089846\n",
      "                , loss1: -3005.216796875\n",
      "                , loss2: 0.17591084241867067\n",
      "=================================\n",
      "in 1180 epoch, average loss: -4.197938919067383\n",
      "                , loss1: -3010.8779296875\n",
      "                , loss2: 0.4342963695526123\n",
      "=================================\n",
      "in 1190 epoch, average loss: -4.199512100219726\n",
      "                , loss1: -3002.4736328125\n",
      "                , loss2: 0.41979308128356935\n",
      "=================================\n",
      "in 1200 epoch, average loss: -4.424877166748047\n",
      "                , loss1: -3001.230859375\n",
      "                , loss2: 0.19251651763916017\n",
      "=================================\n",
      "in 1210 epoch, average loss: -4.344145584106445\n",
      "                , loss1: -3010.967578125\n",
      "                , loss2: 0.288228440284729\n",
      "=================================\n",
      "in 1220 epoch, average loss: -4.462365341186524\n",
      "                , loss1: -3011.444921875\n",
      "                , loss2: 0.17074296474456788\n",
      "=================================\n",
      "in 1230 epoch, average loss: -4.410513687133789\n",
      "                , loss1: -3020.6666015625\n",
      "                , loss2: 0.23678183555603027\n",
      "=================================\n",
      "in 1240 epoch, average loss: -4.422351837158203\n",
      "                , loss1: -3026.6048828125\n",
      "                , loss2: 0.23407905101776122\n",
      "=================================\n",
      "in 1250 epoch, average loss: -4.533479309082031\n",
      "                , loss1: -3036.19375\n",
      "                , loss2: 0.13770462274551393\n",
      "=================================\n",
      "in 1260 epoch, average loss: -4.575428009033203\n",
      "                , loss1: -3047.144140625\n",
      "                , loss2: 0.1126032829284668\n",
      "=================================\n",
      "in 1270 epoch, average loss: -4.4678386688232425\n",
      "                , loss1: -3060.28671875\n",
      "                , loss2: 0.24041252136230468\n",
      "=================================\n",
      "in 1280 epoch, average loss: -4.661479949951172\n",
      "                , loss1: -3080.0505859375\n",
      "                , loss2: 0.07717803716659546\n",
      "=================================\n",
      "in 1290 epoch, average loss: -4.583614730834961\n",
      "                , loss1: -3103.392578125\n",
      "                , loss2: 0.19095442295074463\n",
      "=================================\n",
      "in 1300 epoch, average loss: -4.710782623291015\n",
      "                , loss1: -3138.23203125\n",
      "                , loss2: 0.11738678216934204\n",
      "=================================\n",
      "in 1310 epoch, average loss: -4.771430969238281\n",
      "                , loss1: -3193.6609375\n",
      "                , loss2: 0.1420161485671997\n",
      "=================================\n",
      "in 1320 epoch, average loss: -4.747513961791992\n",
      "                , loss1: -3283.17421875\n",
      "                , loss2: 0.30364944934844973\n",
      "=================================\n",
      "in 1330 epoch, average loss: -4.681001281738281\n",
      "                , loss1: -3368.4125\n",
      "                , loss2: 0.5013011455535888\n",
      "=================================\n",
      "in 1340 epoch, average loss: -4.929392623901367\n",
      "                , loss1: -3443.848046875\n",
      "                , loss2: 0.36896724700927735\n",
      "=================================\n",
      "in 1350 epoch, average loss: -5.001768493652344\n",
      "                , loss1: -3544.7140625\n",
      "                , loss2: 0.45177392959594725\n",
      "=================================\n",
      "in 1360 epoch, average loss: -5.389300537109375\n",
      "                , loss1: -3654.146484375\n",
      "                , loss2: 0.23260347843170165\n",
      "=================================\n",
      "in 1370 epoch, average loss: -5.681603622436524\n",
      "                , loss1: -3771.57109375\n",
      "                , loss2: 0.12095793485641479\n",
      "=================================\n",
      "in 1380 epoch, average loss: -5.346500396728516\n",
      "                , loss1: -3880.465234375\n",
      "                , loss2: 0.6235957622528077\n",
      "=================================\n",
      "in 1390 epoch, average loss: -4.6275993347167965\n",
      "                , loss1: -3907.93359375\n",
      "                , loss2: 1.3847559928894042\n",
      "=================================\n",
      "in 1400 epoch, average loss: -4.840882873535156\n",
      "                , loss1: -3875.548046875\n",
      "                , loss2: 1.1216483116149902\n",
      "=================================\n",
      "in 1410 epoch, average loss: -5.318857192993164\n",
      "                , loss1: -3893.2046875\n",
      "                , loss2: 0.6708376407623291\n",
      "=================================\n",
      "in 1420 epoch, average loss: -5.783248519897461\n",
      "                , loss1: -3956.405859375\n",
      "                , loss2: 0.30368149280548096\n",
      "=================================\n",
      "in 1430 epoch, average loss: -5.973768997192383\n",
      "                , loss1: -4033.96171875\n",
      "                , loss2: 0.2324812412261963\n",
      "=================================\n",
      "in 1440 epoch, average loss: -6.191380310058594\n",
      "                , loss1: -4113.7171875\n",
      "                , loss2: 0.13757412433624266\n",
      "=================================\n",
      "in 1450 epoch, average loss: -6.259724426269531\n",
      "                , loss1: -4180.631640625\n",
      "                , loss2: 0.17217718362808226\n",
      "=================================\n",
      "in 1460 epoch, average loss: -6.117238998413086\n",
      "                , loss1: -4231.57890625\n",
      "                , loss2: 0.39304494857788086\n",
      "=================================\n",
      "in 1470 epoch, average loss: -6.469795227050781\n",
      "                , loss1: -4266.566796875\n",
      "                , loss2: 0.09431691765785218\n",
      "=================================\n",
      "in 1480 epoch, average loss: -6.366520690917969\n",
      "                , loss1: -4301.643359375\n",
      "                , loss2: 0.2515580177307129\n",
      "=================================\n",
      "in 1490 epoch, average loss: -6.433250427246094\n",
      "                , loss1: -4322.041796875\n",
      "                , loss2: 0.2162097692489624\n",
      "=================================\n",
      "in 1500 epoch, average loss: -6.24766960144043\n",
      "                , loss1: -4340.61875\n",
      "                , loss2: 0.4303723335266113\n",
      "=================================\n",
      "in 1510 epoch, average loss: -6.434368133544922\n",
      "                , loss1: -4352.808203125\n",
      "                , loss2: 0.26242685317993164\n",
      "=================================\n",
      "in 1520 epoch, average loss: -6.429313659667969\n",
      "                , loss1: -4361.74609375\n",
      "                , loss2: 0.28123290538787843\n",
      "=================================\n",
      "in 1530 epoch, average loss: -6.616220092773437\n",
      "                , loss1: -4377.433984375\n",
      "                , loss2: 0.11846212148666382\n",
      "=================================\n",
      "in 1540 epoch, average loss: -6.546939086914063\n",
      "                , loss1: -4388.3484375\n",
      "                , loss2: 0.20453429222106934\n",
      "=================================\n",
      "in 1550 epoch, average loss: -6.541414642333985\n",
      "                , loss1: -4399.804296875\n",
      "                , loss2: 0.22768492698669435\n",
      "=================================\n",
      "in 1560 epoch, average loss: -6.364494323730469\n",
      "                , loss1: -4406.68828125\n",
      "                , loss2: 0.4151957035064697\n",
      "=================================\n",
      "in 1570 epoch, average loss: -6.425482940673828\n",
      "                , loss1: -4406.711328125\n",
      "                , loss2: 0.35424184799194336\n",
      "=================================\n",
      "in 1580 epoch, average loss: -6.543363189697265\n",
      "                , loss1: -4411.319921875\n",
      "                , loss2: 0.24345240592956544\n",
      "=================================\n",
      "in 1590 epoch, average loss: -6.466957092285156\n",
      "                , loss1: -4414.08984375\n",
      "                , loss2: 0.3241199254989624\n",
      "=================================\n",
      "in 1600 epoch, average loss: -6.485250854492188\n",
      "                , loss1: -4419.946484375\n",
      "                , loss2: 0.31483778953552244\n",
      "=================================\n",
      "in 1610 epoch, average loss: -6.595645904541016\n",
      "                , loss1: -4424.671484375\n",
      "                , loss2: 0.21171081066131592\n",
      "=================================\n",
      "in 1620 epoch, average loss: -6.7102210998535154\n",
      "                , loss1: -4432.47890625\n",
      "                , loss2: 0.10914711952209473\n",
      "=================================\n",
      "in 1630 epoch, average loss: -6.682915496826172\n",
      "                , loss1: -4441.3265625\n",
      "                , loss2: 0.15006495714187623\n",
      "=================================\n",
      "in 1640 epoch, average loss: -6.620046997070313\n",
      "                , loss1: -4448.9453125\n",
      "                , loss2: 0.2246548891067505\n",
      "=================================\n",
      "in 1650 epoch, average loss: -6.655526733398437\n",
      "                , loss1: -4455.4734375\n",
      "                , loss2: 0.19921883344650268\n",
      "=================================\n",
      "in 1660 epoch, average loss: -6.6977386474609375\n",
      "                , loss1: -4459.943359375\n",
      "                , loss2: 0.16388328075408937\n",
      "=================================\n",
      "in 1670 epoch, average loss: -6.670893859863281\n",
      "                , loss1: -4463.26328125\n",
      "                , loss2: 0.19583691358566285\n",
      "=================================\n",
      "in 1680 epoch, average loss: -6.610918426513672\n",
      "                , loss1: -4467.4015625\n",
      "                , loss2: 0.26217803955078123\n",
      "=================================\n",
      "in 1690 epoch, average loss: -6.756092071533203\n",
      "                , loss1: -4467.104296875\n",
      "                , loss2: 0.11654728651046753\n",
      "=================================\n",
      "in 1700 epoch, average loss: -6.660575103759766\n",
      "                , loss1: -4471.9953125\n",
      "                , loss2: 0.2195894956588745\n",
      "=================================\n",
      "in 1710 epoch, average loss: -6.675800323486328\n",
      "                , loss1: -4474.094140625\n",
      "                , loss2: 0.20759336948394774\n",
      "=================================\n",
      "in 1720 epoch, average loss: -6.693525695800782\n",
      "                , loss1: -4476.657421875\n",
      "                , loss2: 0.19381177425384521\n",
      "=================================\n",
      "in 1730 epoch, average loss: -6.784555053710937\n",
      "                , loss1: -4481.81015625\n",
      "                , loss2: 0.11071038246154785\n",
      "=================================\n",
      "in 1740 epoch, average loss: -6.8081298828125\n",
      "                , loss1: -4490.612890625\n",
      "                , loss2: 0.1006775975227356\n",
      "=================================\n",
      "in 1750 epoch, average loss: -6.803534698486328\n",
      "                , loss1: -4495.3609375\n",
      "                , loss2: 0.11257698535919189\n",
      "=================================\n",
      "in 1760 epoch, average loss: -6.794252014160156\n",
      "                , loss1: -4499.828515625\n",
      "                , loss2: 0.12873369455337524\n",
      "=================================\n",
      "in 1770 epoch, average loss: -6.825489044189453\n",
      "                , loss1: -4506.778125\n",
      "                , loss2: 0.10818856954574585\n",
      "=================================\n",
      "in 1780 epoch, average loss: -6.795094299316406\n",
      "                , loss1: -4512.0734375\n",
      "                , loss2: 0.1467311143875122\n",
      "=================================\n",
      "in 1790 epoch, average loss: -6.74810562133789\n",
      "                , loss1: -4515.45546875\n",
      "                , loss2: 0.19892255067825318\n",
      "=================================\n",
      "in 1800 epoch, average loss: -6.776449584960938\n",
      "                , loss1: -4516.49453125\n",
      "                , loss2: 0.17217702865600587\n",
      "=================================\n",
      "in 1810 epoch, average loss: -6.831682586669922\n",
      "                , loss1: -4519.22890625\n",
      "                , loss2: 0.12115076780319214\n",
      "=================================\n",
      "in 1820 epoch, average loss: -6.851471710205078\n",
      "                , loss1: -4522.47109375\n",
      "                , loss2: 0.10634852647781372\n",
      "=================================\n",
      "in 1830 epoch, average loss: -6.901974487304687\n",
      "                , loss1: -4524.803515625\n",
      "                , loss2: 0.059435272216796876\n",
      "=================================\n",
      "in 1840 epoch, average loss: -6.851784515380859\n",
      "                , loss1: -4534.64140625\n",
      "                , loss2: 0.12476074695587158\n",
      "=================================\n",
      "in 1850 epoch, average loss: -6.920769500732422\n",
      "                , loss1: -4539.9859375\n",
      "                , loss2: 0.06399871110916137\n",
      "=================================\n",
      "in 1860 epoch, average loss: -6.895475769042969\n",
      "                , loss1: -4546.862109375\n",
      "                , loss2: 0.09987244606018067\n",
      "=================================\n",
      "in 1870 epoch, average loss: -6.924538421630859\n",
      "                , loss1: -4552.438671875\n",
      "                , loss2: 0.0793882131576538\n",
      "=================================\n",
      "in 1880 epoch, average loss: -6.856739807128906\n",
      "                , loss1: -4559.775390625\n",
      "                , loss2: 0.1584743618965149\n",
      "=================================\n",
      "in 1890 epoch, average loss: -6.834649658203125\n",
      "                , loss1: -4560.6984375\n",
      "                , loss2: 0.18198422193527222\n",
      "=================================\n",
      "in 1900 epoch, average loss: -6.884102630615234\n",
      "                , loss1: -4562.1984375\n",
      "                , loss2: 0.1348395824432373\n",
      "=================================\n",
      "in 1910 epoch, average loss: -6.902003479003906\n",
      "                , loss1: -4563.721875\n",
      "                , loss2: 0.11928268671035766\n",
      "=================================\n",
      "in 1920 epoch, average loss: -6.923649597167969\n",
      "                , loss1: -4567.823046875\n",
      "                , loss2: 0.10394501686096191\n",
      "=================================\n",
      "in 1930 epoch, average loss: -6.729163360595703\n",
      "                , loss1: -4570.858984375\n",
      "                , loss2: 0.3031033515930176\n",
      "=================================\n",
      "in 1940 epoch, average loss: -6.766446685791015\n",
      "                , loss1: -4566.91171875\n",
      "                , loss2: 0.25974745750427247\n",
      "=================================\n",
      "in 1950 epoch, average loss: -6.740421295166016\n",
      "                , loss1: -4564.0546875\n",
      "                , loss2: 0.2813772439956665\n",
      "=================================\n",
      "in 1960 epoch, average loss: -6.759919738769531\n",
      "                , loss1: -4560.9\n",
      "                , loss2: 0.25702457427978515\n",
      "=================================\n",
      "in 1970 epoch, average loss: -6.671395874023437\n",
      "                , loss1: -4557.946875\n",
      "                , loss2: 0.3410043239593506\n",
      "=================================\n",
      "in 1980 epoch, average loss: -6.856147766113281\n",
      "                , loss1: -4556.927734375\n",
      "                , loss2: 0.15468548536300658\n",
      "=================================\n",
      "in 1990 epoch, average loss: -6.830420684814453\n",
      "                , loss1: -4558.4078125\n",
      "                , loss2: 0.18268948793411255\n",
      "=================================\n",
      "in 2000 epoch, average loss: -6.823038482666016\n",
      "                , loss1: -4562.158984375\n",
      "                , loss2: 0.1958429217338562\n",
      "=================================\n",
      "in 2010 epoch, average loss: -6.836717224121093\n",
      "                , loss1: -4563.125390625\n",
      "                , loss2: 0.18365107774734496\n",
      "=================================\n",
      "in 2020 epoch, average loss: -6.83824462890625\n",
      "                , loss1: -4570.380078125\n",
      "                , loss2: 0.1932855725288391\n",
      "=================================\n",
      "in 2030 epoch, average loss: -6.922148895263672\n",
      "                , loss1: -4573.24296875\n",
      "                , loss2: 0.11378567218780518\n",
      "=================================\n",
      "in 2040 epoch, average loss: -6.89764175415039\n",
      "                , loss1: -4578.163671875\n",
      "                , loss2: 0.14586265087127687\n",
      "=================================\n",
      "in 2050 epoch, average loss: -6.802371215820313\n",
      "                , loss1: -4584.89921875\n",
      "                , loss2: 0.25149686336517335\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m      4\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.002\u001b[39m\n\u001b[0;32m----> 5\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      7\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     41\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     42\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(\n\u001b[1;32m     43\u001b[0m     outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight >= 2:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.002\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
