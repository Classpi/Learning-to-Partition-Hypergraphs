{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 4\n",
    "#1304\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer131\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":4, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7818, 327)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/high\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): ReLU()\n",
       "  (19): Dropout(p=0.05, inplace=False)\n",
       "  (20): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): ReLU()\n",
       "  (23): Dropout(p=0.05, inplace=False)\n",
       "  (24): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU()\n",
       "  (27): Dropout(p=0.05, inplace=False)\n",
       "  (28): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (29): ReLU()\n",
       "  (30): Dropout(p=0.05, inplace=False)\n",
       "  (31): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (32): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -221.9353271484375\n",
      "                , loss1: -39.26259155273438\n",
      "                , loss2: 13.247575378417968\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: -3834.69921875\n",
      "                , loss1: -655.123974609375\n",
      "                , loss2: 47.022467041015624\n",
      "=================================\n",
      "in 20 epoch, average loss: -8098.18125\n",
      "                , loss1: -1401.928125\n",
      "                , loss2: 77.10936889648437\n",
      "=================================\n",
      "in 30 epoch, average loss: -11350.41796875\n",
      "                , loss1: -2028.96875\n",
      "                , loss2: 277.48310546875\n",
      "=================================\n",
      "in 40 epoch, average loss: -16791.0234375\n",
      "                , loss1: -3138.435546875\n",
      "                , loss2: 888.25546875\n",
      "=================================\n",
      "in 50 epoch, average loss: -19838.4296875\n",
      "                , loss1: -3781.52890625\n",
      "                , loss2: 1083.0650390625\n",
      "=================================\n",
      "in 60 epoch, average loss: -24396.8\n",
      "                , loss1: -4778.5234375\n",
      "                , loss2: 1569.9560546875\n",
      "=================================\n",
      "in 70 epoch, average loss: -26020.0890625\n",
      "                , loss1: -5115.4890625\n",
      "                , loss2: 1269.17314453125\n",
      "=================================\n",
      "in 80 epoch, average loss: -29348.740625\n",
      "                , loss1: -5949.801953125\n",
      "                , loss2: 1786.372265625\n",
      "=================================\n",
      "in 90 epoch, average loss: -32589.778125\n",
      "                , loss1: -6879.78515625\n",
      "                , loss2: 2734.1560546875\n",
      "=================================\n",
      "in 100 epoch, average loss: -32760.88125\n",
      "                , loss1: -7063.5484375\n",
      "                , loss2: 2803.675390625\n",
      "=================================\n",
      "in 110 epoch, average loss: -32315.975\n",
      "                , loss1: -7094.109375\n",
      "                , loss2: 2693.2744140625\n",
      "=================================\n",
      "in 120 epoch, average loss: -31733.921875\n",
      "                , loss1: -7110.346875\n",
      "                , loss2: 2644.495703125\n",
      "=================================\n",
      "in 130 epoch, average loss: -31079.6875\n",
      "                , loss1: -7119.4265625\n",
      "                , loss2: 2630.7115234375\n",
      "=================================\n",
      "in 140 epoch, average loss: -30395.253125\n",
      "                , loss1: -7125.340625\n",
      "                , loss2: 2630.646484375\n",
      "=================================\n",
      "in 150 epoch, average loss: -29703.190625\n",
      "                , loss1: -7129.63125\n",
      "                , loss2: 2629.6431640625\n",
      "=================================\n",
      "in 160 epoch, average loss: -29003.409375\n",
      "                , loss1: -7132.25390625\n",
      "                , loss2: 2628.126953125\n",
      "=================================\n",
      "in 170 epoch, average loss: -28302.159375\n",
      "                , loss1: -7135.01171875\n",
      "                , loss2: 2628.1005859375\n",
      "=================================\n",
      "in 180 epoch, average loss: -27594.471875\n",
      "                , loss1: -7136.2515625\n",
      "                , loss2: 2627.552734375\n",
      "=================================\n",
      "in 190 epoch, average loss: -26890.203125\n",
      "                , loss1: -7138.1875\n",
      "                , loss2: 2626.187109375\n",
      "=================================\n",
      "in 200 epoch, average loss: -26180.4\n",
      "                , loss1: -7139.2984375\n",
      "                , loss2: 2626.6591796875\n",
      "=================================\n",
      "in 210 epoch, average loss: -25468.7171875\n",
      "                , loss1: -7139.965625\n",
      "                , loss2: 2627.0451171875\n",
      "=================================\n",
      "in 220 epoch, average loss: -24757.9921875\n",
      "                , loss1: -7140.66171875\n",
      "                , loss2: 2626.4318359375\n",
      "=================================\n",
      "in 230 epoch, average loss: -24047.8828125\n",
      "                , loss1: -7141.559375\n",
      "                , loss2: 2625.838671875\n",
      "=================================\n",
      "in 240 epoch, average loss: -23334.896875\n",
      "                , loss1: -7141.98125\n",
      "                , loss2: 2626.1955078125\n",
      "=================================\n",
      "in 250 epoch, average loss: -22623.421875\n",
      "                , loss1: -7142.6625\n",
      "                , loss2: 2625.88515625\n",
      "=================================\n",
      "in 260 epoch, average loss: -21911.4515625\n",
      "                , loss1: -7143.20625\n",
      "                , loss2: 2625.4537109375\n",
      "=================================\n",
      "in 270 epoch, average loss: -21197.6515625\n",
      "                , loss1: -7143.459375\n",
      "                , loss2: 2625.784765625\n",
      "=================================\n",
      "in 280 epoch, average loss: -20484.7796875\n",
      "                , loss1: -7143.8515625\n",
      "                , loss2: 2625.5845703125\n",
      "=================================\n",
      "in 290 epoch, average loss: -19772.15625\n",
      "                , loss1: -7144.34140625\n",
      "                , loss2: 2625.3537109375\n",
      "=================================\n",
      "in 300 epoch, average loss: -19058.2671875\n",
      "                , loss1: -7144.55625\n",
      "                , loss2: 2625.46484375\n",
      "=================================\n",
      "in 310 epoch, average loss: -18344.484375\n",
      "                , loss1: -7144.73359375\n",
      "                , loss2: 2625.3107421875\n",
      "=================================\n",
      "in 320 epoch, average loss: -17630.76875\n",
      "                , loss1: -7145.0125\n",
      "                , loss2: 2625.3416015625\n",
      "=================================\n",
      "in 330 epoch, average loss: -16916.7875\n",
      "                , loss1: -7145.2265625\n",
      "                , loss2: 2625.404296875\n",
      "=================================\n",
      "in 340 epoch, average loss: -16201.9203125\n",
      "                , loss1: -7145.0796875\n",
      "                , loss2: 2625.364453125\n",
      "=================================\n",
      "in 350 epoch, average loss: -15488.88125\n",
      "                , loss1: -7145.56953125\n",
      "                , loss2: 2625.1390625\n",
      "=================================\n",
      "in 360 epoch, average loss: -14774.5234375\n",
      "                , loss1: -7145.73203125\n",
      "                , loss2: 2625.335546875\n",
      "=================================\n",
      "in 370 epoch, average loss: -14060.3078125\n",
      "                , loss1: -7145.97578125\n",
      "                , loss2: 2625.54140625\n",
      "=================================\n",
      "in 380 epoch, average loss: -13346.55\n",
      "                , loss1: -7146.06796875\n",
      "                , loss2: 2624.9123046875\n",
      "=================================\n",
      "in 390 epoch, average loss: -12632.34375\n",
      "                , loss1: -7146.2953125\n",
      "                , loss2: 2624.9984375\n",
      "=================================\n",
      "in 400 epoch, average loss: -11917.9234375\n",
      "                , loss1: -7146.37890625\n",
      "                , loss2: 2624.96015625\n",
      "=================================\n",
      "in 410 epoch, average loss: -11203.6765625\n",
      "                , loss1: -7146.553125\n",
      "                , loss2: 2624.9064453125\n",
      "=================================\n",
      "in 420 epoch, average loss: -10488.81015625\n",
      "                , loss1: -7146.475\n",
      "                , loss2: 2624.9736328125\n",
      "=================================\n",
      "in 430 epoch, average loss: -9774.2671875\n",
      "                , loss1: -7146.46953125\n",
      "                , loss2: 2624.8560546875\n",
      "=================================\n",
      "in 440 epoch, average loss: -9060.11640625\n",
      "                , loss1: -7146.6890625\n",
      "                , loss2: 2624.716796875\n",
      "=================================\n",
      "in 450 epoch, average loss: -8345.20625\n",
      "                , loss1: -7146.73984375\n",
      "                , loss2: 2625.040234375\n",
      "=================================\n",
      "in 460 epoch, average loss: -7630.9484375\n",
      "                , loss1: -7146.79765625\n",
      "                , loss2: 2624.705859375\n",
      "=================================\n",
      "in 470 epoch, average loss: -6916.3890625\n",
      "                , loss1: -7146.76171875\n",
      "                , loss2: 2624.53984375\n",
      "=================================\n",
      "in 480 epoch, average loss: -6201.806640625\n",
      "                , loss1: -7146.740625\n",
      "                , loss2: 2624.4197265625\n",
      "=================================\n",
      "in 490 epoch, average loss: -5556.29375\n",
      "                , loss1: -7121.8390625\n",
      "                , loss2: 2527.8556640625\n",
      "=================================\n",
      "in 500 epoch, average loss: -5048.5515625\n",
      "                , loss1: -7068.525\n",
      "                , loss2: 2268.1859375\n",
      "=================================\n",
      "in 510 epoch, average loss: -5857.4953125\n",
      "                , loss1: -6958.0375\n",
      "                , loss2: 647.717431640625\n",
      "=================================\n",
      "in 520 epoch, average loss: -5222.428125\n",
      "                , loss1: -6993.22734375\n",
      "                , loss2: 616.831005859375\n",
      "=================================\n",
      "in 530 epoch, average loss: -4532.430859375\n",
      "                , loss1: -6998.79921875\n",
      "                , loss2: 611.673095703125\n",
      "=================================\n",
      "in 540 epoch, average loss: -3834.81015625\n",
      "                , loss1: -6999.77890625\n",
      "                , loss2: 610.052099609375\n",
      "=================================\n",
      "in 550 epoch, average loss: -3135.6998046875\n",
      "                , loss1: -7000.07265625\n",
      "                , loss2: 609.33603515625\n",
      "=================================\n",
      "in 560 epoch, average loss: -2436.067578125\n",
      "                , loss1: -7000.43984375\n",
      "                , loss2: 609.11826171875\n",
      "=================================\n",
      "in 570 epoch, average loss: -1736.64453125\n",
      "                , loss1: -7000.89921875\n",
      "                , loss2: 608.660302734375\n",
      "=================================\n",
      "in 580 epoch, average loss: -1036.45830078125\n",
      "                , loss1: -7001.05625\n",
      "                , loss2: 608.778515625\n",
      "=================================\n",
      "in 590 epoch, average loss: -336.71181640625\n",
      "                , loss1: -7001.2859375\n",
      "                , loss2: 608.4640625\n",
      "=================================\n",
      "in 600 epoch, average loss: 258.16875\n",
      "                , loss1: -7001.00390625\n",
      "                , loss2: 608.22060546875\n",
      "=================================\n",
      "in 610 epoch, average loss: 328.0271728515625\n",
      "                , loss1: -7000.96328125\n",
      "                , loss2: 608.065673828125\n",
      "=================================\n",
      "in 620 epoch, average loss: 135.5447509765625\n",
      "                , loss1: -6850.63125\n",
      "                , loss2: 409.57001953125\n",
      "=================================\n",
      "in 630 epoch, average loss: -179.15308837890626\n",
      "                , loss1: -6660.46953125\n",
      "                , loss2: 87.265673828125\n",
      "=================================\n",
      "in 640 epoch, average loss: -197.30478515625\n",
      "                , loss1: -6719.4125\n",
      "                , loss2: 71.47169189453125\n",
      "=================================\n",
      "in 650 epoch, average loss: -201.07890625\n",
      "                , loss1: -6727.203125\n",
      "                , loss2: 68.009228515625\n",
      "=================================\n",
      "in 660 epoch, average loss: -201.42076416015624\n",
      "                , loss1: -6730.53125\n",
      "                , loss2: 67.80048828125\n",
      "=================================\n",
      "in 670 epoch, average loss: -201.6457763671875\n",
      "                , loss1: -6731.92734375\n",
      "                , loss2: 67.6313232421875\n",
      "=================================\n",
      "in 680 epoch, average loss: -201.7469970703125\n",
      "                , loss1: -6734.0171875\n",
      "                , loss2: 67.6136962890625\n",
      "=================================\n",
      "in 690 epoch, average loss: -201.93690185546876\n",
      "                , loss1: -6739.4703125\n",
      "                , loss2: 67.64189453125\n",
      "=================================\n",
      "in 700 epoch, average loss: -202.14852294921874\n",
      "                , loss1: -6746.52265625\n",
      "                , loss2: 67.7123779296875\n",
      "=================================\n",
      "in 710 epoch, average loss: -202.3445556640625\n",
      "                , loss1: -6748.87734375\n",
      "                , loss2: 67.6105224609375\n",
      "=================================\n",
      "in 720 epoch, average loss: -202.335595703125\n",
      "                , loss1: -6748.925\n",
      "                , loss2: 67.6214111328125\n",
      "=================================\n",
      "in 730 epoch, average loss: -202.4119873046875\n",
      "                , loss1: -6749.9359375\n",
      "                , loss2: 67.58543701171875\n",
      "=================================\n",
      "in 740 epoch, average loss: -202.4833251953125\n",
      "                , loss1: -6750.3859375\n",
      "                , loss2: 67.53212890625\n",
      "=================================\n",
      "in 750 epoch, average loss: -202.505029296875\n",
      "                , loss1: -6750.5328125\n",
      "                , loss2: 67.51624755859375\n",
      "=================================\n",
      "in 760 epoch, average loss: -202.53516845703126\n",
      "                , loss1: -6751.27265625\n",
      "                , loss2: 67.5157470703125\n",
      "=================================\n",
      "in 770 epoch, average loss: -202.6443115234375\n",
      "                , loss1: -6752.25546875\n",
      "                , loss2: 67.44588012695313\n",
      "=================================\n",
      "in 780 epoch, average loss: -202.5035400390625\n",
      "                , loss1: -6750.55859375\n",
      "                , loss2: 67.51882934570312\n",
      "=================================\n",
      "in 790 epoch, average loss: -202.6798583984375\n",
      "                , loss1: -6752.54453125\n",
      "                , loss2: 67.42190551757812\n",
      "=================================\n",
      "in 800 epoch, average loss: -202.726513671875\n",
      "                , loss1: -6753.2125\n",
      "                , loss2: 67.40198974609375\n",
      "=================================\n",
      "in 810 epoch, average loss: -202.76201171875\n",
      "                , loss1: -6753.365625\n",
      "                , loss2: 67.372607421875\n",
      "=================================\n",
      "in 820 epoch, average loss: -202.7779052734375\n",
      "                , loss1: -6753.50859375\n",
      "                , loss2: 67.36242065429687\n",
      "=================================\n",
      "in 830 epoch, average loss: -202.8628662109375\n",
      "                , loss1: -6754.665625\n",
      "                , loss2: 67.32379150390625\n",
      "=================================\n",
      "in 840 epoch, average loss: -202.82308349609374\n",
      "                , loss1: -6754.359375\n",
      "                , loss2: 67.35128173828124\n",
      "=================================\n",
      "in 850 epoch, average loss: -202.80469970703126\n",
      "                , loss1: -6753.78984375\n",
      "                , loss2: 67.34686889648438\n",
      "=================================\n",
      "in 860 epoch, average loss: -202.8234375\n",
      "                , loss1: -6754.0328125\n",
      "                , loss2: 67.3378662109375\n",
      "=================================\n",
      "in 870 epoch, average loss: -202.8589599609375\n",
      "                , loss1: -6754.4671875\n",
      "                , loss2: 67.31968383789062\n",
      "=================================\n",
      "in 880 epoch, average loss: -202.91724853515626\n",
      "                , loss1: -6755.3359375\n",
      "                , loss2: 67.29620361328125\n",
      "=================================\n",
      "in 890 epoch, average loss: -202.92969970703126\n",
      "                , loss1: -6755.15390625\n",
      "                , loss2: 67.27644653320313\n",
      "=================================\n",
      "in 900 epoch, average loss: -202.9719482421875\n",
      "                , loss1: -6755.8109375\n",
      "                , loss2: 67.26048583984375\n",
      "=================================\n",
      "in 910 epoch, average loss: -202.96171875\n",
      "                , loss1: -6755.6078125\n",
      "                , loss2: 67.26257934570313\n",
      "=================================\n",
      "in 920 epoch, average loss: -203.01722412109376\n",
      "                , loss1: -6756.33828125\n",
      "                , loss2: 67.23628540039063\n",
      "=================================\n",
      "in 930 epoch, average loss: -203.034619140625\n",
      "                , loss1: -6756.42265625\n",
      "                , loss2: 67.22227783203125\n",
      "=================================\n",
      "in 940 epoch, average loss: -202.9626953125\n",
      "                , loss1: -6755.38984375\n",
      "                , loss2: 67.25286254882812\n",
      "=================================\n",
      "in 950 epoch, average loss: -203.0517333984375\n",
      "                , loss1: -6756.3984375\n",
      "                , loss2: 67.20421752929687\n",
      "=================================\n",
      "in 960 epoch, average loss: -203.060693359375\n",
      "                , loss1: -6756.5125\n",
      "                , loss2: 67.19981079101562\n",
      "=================================\n",
      "in 970 epoch, average loss: -203.07896728515624\n",
      "                , loss1: -6756.80390625\n",
      "                , loss2: 67.19315795898437\n",
      "=================================\n",
      "in 980 epoch, average loss: -203.0454833984375\n",
      "                , loss1: -6756.3234375\n",
      "                , loss2: 67.20747680664063\n",
      "=================================\n",
      "in 990 epoch, average loss: -203.1087890625\n",
      "                , loss1: -6757.171875\n",
      "                , loss2: 67.1781005859375\n",
      "=================================\n",
      "in 1000 epoch, average loss: -203.0909423828125\n",
      "                , loss1: -6756.64765625\n",
      "                , loss2: 67.17496948242187\n",
      "=================================\n",
      "in 1010 epoch, average loss: -203.0974609375\n",
      "                , loss1: -6756.8296875\n",
      "                , loss2: 67.17573852539063\n",
      "=================================\n",
      "in 1020 epoch, average loss: -203.225634765625\n",
      "                , loss1: -6757.13359375\n",
      "                , loss2: 67.05970458984375\n",
      "=================================\n",
      "in 1030 epoch, average loss: -215.422607421875\n",
      "                , loss1: -6739.0921875\n",
      "                , loss2: 54.141082763671875\n",
      "=================================\n",
      "in 1040 epoch, average loss: -245.329150390625\n",
      "                , loss1: -6655.14140625\n",
      "                , loss2: 20.876507568359376\n",
      "=================================\n",
      "in 1050 epoch, average loss: -248.9362060546875\n",
      "                , loss1: -6667.234375\n",
      "                , loss2: 17.753199768066406\n",
      "=================================\n",
      "in 1060 epoch, average loss: -254.7342041015625\n",
      "                , loss1: -6669.275\n",
      "                , loss2: 12.036788940429688\n",
      "=================================\n",
      "in 1070 epoch, average loss: -258.39228515625\n",
      "                , loss1: -6650.6015625\n",
      "                , loss2: 7.631784057617187\n",
      "=================================\n",
      "in 1080 epoch, average loss: -259.1936279296875\n",
      "                , loss1: -6639.3265625\n",
      "                , loss2: 6.379423904418945\n",
      "=================================\n",
      "in 1090 epoch, average loss: -259.549365234375\n",
      "                , loss1: -6653.59296875\n",
      "                , loss2: 6.594373321533203\n",
      "=================================\n",
      "in 1100 epoch, average loss: -259.6501953125\n",
      "                , loss1: -6657.9265625\n",
      "                , loss2: 6.666873931884766\n",
      "=================================\n",
      "in 1110 epoch, average loss: -259.678662109375\n",
      "                , loss1: -6659.14453125\n",
      "                , loss2: 6.687117004394532\n",
      "=================================\n",
      "in 1120 epoch, average loss: -259.6772216796875\n",
      "                , loss1: -6658.284375\n",
      "                , loss2: 6.654146575927735\n",
      "=================================\n",
      "in 1130 epoch, average loss: -259.6499267578125\n",
      "                , loss1: -6658.9546875\n",
      "                , loss2: 6.70825424194336\n",
      "=================================\n",
      "in 1140 epoch, average loss: -259.7168701171875\n",
      "                , loss1: -6659.771875\n",
      "                , loss2: 6.673983764648438\n",
      "=================================\n",
      "in 1150 epoch, average loss: -259.69384765625\n",
      "                , loss1: -6659.2625\n",
      "                , loss2: 6.676625061035156\n",
      "=================================\n",
      "in 1160 epoch, average loss: -259.6888427734375\n",
      "                , loss1: -6658.67578125\n",
      "                , loss2: 6.658189392089843\n",
      "=================================\n",
      "in 1170 epoch, average loss: -259.7221435546875\n",
      "                , loss1: -6658.925\n",
      "                , loss2: 6.634844970703125\n",
      "=================================\n",
      "in 1180 epoch, average loss: -259.70947265625\n",
      "                , loss1: -6658.9296875\n",
      "                , loss2: 6.647704315185547\n",
      "=================================\n",
      "in 1190 epoch, average loss: -259.7159912109375\n",
      "                , loss1: -6659.20703125\n",
      "                , loss2: 6.65228042602539\n",
      "=================================\n",
      "in 1200 epoch, average loss: -259.720458984375\n",
      "                , loss1: -6658.5140625\n",
      "                , loss2: 6.620082092285156\n",
      "=================================\n",
      "in 1210 epoch, average loss: -259.7173828125\n",
      "                , loss1: -6658.49296875\n",
      "                , loss2: 6.6223808288574215\n",
      "=================================\n",
      "in 1220 epoch, average loss: -259.7539794921875\n",
      "                , loss1: -6659.82109375\n",
      "                , loss2: 6.638882446289062\n",
      "=================================\n",
      "in 1230 epoch, average loss: -259.730517578125\n",
      "                , loss1: -6658.9703125\n",
      "                , loss2: 6.628295135498047\n",
      "=================================\n",
      "in 1240 epoch, average loss: -259.7340087890625\n",
      "                , loss1: -6658.83046875\n",
      "                , loss2: 6.619223785400391\n",
      "=================================\n",
      "in 1250 epoch, average loss: -259.7502685546875\n",
      "                , loss1: -6659.59609375\n",
      "                , loss2: 6.633588409423828\n",
      "=================================\n",
      "in 1260 epoch, average loss: -259.7404052734375\n",
      "                , loss1: -6658.41171875\n",
      "                , loss2: 6.596049499511719\n",
      "=================================\n",
      "in 1270 epoch, average loss: -259.784033203125\n",
      "                , loss1: -6659.44921875\n",
      "                , loss2: 6.5939178466796875\n",
      "=================================\n",
      "in 1280 epoch, average loss: -259.7677978515625\n",
      "                , loss1: -6658.97890625\n",
      "                , loss2: 6.591361236572266\n",
      "=================================\n",
      "in 1290 epoch, average loss: -259.7818115234375\n",
      "                , loss1: -6657.4765625\n",
      "                , loss2: 6.517244720458985\n",
      "=================================\n",
      "in 1300 epoch, average loss: -259.6154296875\n",
      "                , loss1: -6657.95859375\n",
      "                , loss2: 6.702919006347656\n",
      "=================================\n",
      "in 1310 epoch, average loss: -259.754443359375\n",
      "                , loss1: -6658.36484375\n",
      "                , loss2: 6.580128479003906\n",
      "=================================\n",
      "in 1320 epoch, average loss: -259.790771484375\n",
      "                , loss1: -6659.32421875\n",
      "                , loss2: 6.582164764404297\n",
      "=================================\n",
      "in 1330 epoch, average loss: -259.8075927734375\n",
      "                , loss1: -6659.13828125\n",
      "                , loss2: 6.557899475097656\n",
      "=================================\n",
      "in 1340 epoch, average loss: -260.66689453125\n",
      "                , loss1: -6658.24375\n",
      "                , loss2: 5.662870788574219\n",
      "=================================\n",
      "in 1350 epoch, average loss: -262.7190673828125\n",
      "                , loss1: -6652.95234375\n",
      "                , loss2: 3.3990409851074217\n",
      "=================================\n",
      "in 1360 epoch, average loss: -264.1883544921875\n",
      "                , loss1: -6640.39765625\n",
      "                , loss2: 1.4275565147399902\n",
      "=================================\n",
      "in 1370 epoch, average loss: -264.90126953125\n",
      "                , loss1: -6628.66953125\n",
      "                , loss2: 0.24548723697662353\n",
      "=================================\n",
      "in 1380 epoch, average loss: -265.0251220703125\n",
      "                , loss1: -6630.81015625\n",
      "                , loss2: 0.20724208354949952\n",
      "=================================\n",
      "in 1390 epoch, average loss: -265.03408203125\n",
      "                , loss1: -6630.7421875\n",
      "                , loss2: 0.19562716484069825\n",
      "=================================\n",
      "in 1400 epoch, average loss: -265.0669189453125\n",
      "                , loss1: -6631.2\n",
      "                , loss2: 0.1810652494430542\n",
      "=================================\n",
      "in 1410 epoch, average loss: -265.08994140625\n",
      "                , loss1: -6631.9953125\n",
      "                , loss2: 0.18991900682449342\n",
      "=================================\n",
      "in 1420 epoch, average loss: -265.0939208984375\n",
      "                , loss1: -6631.59296875\n",
      "                , loss2: 0.16975276470184325\n",
      "=================================\n",
      "in 1430 epoch, average loss: -265.114892578125\n",
      "                , loss1: -6631.7328125\n",
      "                , loss2: 0.15441285371780394\n",
      "=================================\n",
      "in 1440 epoch, average loss: -265.1170654296875\n",
      "                , loss1: -6632.659375\n",
      "                , loss2: 0.1893252968788147\n",
      "=================================\n",
      "in 1450 epoch, average loss: -265.086669921875\n",
      "                , loss1: -6632.30234375\n",
      "                , loss2: 0.20540244579315187\n",
      "=================================\n",
      "in 1460 epoch, average loss: -265.157421875\n",
      "                , loss1: -6632.59375\n",
      "                , loss2: 0.14631116390228271\n",
      "=================================\n",
      "in 1470 epoch, average loss: -265.1534423828125\n",
      "                , loss1: -6632.5453125\n",
      "                , loss2: 0.14836499691009522\n",
      "=================================\n",
      "in 1480 epoch, average loss: -265.1540771484375\n",
      "                , loss1: -6633.4125\n",
      "                , loss2: 0.1824643135070801\n",
      "=================================\n",
      "in 1490 epoch, average loss: -265.138720703125\n",
      "                , loss1: -6632.7203125\n",
      "                , loss2: 0.1701122760772705\n",
      "=================================\n",
      "in 1500 epoch, average loss: -265.142919921875\n",
      "                , loss1: -6633.65703125\n",
      "                , loss2: 0.2033717393875122\n",
      "=================================\n",
      "in 1510 epoch, average loss: -265.148974609375\n",
      "                , loss1: -6632.9375\n",
      "                , loss2: 0.16849467754364014\n",
      "=================================\n",
      "in 1520 epoch, average loss: -265.1375244140625\n",
      "                , loss1: -6633.275\n",
      "                , loss2: 0.193505597114563\n",
      "=================================\n",
      "in 1530 epoch, average loss: -265.137060546875\n",
      "                , loss1: -6633.19765625\n",
      "                , loss2: 0.19084798097610473\n",
      "=================================\n",
      "in 1540 epoch, average loss: -265.172314453125\n",
      "                , loss1: -6633.778125\n",
      "                , loss2: 0.1787843108177185\n",
      "=================================\n",
      "in 1550 epoch, average loss: -265.2042724609375\n",
      "                , loss1: -6634.02109375\n",
      "                , loss2: 0.15655114650726318\n",
      "=================================\n",
      "in 1560 epoch, average loss: -265.1881591796875\n",
      "                , loss1: -6634.146875\n",
      "                , loss2: 0.17773783206939697\n",
      "=================================\n",
      "in 1570 epoch, average loss: -265.16103515625\n",
      "                , loss1: -6633.65078125\n",
      "                , loss2: 0.18500157594680786\n",
      "=================================\n",
      "in 1580 epoch, average loss: -265.199462890625\n",
      "                , loss1: -6634.36171875\n",
      "                , loss2: 0.17497161626815796\n",
      "=================================\n",
      "in 1590 epoch, average loss: -265.1667724609375\n",
      "                , loss1: -6633.8421875\n",
      "                , loss2: 0.18692842721939087\n",
      "=================================\n",
      "in 1600 epoch, average loss: -265.183203125\n",
      "                , loss1: -6634.06640625\n",
      "                , loss2: 0.17948044538497926\n",
      "=================================\n",
      "in 1610 epoch, average loss: -265.2218505859375\n",
      "                , loss1: -6634.8\n",
      "                , loss2: 0.17011449337005616\n",
      "=================================\n",
      "in 1620 epoch, average loss: -265.1682861328125\n",
      "                , loss1: -6634.371875\n",
      "                , loss2: 0.20654871463775634\n",
      "=================================\n",
      "in 1630 epoch, average loss: -265.204296875\n",
      "                , loss1: -6634.278125\n",
      "                , loss2: 0.1668335199356079\n",
      "=================================\n",
      "in 1640 epoch, average loss: -265.200732421875\n",
      "                , loss1: -6635.22578125\n",
      "                , loss2: 0.2082885503768921\n",
      "=================================\n",
      "in 1650 epoch, average loss: -265.2022216796875\n",
      "                , loss1: -6634.515625\n",
      "                , loss2: 0.17839841842651366\n",
      "=================================\n",
      "in 1660 epoch, average loss: -265.179638671875\n",
      "                , loss1: -6634.95\n",
      "                , loss2: 0.2183516502380371\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.044\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[76], line 30\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | 设置为训练模式\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[76], line 22\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     24\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition/examples/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1466\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_neg_1, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight > 0.044:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.01\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82., 82., 81., 82.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0030581039755351682"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
