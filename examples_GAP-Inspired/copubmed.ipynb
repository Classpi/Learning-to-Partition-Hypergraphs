{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycq/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import dhg\n",
    "from dhg import Hypergraph\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP\n",
    "from hgp.utils import from_pickle_to_hypergraph,from_file_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 3\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.25}\n",
    "#h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1536, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 1536, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 2048, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer34\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":16, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, H, H_degree,device):\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "\n",
    "    loss = 100 * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.H = self.hg.H.to_dense().to(DEVICE)\n",
    "        self.H_degree = torch.sum(self.H, dim=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train() \n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(\n",
    "            outs, self.H, self.H_degree, device=DEVICE\n",
    "        )\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dhg.data.CocitationPubmed()\n",
    "G = dhg.Hypergraph(G[\"num_vertices\"],G[\"edge_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "#X = torch.eye(n=G.num_v)\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=8e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -1183.530078125\n",
      "                , loss1: -42.131036376953126\n",
      "                , loss2: 3029.5734375\n",
      "=================================\n",
      "in 10 epoch, average loss: -40400.478125\n",
      "                , loss1: -424.30126953125\n",
      "                , loss2: 2029.6490234375\n",
      "=================================\n",
      "in 20 epoch, average loss: -42645.965625\n",
      "                , loss1: -427.01123046875\n",
      "                , loss2: 55.1591796875\n",
      "=================================\n",
      "in 30 epoch, average loss: -43523.7125\n",
      "                , loss1: -435.898046875\n",
      "                , loss2: 66.0960693359375\n",
      "=================================\n",
      "in 40 epoch, average loss: -45418.190625\n",
      "                , loss1: -456.101220703125\n",
      "                , loss2: 191.93011474609375\n",
      "=================================\n",
      "in 50 epoch, average loss: -53382.775\n",
      "                , loss1: -537.44453125\n",
      "                , loss2: 361.668115234375\n",
      "=================================\n",
      "in 60 epoch, average loss: -97494.6625\n",
      "                , loss1: -985.40732421875\n",
      "                , loss2: 1046.076953125\n",
      "=================================\n",
      "in 70 epoch, average loss: -176489.5\n",
      "                , loss1: -1815.8767578125\n",
      "                , loss2: 5098.183203125\n",
      "=================================\n",
      "in 80 epoch, average loss: -370378.525\n",
      "                , loss1: -3871.08359375\n",
      "                , loss2: 16729.840625\n",
      "=================================\n",
      "in 90 epoch, average loss: -546207.0\n",
      "                , loss1: -5653.004296875\n",
      "                , loss2: 19093.446875\n",
      "=================================\n",
      "in 100 epoch, average loss: -735788.4\n",
      "                , loss1: -7472.06171875\n",
      "                , loss2: 11417.80234375\n",
      "=================================\n",
      "in 110 epoch, average loss: -750184.15\n",
      "                , loss1: -7521.9078125\n",
      "                , loss2: 2006.7064453125\n",
      "=================================\n",
      "in 120 epoch, average loss: -752201.05\n",
      "                , loss1: -7527.33671875\n",
      "                , loss2: 532.595654296875\n",
      "=================================\n",
      "in 130 epoch, average loss: -752685.4\n",
      "                , loss1: -7528.96796875\n",
      "                , loss2: 211.4228759765625\n",
      "=================================\n",
      "in 140 epoch, average loss: -752690.3\n",
      "                , loss1: -7527.7671875\n",
      "                , loss2: 86.56744384765625\n",
      "=================================\n",
      "in 150 epoch, average loss: -752887.3\n",
      "                , loss1: -7529.2296875\n",
      "                , loss2: 35.752688598632815\n",
      "=================================\n",
      "in 160 epoch, average loss: -752886.9\n",
      "                , loss1: -7529.0390625\n",
      "                , loss2: 16.957743835449218\n",
      "=================================\n",
      "in 170 epoch, average loss: -752898.1\n",
      "                , loss1: -7529.15703125\n",
      "                , loss2: 17.546110534667967\n",
      "=================================\n",
      "in 180 epoch, average loss: -752852.3\n",
      "                , loss1: -7528.7640625\n",
      "                , loss2: 24.13042755126953\n",
      "=================================\n",
      "in 190 epoch, average loss: -752834.6\n",
      "                , loss1: -7528.90859375\n",
      "                , loss2: 56.24273681640625\n",
      "=================================\n",
      "in 200 epoch, average loss: -752931.15\n",
      "                , loss1: -7529.61953125\n",
      "                , loss2: 30.79891357421875\n",
      "=================================\n",
      "in 210 epoch, average loss: -752798.1\n",
      "                , loss1: -7528.14921875\n",
      "                , loss2: 16.85039825439453\n",
      "=================================\n",
      "in 220 epoch, average loss: -752847.05\n",
      "                , loss1: -7528.63828125\n",
      "                , loss2: 16.771141052246094\n",
      "=================================\n",
      "in 230 epoch, average loss: -752940.95\n",
      "                , loss1: -7529.61484375\n",
      "                , loss2: 20.562469482421875\n",
      "=================================\n",
      "in 240 epoch, average loss: -752797.8\n",
      "                , loss1: -7528.675\n",
      "                , loss2: 69.76359252929687\n",
      "=================================\n",
      "in 250 epoch, average loss: -752832.95\n",
      "                , loss1: -7529.215625\n",
      "                , loss2: 88.56964111328125\n",
      "=================================\n",
      "in 260 epoch, average loss: -752905.6\n",
      "                , loss1: -7529.54375\n",
      "                , loss2: 48.774530029296876\n",
      "=================================\n",
      "in 270 epoch, average loss: -752947.15\n",
      "                , loss1: -7529.65390625\n",
      "                , loss2: 18.241177368164063\n",
      "=================================\n",
      "in 280 epoch, average loss: -752949.3\n",
      "                , loss1: -7529.6703125\n",
      "                , loss2: 17.657945251464845\n",
      "=================================\n",
      "in 290 epoch, average loss: -752938.35\n",
      "                , loss1: -7529.621875\n",
      "                , loss2: 23.7724365234375\n",
      "=================================\n",
      "in 300 epoch, average loss: -752868.1\n",
      "                , loss1: -7528.7359375\n",
      "                , loss2: 5.495759582519531\n",
      "=================================\n",
      "in 310 epoch, average loss: -752928.75\n",
      "                , loss1: -7529.5796875\n",
      "                , loss2: 29.20784912109375\n",
      "=================================\n",
      "in 320 epoch, average loss: -752964.85\n",
      "                , loss1: -7529.771875\n",
      "                , loss2: 12.232321166992188\n",
      "=================================\n",
      "in 330 epoch, average loss: -752981.4\n",
      "                , loss1: -7530.26796875\n",
      "                , loss2: 45.40931091308594\n",
      "=================================\n",
      "in 340 epoch, average loss: -752965.95\n",
      "                , loss1: -7530.0\n",
      "                , loss2: 34.0827392578125\n",
      "=================================\n",
      "in 350 epoch, average loss: -752830.5\n",
      "                , loss1: -7528.5234375\n",
      "                , loss2: 21.773416137695314\n",
      "=================================\n",
      "in 360 epoch, average loss: -753023.75\n",
      "                , loss1: -7530.4625\n",
      "                , loss2: 22.566078186035156\n",
      "=================================\n",
      "in 370 epoch, average loss: -752964.35\n",
      "                , loss1: -7529.871875\n",
      "                , loss2: 22.895970153808594\n",
      "=================================\n",
      "in 380 epoch, average loss: -753040.15\n",
      "                , loss1: -7530.74375\n",
      "                , loss2: 34.304779052734375\n",
      "=================================\n",
      "in 390 epoch, average loss: -753066.1\n",
      "                , loss1: -7530.78515625\n",
      "                , loss2: 12.410509490966797\n",
      "=================================\n",
      "in 400 epoch, average loss: -753043.0\n",
      "                , loss1: -7530.5109375\n",
      "                , loss2: 8.032553100585938\n",
      "=================================\n",
      "in 410 epoch, average loss: -753064.2\n",
      "                , loss1: -7530.784375\n",
      "                , loss2: 14.279716491699219\n",
      "=================================\n",
      "in 420 epoch, average loss: -753061.4\n",
      "                , loss1: -7530.69375\n",
      "                , loss2: 8.034075164794922\n",
      "=================================\n",
      "in 430 epoch, average loss: -753024.3\n",
      "                , loss1: -7530.36484375\n",
      "                , loss2: 12.206618499755859\n",
      "=================================\n",
      "in 440 epoch, average loss: -753034.25\n",
      "                , loss1: -7530.75078125\n",
      "                , loss2: 40.8232421875\n",
      "=================================\n",
      "in 450 epoch, average loss: -753048.25\n",
      "                , loss1: -7530.84453125\n",
      "                , loss2: 36.2936279296875\n",
      "=================================\n",
      "in 460 epoch, average loss: -753041.05\n",
      "                , loss1: -7530.534375\n",
      "                , loss2: 12.345536041259766\n",
      "=================================\n",
      "in 470 epoch, average loss: -753071.4\n",
      "                , loss1: -7530.840625\n",
      "                , loss2: 12.613433074951171\n",
      "=================================\n",
      "in 480 epoch, average loss: -753061.5\n",
      "                , loss1: -7530.703125\n",
      "                , loss2: 8.739413452148437\n",
      "=================================\n",
      "in 490 epoch, average loss: -753059.15\n",
      "                , loss1: -7530.7046875\n",
      "                , loss2: 11.423783874511718\n",
      "=================================\n",
      "in 500 epoch, average loss: -753069.0\n",
      "                , loss1: -7530.84921875\n",
      "                , loss2: 15.993609619140624\n",
      "=================================\n",
      "in 510 epoch, average loss: -753051.45\n",
      "                , loss1: -7530.6859375\n",
      "                , loss2: 17.122515869140624\n",
      "=================================\n",
      "in 520 epoch, average loss: -753012.05\n",
      "                , loss1: -7530.53046875\n",
      "                , loss2: 41.04490356445312\n",
      "=================================\n",
      "in 530 epoch, average loss: -753060.6\n",
      "                , loss1: -7530.80546875\n",
      "                , loss2: 19.90961456298828\n",
      "=================================\n",
      "in 540 epoch, average loss: -753037.1\n",
      "                , loss1: -7530.565625\n",
      "                , loss2: 19.54630889892578\n",
      "=================================\n",
      "in 550 epoch, average loss: -753064.8\n",
      "                , loss1: -7530.75\n",
      "                , loss2: 10.157607269287109\n",
      "=================================\n",
      "in 560 epoch, average loss: -752942.45\n",
      "                , loss1: -7529.68046875\n",
      "                , loss2: 25.624081420898438\n",
      "=================================\n",
      "in 570 epoch, average loss: -753043.05\n",
      "                , loss1: -7530.8046875\n",
      "                , loss2: 37.411111450195314\n",
      "=================================\n",
      "in 580 epoch, average loss: -752997.1\n",
      "                , loss1: -7530.1640625\n",
      "                , loss2: 19.240634155273437\n",
      "=================================\n",
      "in 590 epoch, average loss: -753063.05\n",
      "                , loss1: -7530.83203125\n",
      "                , loss2: 20.14022216796875\n",
      "=================================\n",
      "in 600 epoch, average loss: -753058.25\n",
      "                , loss1: -7530.7359375\n",
      "                , loss2: 15.397157287597656\n",
      "=================================\n",
      "in 610 epoch, average loss: -753056.2\n",
      "                , loss1: -7530.80859375\n",
      "                , loss2: 24.619696044921874\n",
      "=================================\n",
      "in 620 epoch, average loss: -753041.6\n",
      "                , loss1: -7530.67421875\n",
      "                , loss2: 25.766970825195312\n",
      "=================================\n",
      "in 630 epoch, average loss: -753059.6\n",
      "                , loss1: -7530.7765625\n",
      "                , loss2: 18.11968536376953\n",
      "=================================\n",
      "in 640 epoch, average loss: -753038.0\n",
      "                , loss1: -7530.70859375\n",
      "                , loss2: 32.859246826171876\n",
      "=================================\n",
      "in 650 epoch, average loss: -752985.0\n",
      "                , loss1: -7530.834375\n",
      "                , loss2: 98.34139404296874\n",
      "=================================\n",
      "in 660 epoch, average loss: -753041.7\n",
      "                , loss1: -7530.58203125\n",
      "                , loss2: 16.503309631347655\n",
      "=================================\n",
      "in 670 epoch, average loss: -753068.4\n",
      "                , loss1: -7530.9078125\n",
      "                , loss2: 22.344322204589844\n",
      "=================================\n",
      "in 680 epoch, average loss: -753057.1\n",
      "                , loss1: -7530.7984375\n",
      "                , loss2: 22.744175720214844\n",
      "=================================\n",
      "in 690 epoch, average loss: -753058.6\n",
      "                , loss1: -7530.84921875\n",
      "                , loss2: 26.262545776367187\n",
      "=================================\n",
      "in 700 epoch, average loss: -753034.25\n",
      "                , loss1: -7530.7921875\n",
      "                , loss2: 44.9201904296875\n",
      "=================================\n",
      "in 710 epoch, average loss: -753055.05\n",
      "                , loss1: -7530.8640625\n",
      "                , loss2: 31.457275390625\n",
      "=================================\n",
      "in 720 epoch, average loss: -753061.4\n",
      "                , loss1: -7530.8546875\n",
      "                , loss2: 24.027496337890625\n",
      "=================================\n",
      "in 730 epoch, average loss: -753051.1\n",
      "                , loss1: -7530.76328125\n",
      "                , loss2: 25.229704284667967\n",
      "=================================\n",
      "in 740 epoch, average loss: -753059.9\n",
      "                , loss1: -7530.821875\n",
      "                , loss2: 22.321456909179688\n",
      "=================================\n",
      "in 750 epoch, average loss: -753041.35\n",
      "                , loss1: -7530.67578125\n",
      "                , loss2: 26.204916381835936\n",
      "=================================\n",
      "in 760 epoch, average loss: -753050.95\n",
      "                , loss1: -7530.9390625\n",
      "                , loss2: 42.994256591796876\n",
      "=================================\n",
      "in 770 epoch, average loss: -753046.55\n",
      "                , loss1: -7530.7953125\n",
      "                , loss2: 32.89466857910156\n",
      "=================================\n",
      "in 780 epoch, average loss: -752983.4\n",
      "                , loss1: -7529.93359375\n",
      "                , loss2: 9.911139678955077\n",
      "=================================\n",
      "in 790 epoch, average loss: -753045.5\n",
      "                , loss1: -7530.73046875\n",
      "                , loss2: 27.538595581054686\n",
      "=================================\n",
      "in 800 epoch, average loss: -753064.9\n",
      "                , loss1: -7530.88203125\n",
      "                , loss2: 23.226164245605467\n",
      "=================================\n",
      "in 810 epoch, average loss: -753067.05\n",
      "                , loss1: -7530.79609375\n",
      "                , loss2: 12.56554718017578\n",
      "=================================\n",
      "in 820 epoch, average loss: -753055.7\n",
      "                , loss1: -7530.7453125\n",
      "                , loss2: 18.770623779296876\n",
      "=================================\n",
      "in 830 epoch, average loss: -753032.6\n",
      "                , loss1: -7530.88984375\n",
      "                , loss2: 56.34527587890625\n",
      "=================================\n",
      "in 840 epoch, average loss: -753018.9\n",
      "                , loss1: -7530.778125\n",
      "                , loss2: 58.92298583984375\n",
      "=================================\n",
      "in 850 epoch, average loss: -752946.35\n",
      "                , loss1: -7530.64140625\n",
      "                , loss2: 117.72423095703125\n",
      "=================================\n",
      "in 860 epoch, average loss: -752998.9\n",
      "                , loss1: -7530.91015625\n",
      "                , loss2: 92.17245483398438\n",
      "=================================\n",
      "in 870 epoch, average loss: -753055.2\n",
      "                , loss1: -7530.8390625\n",
      "                , loss2: 28.745904541015626\n",
      "=================================\n",
      "in 880 epoch, average loss: -753030.6\n",
      "                , loss1: -7530.9\n",
      "                , loss2: 59.445928955078124\n",
      "=================================\n",
      "in 890 epoch, average loss: -753014.25\n",
      "                , loss1: -7530.3984375\n",
      "                , loss2: 25.678179931640624\n",
      "=================================\n",
      "in 900 epoch, average loss: -752977.4\n",
      "                , loss1: -7530.70546875\n",
      "                , loss2: 93.17937622070312\n",
      "=================================\n",
      "in 910 epoch, average loss: -753031.0\n",
      "                , loss1: -7530.7109375\n",
      "                , loss2: 40.092306518554686\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m temp_loss_total,temp_loss1,temp_loss2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(15000):\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6601.8999, 6596.1392, 6518.9629], device='cuda:1',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = torch.sum(outs_straight, dim = 0)\n",
    "bs = torch.sum(outs, dim = 0)\n",
    "bs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
