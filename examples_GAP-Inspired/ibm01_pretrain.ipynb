{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行前请安装dhg: `pip install git+https://github.com/iMoonLab/DeepHypergraph.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import pickle\n",
    "\n",
    "import dhg\n",
    "from dhg import Hypergraph\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP\n",
    "from hgp.loss import loss_bs_matrix\n",
    "from hgp.utils import from_file_to_hypergraph\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 3\n",
    "\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.4}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.1}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 4096, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.25}\n",
    "#h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1536, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 1536, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "#h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 2048, \"out_channels\": 1536, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "#h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer2\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer21\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer22\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer23\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer3\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer33\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":16, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\"\"\"\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.3}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers4\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":128, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_label_loss(outs,label,device):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对网络进行预训练的代码\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``label``(`Hypergraph`):  vertex的标签. Size :math:`(N, nums_classes)`. \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree -0.35\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "\n",
    "    loss = 50*loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer,label):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.label = label.to(DEVICE)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def pre_train(self, epoch):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss = nn.CrossEntropyLoss()(outs, self.label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13257, 12752, 12752)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = from_file_to_hypergraph(\"../data/ISPD_benchmark/ibm01_new.hgr\",reset_vertex_index=True)\n",
    "edges, num_v = G.e,G.num_v\n",
    "with open(\"../data/pretrain/label_ibm01.pkl\", \"rb\") as f:\n",
    "    label = pickle.load(f)\n",
    "G.num_e,G.num_v,len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size is inconsistent with indices: for dim 0, size is 12752 but found index 12752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(num_v, hyper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_features_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      2\u001b[0m net \u001b[38;5;241m=\u001b[39m HGNNP(hyper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_hyper_prmts\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 3\u001b[0m hgnn_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m hyper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml_hyper_prmts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(num_features\u001b[38;5;241m=\u001b[39mv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(DEVICE)) \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, net, X, hg, optimizer, label)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg \u001b[38;5;241m=\u001b[39m hg\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:776\u001b[0m, in \u001b[0;36mHypergraph.H\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the hypergraph incidence matrix :math:`\\mathbf{H}` with ``torch.sparse_coo_tensor`` format.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH_v2e\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/base.py:844\u001b[0m, in \u001b[0;36mBaseHypergraph.H_v2e\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the hypergraph incidence matrix with ``sparse matrix`` format.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_v2e_of_group(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_names]\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(_tmp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/base.py:844\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the hypergraph incidence matrix with ``sparse matrix`` format.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH_v2e_of_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_names]\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(_tmp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/base.py:856\u001b[0m, in \u001b[0;36mBaseHypergraph.H_v2e_of_group\u001b[0;34m(self, group_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m group_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_names, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in existing hyperedge groups.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_cache[group_name]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_cache[group_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_H_of_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv2e\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_cache[group_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH_v2e\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/base.py:560\u001b[0m, in \u001b[0;36mBaseHypergraph._fetch_H_of_group\u001b[0;34m(self, direction, group_name)\u001b[0m\n\u001b[1;32m    558\u001b[0m     v_idx\u001b[38;5;241m.\u001b[39mextend(sub_e)\n\u001b[1;32m    559\u001b[0m     e_idx\u001b[38;5;241m.\u001b[39mextend([_e_idx] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sub_e))\n\u001b[0;32m--> 560\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_coo_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_e\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcoalesce()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m H\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size is inconsistent with indices: for dim 0, size is 12752 but found index 12752"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(num_v, hyper[\"init_features_dim\"]))\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None, label=label).to(DEVICE)\n",
    "\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "\n",
    "optim = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=0)\n",
    "hgnn_trainer.optimizer = optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 0.11060378551483155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 0.8473074913024903\n",
      "in 20 epoch, average loss: 0.6908018589019775\n",
      "in 30 epoch, average loss: 0.6286892414093017\n",
      "in 40 epoch, average loss: 0.5772213935852051\n",
      "in 50 epoch, average loss: 0.5300322055816651\n",
      "in 60 epoch, average loss: 0.4841326713562012\n",
      "in 70 epoch, average loss: 0.43969430923461916\n",
      "in 80 epoch, average loss: 0.4011239051818848\n",
      "in 90 epoch, average loss: 0.3679665565490723\n",
      "in 100 epoch, average loss: 0.339047908782959\n",
      "in 110 epoch, average loss: 0.3145130157470703\n",
      "in 120 epoch, average loss: 0.29218101501464844\n",
      "in 130 epoch, average loss: 0.2736319303512573\n",
      "in 140 epoch, average loss: 0.2569025754928589\n",
      "in 150 epoch, average loss: 0.2419965982437134\n",
      "in 160 epoch, average loss: 0.22755489349365235\n",
      "in 170 epoch, average loss: 0.21446738243103028\n",
      "in 180 epoch, average loss: 0.20234029293060302\n",
      "in 190 epoch, average loss: 0.19116994142532348\n"
     ]
    }
   ],
   "source": [
    "temp_loss_total= torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(200):\n",
    "    loss = hgnn_trainer.pre_train(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    # validation\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total = torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6352,  1.3473, -3.0027],\n",
       "        [-2.6518,  1.3088, -2.9908],\n",
       "        [-2.0291,  1.5059, -2.4597],\n",
       "        ...,\n",
       "        [-2.5938,  1.3281, -2.9586],\n",
       "        [-2.2752,  1.1233, -2.5182],\n",
       "        [-2.4658,  1.3350, -2.8108]], device='cuda:1',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.forward(hgnn_trainer.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.3, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.3, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.3, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.05, inplace=False)\n",
       "  (7): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU()\n",
       "  (10): Dropout(p=0.05, inplace=False)\n",
       "  (11): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (12): ReLU()\n",
       "  (13): Dropout(p=0.05, inplace=False)\n",
       "  (14): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (15): ReLU()\n",
       "  (16): Dropout(p=0.05, inplace=False)\n",
       "  (17): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (18): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): ReLU()\n",
       "  (20): Dropout(p=0.05, inplace=False)\n",
       "  (21): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (22): ReLU()\n",
       "  (23): Dropout(p=0.05, inplace=False)\n",
       "  (24): Linear(in_features=16, out_features=3, bias=True)\n",
       "  (25): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 973518.9375\n",
      "                , loss1: -6697.68603515625\n",
      "                , loss2: 1308403.25\n",
      "=================================\n",
      "in 1 epoch, average loss: 773803.6875\n",
      "                , loss1: -6405.81640625\n",
      "                , loss2: 1094094.5\n",
      "=================================\n",
      "in 2 epoch, average loss: 973464.375\n",
      "                , loss1: -4559.97998046875\n",
      "                , loss2: 1201463.375\n",
      "=================================\n",
      "in 3 epoch, average loss: 273287.4375\n",
      "                , loss1: -5463.208984375\n",
      "                , loss2: 546447.875\n",
      "=================================\n",
      "in 4 epoch, average loss: 94962.015625\n",
      "                , loss1: -4428.5078125\n",
      "                , loss2: 316387.40625\n",
      "=================================\n",
      "in 5 epoch, average loss: -184372.90625\n",
      "                , loss1: -4392.8701171875\n",
      "                , loss2: 35270.5859375\n",
      "=================================\n",
      "in 6 epoch, average loss: -190836.546875\n",
      "                , loss1: -3857.427734375\n",
      "                , loss2: 2034.845458984375\n",
      "=================================\n",
      "in 7 epoch, average loss: -146988.375\n",
      "                , loss1: -3513.95654296875\n",
      "                , loss2: 28709.4453125\n",
      "=================================\n",
      "in 8 epoch, average loss: -180610.453125\n",
      "                , loss1: -3661.57861328125\n",
      "                , loss2: 2468.47705078125\n",
      "=================================\n",
      "in 9 epoch, average loss: -182317.578125\n",
      "                , loss1: -3773.88720703125\n",
      "                , loss2: 6376.78759765625\n",
      "=================================\n",
      "in 10 epoch, average loss: -171400.828125\n",
      "                , loss1: -3876.679931640625\n",
      "                , loss2: 22433.166015625\n",
      "=================================\n",
      "in 11 epoch, average loss: -192715.140625\n",
      "                , loss1: -4043.72119140625\n",
      "                , loss2: 9470.9248046875\n",
      "=================================\n",
      "in 12 epoch, average loss: -204880.75\n",
      "                , loss1: -4143.6298828125\n",
      "                , loss2: 2300.743408203125\n",
      "=================================\n",
      "in 13 epoch, average loss: -207118.28125\n",
      "                , loss1: -4198.9912109375\n",
      "                , loss2: 2831.27783203125\n",
      "=================================\n",
      "in 14 epoch, average loss: -216551.171875\n",
      "                , loss1: -4345.6171875\n",
      "                , loss2: 729.6890258789062\n",
      "=================================\n",
      "in 15 epoch, average loss: -221283.15625\n",
      "                , loss1: -4437.923828125\n",
      "                , loss2: 613.0333251953125\n",
      "=================================\n",
      "in 16 epoch, average loss: -226227.28125\n",
      "                , loss1: -4550.4912109375\n",
      "                , loss2: 1297.28759765625\n",
      "=================================\n",
      "in 17 epoch, average loss: -227941.921875\n",
      "                , loss1: -4631.60400390625\n",
      "                , loss2: 3638.276123046875\n",
      "=================================\n",
      "in 18 epoch, average loss: -229447.484375\n",
      "                , loss1: -4734.27392578125\n",
      "                , loss2: 7266.2236328125\n",
      "=================================\n",
      "in 19 epoch, average loss: -235109.546875\n",
      "                , loss1: -4749.30078125\n",
      "                , loss2: 2355.4912109375\n",
      "=================================\n",
      "in 20 epoch, average loss: -236823.296875\n",
      "                , loss1: -4766.71875\n",
      "                , loss2: 1512.6409912109375\n",
      "=================================\n",
      "in 21 epoch, average loss: -238829.796875\n",
      "                , loss1: -4801.9091796875\n",
      "                , loss2: 1265.663330078125\n",
      "=================================\n",
      "in 22 epoch, average loss: -242744.40625\n",
      "                , loss1: -4870.4130859375\n",
      "                , loss2: 776.251953125\n",
      "=================================\n",
      "in 23 epoch, average loss: -242226.75\n",
      "                , loss1: -4888.591796875\n",
      "                , loss2: 2202.83642578125\n",
      "=================================\n",
      "in 24 epoch, average loss: -247183.0625\n",
      "                , loss1: -4966.70947265625\n",
      "                , loss2: 1152.4117431640625\n",
      "=================================\n",
      "in 25 epoch, average loss: -249224.421875\n",
      "                , loss1: -4989.6376953125\n",
      "                , loss2: 257.4669189453125\n",
      "=================================\n",
      "in 26 epoch, average loss: -251181.15625\n",
      "                , loss1: -5027.31591796875\n",
      "                , loss2: 184.6457977294922\n",
      "=================================\n",
      "in 27 epoch, average loss: -255292.15625\n",
      "                , loss1: -5123.171875\n",
      "                , loss2: 866.4385986328125\n",
      "=================================\n",
      "in 28 epoch, average loss: -256159.234375\n",
      "                , loss1: -5170.5751953125\n",
      "                , loss2: 2369.53662109375\n",
      "=================================\n",
      "in 29 epoch, average loss: -257588.8125\n",
      "                , loss1: -5210.0927734375\n",
      "                , loss2: 2915.835693359375\n",
      "=================================\n",
      "in 30 epoch, average loss: -259316.21875\n",
      "                , loss1: -5228.169921875\n",
      "                , loss2: 2092.28125\n",
      "=================================\n",
      "in 31 epoch, average loss: -261557.703125\n",
      "                , loss1: -5259.6279296875\n",
      "                , loss2: 1423.703369140625\n",
      "=================================\n",
      "in 32 epoch, average loss: -263573.5625\n",
      "                , loss1: -5301.162109375\n",
      "                , loss2: 1484.542724609375\n",
      "=================================\n",
      "in 33 epoch, average loss: -264075.0625\n",
      "                , loss1: -5309.861328125\n",
      "                , loss2: 1417.9970703125\n",
      "=================================\n",
      "in 34 epoch, average loss: -265934.8125\n",
      "                , loss1: -5345.88720703125\n",
      "                , loss2: 1359.5538330078125\n",
      "=================================\n",
      "in 35 epoch, average loss: -268154.8125\n",
      "                , loss1: -5382.34765625\n",
      "                , loss2: 962.5634155273438\n",
      "=================================\n",
      "in 36 epoch, average loss: -269974.71875\n",
      "                , loss1: -5415.4169921875\n",
      "                , loss2: 796.1394653320312\n",
      "=================================\n",
      "in 37 epoch, average loss: -270248.34375\n",
      "                , loss1: -5409.8427734375\n",
      "                , loss2: 243.7821044921875\n",
      "=================================\n",
      "in 38 epoch, average loss: -272525.65625\n",
      "                , loss1: -5461.1865234375\n",
      "                , loss2: 533.6580200195312\n",
      "=================================\n",
      "in 39 epoch, average loss: -273750.875\n",
      "                , loss1: -5499.37353515625\n",
      "                , loss2: 1217.801513671875\n",
      "=================================\n",
      "in 40 epoch, average loss: -275448.25\n",
      "                , loss1: -5538.3603515625\n",
      "                , loss2: 1469.7877197265625\n",
      "=================================\n",
      "in 41 epoch, average loss: -277257.375\n",
      "                , loss1: -5584.8544921875\n",
      "                , loss2: 1985.34716796875\n",
      "=================================\n",
      "in 42 epoch, average loss: -279199.03125\n",
      "                , loss1: -5602.9873046875\n",
      "                , loss2: 950.3416137695312\n",
      "=================================\n",
      "in 43 epoch, average loss: -279296.25\n",
      "                , loss1: -5600.650390625\n",
      "                , loss2: 736.2793579101562\n",
      "=================================\n",
      "in 44 epoch, average loss: -281375.5625\n",
      "                , loss1: -5649.7373046875\n",
      "                , loss2: 1111.3260498046875\n",
      "=================================\n",
      "in 45 epoch, average loss: -282333.40625\n",
      "                , loss1: -5680.7138671875\n",
      "                , loss2: 1702.2801513671875\n",
      "=================================\n",
      "in 46 epoch, average loss: -283718.1875\n",
      "                , loss1: -5696.0771484375\n",
      "                , loss2: 1085.666015625\n",
      "=================================\n",
      "in 47 epoch, average loss: -284959.3125\n",
      "                , loss1: -5739.3759765625\n",
      "                , loss2: 2009.4862060546875\n",
      "=================================\n",
      "in 48 epoch, average loss: -285824.84375\n",
      "                , loss1: -5753.4560546875\n",
      "                , loss2: 1847.9654541015625\n",
      "=================================\n",
      "in 49 epoch, average loss: -287569.6875\n",
      "                , loss1: -5763.7021484375\n",
      "                , loss2: 615.4024047851562\n",
      "=================================\n",
      "in 50 epoch, average loss: -289489.28125\n",
      "                , loss1: -5811.57470703125\n",
      "                , loss2: 1089.4744873046875\n",
      "=================================\n",
      "in 51 epoch, average loss: -290042.25\n",
      "                , loss1: -5825.1435546875\n",
      "                , loss2: 1214.9393310546875\n",
      "=================================\n",
      "in 52 epoch, average loss: -290690.46875\n",
      "                , loss1: -5830.931640625\n",
      "                , loss2: 856.1199951171875\n",
      "=================================\n",
      "in 53 epoch, average loss: -292408.375\n",
      "                , loss1: -5869.0107421875\n",
      "                , loss2: 1042.1502685546875\n",
      "=================================\n",
      "in 54 epoch, average loss: -294048.625\n",
      "                , loss1: -5903.697265625\n",
      "                , loss2: 1136.2484130859375\n",
      "=================================\n",
      "in 55 epoch, average loss: -294544.6875\n",
      "                , loss1: -5909.91650390625\n",
      "                , loss2: 951.1176147460938\n",
      "=================================\n",
      "in 56 epoch, average loss: -294863.53125\n",
      "                , loss1: -5906.74267578125\n",
      "                , loss2: 473.58343505859375\n",
      "=================================\n",
      "in 57 epoch, average loss: -295532.75\n",
      "                , loss1: -5918.25537109375\n",
      "                , loss2: 380.0399475097656\n",
      "=================================\n",
      "in 58 epoch, average loss: -297500.84375\n",
      "                , loss1: -5965.1943359375\n",
      "                , loss2: 758.8749389648438\n",
      "=================================\n",
      "in 59 epoch, average loss: -298871.75\n",
      "                , loss1: -5992.51806640625\n",
      "                , loss2: 754.1441650390625\n",
      "=================================\n",
      "in 60 epoch, average loss: -299678.125\n",
      "                , loss1: -6016.96484375\n",
      "                , loss2: 1170.1400146484375\n",
      "=================================\n",
      "in 61 epoch, average loss: -300418.46875\n",
      "                , loss1: -6027.462890625\n",
      "                , loss2: 954.700439453125\n",
      "=================================\n",
      "in 62 epoch, average loss: -302109.21875\n",
      "                , loss1: -6055.181640625\n",
      "                , loss2: 649.8857421875\n",
      "=================================\n",
      "in 63 epoch, average loss: -302762.0625\n",
      "                , loss1: -6082.5546875\n",
      "                , loss2: 1365.6961669921875\n",
      "=================================\n",
      "in 64 epoch, average loss: -304528.75\n",
      "                , loss1: -6107.1708984375\n",
      "                , loss2: 829.7664794921875\n",
      "=================================\n",
      "in 65 epoch, average loss: -304418.625\n",
      "                , loss1: -6117.75\n",
      "                , loss2: 1468.86181640625\n",
      "=================================\n",
      "in 66 epoch, average loss: -305509.03125\n",
      "                , loss1: -6141.9091796875\n",
      "                , loss2: 1586.430419921875\n",
      "=================================\n",
      "in 67 epoch, average loss: -306234.71875\n",
      "                , loss1: -6161.32275390625\n",
      "                , loss2: 1831.4208984375\n",
      "=================================\n",
      "in 68 epoch, average loss: -307929.78125\n",
      "                , loss1: -6170.7548828125\n",
      "                , loss2: 607.9678955078125\n",
      "=================================\n",
      "in 69 epoch, average loss: -308174.8125\n",
      "                , loss1: -6172.2060546875\n",
      "                , loss2: 435.5121154785156\n",
      "=================================\n",
      "in 70 epoch, average loss: -309326.0625\n",
      "                , loss1: -6207.1435546875\n",
      "                , loss2: 1031.109375\n",
      "=================================\n",
      "in 71 epoch, average loss: -310497.9375\n",
      "                , loss1: -6227.955078125\n",
      "                , loss2: 899.8106079101562\n",
      "=================================\n",
      "in 72 epoch, average loss: -310687.59375\n",
      "                , loss1: -6240.5888671875\n",
      "                , loss2: 1341.857177734375\n",
      "=================================\n",
      "in 73 epoch, average loss: -312733.34375\n",
      "                , loss1: -6270.08544921875\n",
      "                , loss2: 770.9400634765625\n",
      "=================================\n",
      "in 74 epoch, average loss: -312579.9375\n",
      "                , loss1: -6264.3916015625\n",
      "                , loss2: 639.6410522460938\n",
      "=================================\n",
      "in 75 epoch, average loss: -314588.125\n",
      "                , loss1: -6304.9013671875\n",
      "                , loss2: 656.9273071289062\n",
      "=================================\n",
      "in 76 epoch, average loss: -314478.375\n",
      "                , loss1: -6313.2294921875\n",
      "                , loss2: 1183.1011962890625\n",
      "=================================\n",
      "in 77 epoch, average loss: -315094.4375\n",
      "                , loss1: -6323.53369140625\n",
      "                , loss2: 1082.2498779296875\n",
      "=================================\n",
      "in 78 epoch, average loss: -316172.59375\n",
      "                , loss1: -6340.0546875\n",
      "                , loss2: 830.15771484375\n",
      "=================================\n",
      "in 79 epoch, average loss: -316997.4375\n",
      "                , loss1: -6356.83203125\n",
      "                , loss2: 844.1469116210938\n",
      "=================================\n",
      "in 80 epoch, average loss: -318104.21875\n",
      "                , loss1: -6382.916015625\n",
      "                , loss2: 1041.593505859375\n",
      "=================================\n",
      "in 81 epoch, average loss: -318201.375\n",
      "                , loss1: -6394.091796875\n",
      "                , loss2: 1503.2237548828125\n",
      "=================================\n",
      "in 82 epoch, average loss: -319887.90625\n",
      "                , loss1: -6429.25341796875\n",
      "                , loss2: 1574.73828125\n",
      "=================================\n",
      "in 83 epoch, average loss: -320716.6875\n",
      "                , loss1: -6428.9296875\n",
      "                , loss2: 729.8031616210938\n",
      "=================================\n",
      "in 84 epoch, average loss: -321673.78125\n",
      "                , loss1: -6453.158203125\n",
      "                , loss2: 984.1329956054688\n",
      "=================================\n",
      "in 85 epoch, average loss: -322778.625\n",
      "                , loss1: -6484.796875\n",
      "                , loss2: 1461.211181640625\n",
      "=================================\n",
      "in 86 epoch, average loss: -323034.4375\n",
      "                , loss1: -6483.634765625\n",
      "                , loss2: 1147.312255859375\n",
      "=================================\n",
      "in 87 epoch, average loss: -323371.5\n",
      "                , loss1: -6492.185546875\n",
      "                , loss2: 1237.77734375\n",
      "=================================\n",
      "in 88 epoch, average loss: -323589.0\n",
      "                , loss1: -6484.70703125\n",
      "                , loss2: 646.346923828125\n",
      "=================================\n",
      "in 89 epoch, average loss: -325681.21875\n",
      "                , loss1: -6525.75390625\n",
      "                , loss2: 606.459716796875\n",
      "=================================\n",
      "in 90 epoch, average loss: -325541.46875\n",
      "                , loss1: -6521.3203125\n",
      "                , loss2: 524.52099609375\n",
      "=================================\n",
      "in 91 epoch, average loss: -326820.875\n",
      "                , loss1: -6556.08203125\n",
      "                , loss2: 983.21728515625\n",
      "=================================\n",
      "in 92 epoch, average loss: -327453.25\n",
      "                , loss1: -6570.9775390625\n",
      "                , loss2: 1095.6280517578125\n",
      "=================================\n",
      "in 93 epoch, average loss: -327861.28125\n",
      "                , loss1: -6578.66796875\n",
      "                , loss2: 1072.1171875\n",
      "=================================\n",
      "in 94 epoch, average loss: -327379.625\n",
      "                , loss1: -6565.50830078125\n",
      "                , loss2: 895.7658081054688\n",
      "=================================\n",
      "in 95 epoch, average loss: -329731.6875\n",
      "                , loss1: -6602.7587890625\n",
      "                , loss2: 406.2359924316406\n",
      "=================================\n",
      "in 96 epoch, average loss: -330336.5625\n",
      "                , loss1: -6628.31640625\n",
      "                , loss2: 1079.2486572265625\n",
      "=================================\n",
      "in 97 epoch, average loss: -331032.3125\n",
      "                , loss1: -6639.1884765625\n",
      "                , loss2: 927.1098022460938\n",
      "=================================\n",
      "in 98 epoch, average loss: -331354.78125\n",
      "                , loss1: -6660.47412109375\n",
      "                , loss2: 1668.9429931640625\n",
      "=================================\n",
      "in 99 epoch, average loss: -332690.125\n",
      "                , loss1: -6675.24462890625\n",
      "                , loss2: 1072.088623046875\n",
      "=================================\n",
      "in 100 epoch, average loss: -333101.53125\n",
      "                , loss1: -6675.17138671875\n",
      "                , loss2: 657.0176391601562\n",
      "=================================\n",
      "in 101 epoch, average loss: -333035.28125\n",
      "                , loss1: -6682.02587890625\n",
      "                , loss2: 1066.005859375\n",
      "=================================\n",
      "in 102 epoch, average loss: -334301.28125\n",
      "                , loss1: -6700.3896484375\n",
      "                , loss2: 718.1911010742188\n",
      "=================================\n",
      "in 103 epoch, average loss: -334901.84375\n",
      "                , loss1: -6726.40283203125\n",
      "                , loss2: 1418.3048095703125\n",
      "=================================\n",
      "in 104 epoch, average loss: -334879.65625\n",
      "                , loss1: -6731.2412109375\n",
      "                , loss2: 1682.401123046875\n",
      "=================================\n",
      "in 105 epoch, average loss: -336398.375\n",
      "                , loss1: -6753.38916015625\n",
      "                , loss2: 1271.0919189453125\n",
      "=================================\n",
      "in 106 epoch, average loss: -336817.53125\n",
      "                , loss1: -6753.0361328125\n",
      "                , loss2: 834.2817993164062\n",
      "=================================\n",
      "in 107 epoch, average loss: -337287.0625\n",
      "                , loss1: -6763.6357421875\n",
      "                , loss2: 894.7076416015625\n",
      "=================================\n",
      "in 108 epoch, average loss: -338655.84375\n",
      "                , loss1: -6792.03466796875\n",
      "                , loss2: 945.8643188476562\n",
      "=================================\n",
      "in 109 epoch, average loss: -339385.125\n",
      "                , loss1: -6808.767578125\n",
      "                , loss2: 1053.245361328125\n",
      "=================================\n",
      "in 110 epoch, average loss: -340904.0625\n",
      "                , loss1: -6838.33740234375\n",
      "                , loss2: 1012.8211059570312\n",
      "=================================\n",
      "in 111 epoch, average loss: -340918.46875\n",
      "                , loss1: -6853.1025390625\n",
      "                , loss2: 1736.6644287109375\n",
      "=================================\n",
      "in 112 epoch, average loss: -341417.40625\n",
      "                , loss1: -6857.5517578125\n",
      "                , loss2: 1460.201416015625\n",
      "=================================\n",
      "in 113 epoch, average loss: -342689.4375\n",
      "                , loss1: -6868.0283203125\n",
      "                , loss2: 711.9558715820312\n",
      "=================================\n",
      "in 114 epoch, average loss: -344019.78125\n",
      "                , loss1: -6890.9013671875\n",
      "                , loss2: 525.29345703125\n",
      "=================================\n",
      "in 115 epoch, average loss: -344179.84375\n",
      "                , loss1: -6903.9345703125\n",
      "                , loss2: 1016.8751220703125\n",
      "=================================\n",
      "in 116 epoch, average loss: -344856.34375\n",
      "                , loss1: -6918.1044921875\n",
      "                , loss2: 1048.8812255859375\n",
      "=================================\n",
      "in 117 epoch, average loss: -344766.0\n",
      "                , loss1: -6946.021484375\n",
      "                , loss2: 2535.055419921875\n",
      "=================================\n",
      "in 118 epoch, average loss: -345957.6875\n",
      "                , loss1: -6938.45263671875\n",
      "                , loss2: 964.9454956054688\n",
      "=================================\n",
      "in 119 epoch, average loss: -346265.15625\n",
      "                , loss1: -6934.96435546875\n",
      "                , loss2: 483.0768737792969\n",
      "=================================\n",
      "in 120 epoch, average loss: -347776.625\n",
      "                , loss1: -6964.45947265625\n",
      "                , loss2: 446.3551330566406\n",
      "=================================\n",
      "in 121 epoch, average loss: -346247.875\n",
      "                , loss1: -6934.4599609375\n",
      "                , loss2: 475.12457275390625\n",
      "=================================\n",
      "in 122 epoch, average loss: -348015.40625\n",
      "                , loss1: -6962.453125\n",
      "                , loss2: 107.25886535644531\n",
      "=================================\n",
      "in 123 epoch, average loss: -348434.75\n",
      "                , loss1: -6970.9296875\n",
      "                , loss2: 111.7632064819336\n",
      "=================================\n",
      "in 124 epoch, average loss: -349465.8125\n",
      "                , loss1: -6996.7333984375\n",
      "                , loss2: 370.8516540527344\n",
      "=================================\n",
      "in 125 epoch, average loss: -349532.09375\n",
      "                , loss1: -7001.6669921875\n",
      "                , loss2: 551.2447509765625\n",
      "=================================\n",
      "in 126 epoch, average loss: -350009.15625\n",
      "                , loss1: -7023.6259765625\n",
      "                , loss2: 1172.163330078125\n",
      "=================================\n",
      "in 127 epoch, average loss: -350429.375\n",
      "                , loss1: -7030.98583984375\n",
      "                , loss2: 1119.9154052734375\n",
      "=================================\n",
      "in 128 epoch, average loss: -351267.875\n",
      "                , loss1: -7040.8095703125\n",
      "                , loss2: 772.5982666015625\n",
      "=================================\n",
      "in 129 epoch, average loss: -351003.25\n",
      "                , loss1: -7029.75830078125\n",
      "                , loss2: 484.6410827636719\n",
      "=================================\n",
      "in 130 epoch, average loss: -352140.9375\n",
      "                , loss1: -7052.80859375\n",
      "                , loss2: 499.4876403808594\n",
      "=================================\n",
      "in 131 epoch, average loss: -352546.15625\n",
      "                , loss1: -7056.8505859375\n",
      "                , loss2: 296.3855285644531\n",
      "=================================\n",
      "in 132 epoch, average loss: -352703.4375\n",
      "                , loss1: -7063.515625\n",
      "                , loss2: 472.3537902832031\n",
      "=================================\n",
      "in 133 epoch, average loss: -353764.75\n",
      "                , loss1: -7083.3154296875\n",
      "                , loss2: 401.0325012207031\n",
      "=================================\n",
      "in 134 epoch, average loss: -354307.875\n",
      "                , loss1: -7093.7041015625\n",
      "                , loss2: 377.3454284667969\n",
      "=================================\n",
      "in 135 epoch, average loss: -354743.90625\n",
      "                , loss1: -7101.89990234375\n",
      "                , loss2: 351.0819396972656\n",
      "=================================\n",
      "in 136 epoch, average loss: -355985.75\n",
      "                , loss1: -7127.04345703125\n",
      "                , loss2: 366.45086669921875\n",
      "=================================\n",
      "in 137 epoch, average loss: -356601.84375\n",
      "                , loss1: -7146.4501953125\n",
      "                , loss2: 720.6611938476562\n",
      "=================================\n",
      "in 138 epoch, average loss: -357174.96875\n",
      "                , loss1: -7158.57958984375\n",
      "                , loss2: 754.0114135742188\n",
      "=================================\n",
      "in 139 epoch, average loss: -356690.40625\n",
      "                , loss1: -7158.26171875\n",
      "                , loss2: 1222.6923828125\n",
      "=================================\n",
      "in 140 epoch, average loss: -358448.40625\n",
      "                , loss1: -7175.76171875\n",
      "                , loss2: 339.67822265625\n",
      "=================================\n",
      "in 141 epoch, average loss: -358089.96875\n",
      "                , loss1: -7180.30078125\n",
      "                , loss2: 925.0626831054688\n",
      "=================================\n",
      "in 142 epoch, average loss: -359462.78125\n",
      "                , loss1: -7202.5263671875\n",
      "                , loss2: 663.541259765625\n",
      "=================================\n",
      "in 143 epoch, average loss: -359838.125\n",
      "                , loss1: -7213.92431640625\n",
      "                , loss2: 858.1004638671875\n",
      "=================================\n",
      "in 144 epoch, average loss: -360580.4375\n",
      "                , loss1: -7223.3642578125\n",
      "                , loss2: 587.7871704101562\n",
      "=================================\n",
      "in 145 epoch, average loss: -360598.46875\n",
      "                , loss1: -7218.4619140625\n",
      "                , loss2: 324.6165466308594\n",
      "=================================\n",
      "in 146 epoch, average loss: -361430.59375\n",
      "                , loss1: -7242.55859375\n",
      "                , loss2: 697.3319091796875\n",
      "=================================\n",
      "in 147 epoch, average loss: -362493.125\n",
      "                , loss1: -7261.95849609375\n",
      "                , loss2: 604.8032836914062\n",
      "=================================\n",
      "in 148 epoch, average loss: -362733.59375\n",
      "                , loss1: -7266.8720703125\n",
      "                , loss2: 610.0152587890625\n",
      "=================================\n",
      "in 149 epoch, average loss: -363369.375\n",
      "                , loss1: -7278.4521484375\n",
      "                , loss2: 553.215087890625\n",
      "=================================\n",
      "in 150 epoch, average loss: -363637.90625\n",
      "                , loss1: -7281.5751953125\n",
      "                , loss2: 440.8447265625\n",
      "=================================\n",
      "in 151 epoch, average loss: -363931.125\n",
      "                , loss1: -7296.314453125\n",
      "                , loss2: 884.5992431640625\n",
      "=================================\n",
      "in 152 epoch, average loss: -364655.78125\n",
      "                , loss1: -7302.4091796875\n",
      "                , loss2: 464.6769714355469\n",
      "=================================\n",
      "in 153 epoch, average loss: -364325.46875\n",
      "                , loss1: -7320.763671875\n",
      "                , loss2: 1712.7105712890625\n",
      "=================================\n",
      "in 154 epoch, average loss: -364984.96875\n",
      "                , loss1: -7315.3759765625\n",
      "                , loss2: 783.8513793945312\n",
      "=================================\n",
      "in 155 epoch, average loss: -366043.875\n",
      "                , loss1: -7332.3447265625\n",
      "                , loss2: 573.3698120117188\n",
      "=================================\n",
      "in 156 epoch, average loss: -366580.71875\n",
      "                , loss1: -7339.544921875\n",
      "                , loss2: 396.5230407714844\n",
      "=================================\n",
      "in 157 epoch, average loss: -366519.0\n",
      "                , loss1: -7338.5498046875\n",
      "                , loss2: 408.4994812011719\n",
      "=================================\n",
      "in 158 epoch, average loss: -367743.3125\n",
      "                , loss1: -7362.357421875\n",
      "                , loss2: 374.56787109375\n",
      "=================================\n",
      "in 159 epoch, average loss: -369675.84375\n",
      "                , loss1: -7402.41259765625\n",
      "                , loss2: 444.7947692871094\n",
      "=================================\n",
      "in 160 epoch, average loss: -368600.25\n",
      "                , loss1: -7403.6484375\n",
      "                , loss2: 1582.20068359375\n",
      "=================================\n",
      "in 161 epoch, average loss: -369884.875\n",
      "                , loss1: -7423.2802734375\n",
      "                , loss2: 1279.1239013671875\n",
      "=================================\n",
      "in 162 epoch, average loss: -371230.5625\n",
      "                , loss1: -7440.041015625\n",
      "                , loss2: 771.513916015625\n",
      "=================================\n",
      "in 163 epoch, average loss: -372267.125\n",
      "                , loss1: -7451.91455078125\n",
      "                , loss2: 328.5829162597656\n",
      "=================================\n",
      "in 164 epoch, average loss: -372082.78125\n",
      "                , loss1: -7453.86572265625\n",
      "                , loss2: 610.4903564453125\n",
      "=================================\n",
      "in 165 epoch, average loss: -373510.875\n",
      "                , loss1: -7483.9609375\n",
      "                , loss2: 687.1746215820312\n",
      "=================================\n",
      "in 166 epoch, average loss: -373282.09375\n",
      "                , loss1: -7480.87109375\n",
      "                , loss2: 761.469482421875\n",
      "=================================\n",
      "in 167 epoch, average loss: -373427.6875\n",
      "                , loss1: -7488.7939453125\n",
      "                , loss2: 1012.0110473632812\n",
      "=================================\n",
      "in 168 epoch, average loss: -374238.4375\n",
      "                , loss1: -7499.55712890625\n",
      "                , loss2: 739.3956909179688\n",
      "=================================\n",
      "in 169 epoch, average loss: -375193.59375\n",
      "                , loss1: -7517.8173828125\n",
      "                , loss2: 697.292236328125\n",
      "=================================\n",
      "in 170 epoch, average loss: -375464.4375\n",
      "                , loss1: -7515.65478515625\n",
      "                , loss2: 318.3099365234375\n",
      "=================================\n",
      "in 171 epoch, average loss: -374792.125\n",
      "                , loss1: -7507.115234375\n",
      "                , loss2: 563.6124877929688\n",
      "=================================\n",
      "in 172 epoch, average loss: -376374.71875\n",
      "                , loss1: -7541.5712890625\n",
      "                , loss2: 703.8545532226562\n",
      "=================================\n",
      "in 173 epoch, average loss: -376309.21875\n",
      "                , loss1: -7533.064453125\n",
      "                , loss2: 344.01483154296875\n",
      "=================================\n",
      "in 174 epoch, average loss: -377115.84375\n",
      "                , loss1: -7547.845703125\n",
      "                , loss2: 276.4464416503906\n",
      "=================================\n",
      "in 175 epoch, average loss: -378581.15625\n",
      "                , loss1: -7578.75927734375\n",
      "                , loss2: 356.8252258300781\n",
      "=================================\n",
      "in 176 epoch, average loss: -378446.71875\n",
      "                , loss1: -7589.6806640625\n",
      "                , loss2: 1037.322265625\n",
      "=================================\n",
      "in 177 epoch, average loss: -379402.15625\n",
      "                , loss1: -7594.5859375\n",
      "                , loss2: 327.14373779296875\n",
      "=================================\n",
      "in 178 epoch, average loss: -379080.625\n",
      "                , loss1: -7589.65625\n",
      "                , loss2: 402.2020263671875\n",
      "=================================\n",
      "in 179 epoch, average loss: -379560.3125\n",
      "                , loss1: -7600.294921875\n",
      "                , loss2: 454.42889404296875\n",
      "=================================\n",
      "in 180 epoch, average loss: -380175.96875\n",
      "                , loss1: -7609.57421875\n",
      "                , loss2: 302.7491149902344\n",
      "=================================\n",
      "in 181 epoch, average loss: -381175.40625\n",
      "                , loss1: -7631.96630859375\n",
      "                , loss2: 422.9110412597656\n",
      "=================================\n",
      "in 182 epoch, average loss: -381928.875\n",
      "                , loss1: -7660.60205078125\n",
      "                , loss2: 1101.214111328125\n",
      "=================================\n",
      "in 183 epoch, average loss: -382058.5625\n",
      "                , loss1: -7657.498046875\n",
      "                , loss2: 816.3561401367188\n",
      "=================================\n",
      "in 184 epoch, average loss: -383027.25\n",
      "                , loss1: -7662.8203125\n",
      "                , loss2: 113.74942779541016\n",
      "=================================\n",
      "in 185 epoch, average loss: -382982.09375\n",
      "                , loss1: -7660.38134765625\n",
      "                , loss2: 36.962127685546875\n",
      "=================================\n",
      "in 186 epoch, average loss: -382997.5\n",
      "                , loss1: -7670.0546875\n",
      "                , loss2: 505.2518310546875\n",
      "=================================\n",
      "in 187 epoch, average loss: -384274.125\n",
      "                , loss1: -7687.5966796875\n",
      "                , loss2: 105.7216567993164\n",
      "=================================\n",
      "in 188 epoch, average loss: -384062.15625\n",
      "                , loss1: -7693.0830078125\n",
      "                , loss2: 592.00732421875\n",
      "=================================\n",
      "in 189 epoch, average loss: -384815.28125\n",
      "                , loss1: -7717.2470703125\n",
      "                , loss2: 1047.0576171875\n",
      "=================================\n",
      "in 190 epoch, average loss: -384555.34375\n",
      "                , loss1: -7718.54638671875\n",
      "                , loss2: 1371.9752197265625\n",
      "=================================\n",
      "in 191 epoch, average loss: -386381.15625\n",
      "                , loss1: -7734.529296875\n",
      "                , loss2: 345.3219909667969\n",
      "=================================\n",
      "in 192 epoch, average loss: -386103.5\n",
      "                , loss1: -7725.171875\n",
      "                , loss2: 155.0818328857422\n",
      "=================================\n",
      "in 193 epoch, average loss: -386715.59375\n",
      "                , loss1: -7734.3994140625\n",
      "                , loss2: 4.385509967803955\n",
      "=================================\n",
      "in 194 epoch, average loss: -387807.78125\n",
      "                , loss1: -7761.89453125\n",
      "                , loss2: 286.9525451660156\n",
      "=================================\n",
      "in 195 epoch, average loss: -388050.21875\n",
      "                , loss1: -7780.8671875\n",
      "                , loss2: 993.1703491210938\n",
      "=================================\n",
      "in 196 epoch, average loss: -389700.125\n",
      "                , loss1: -7820.80810546875\n",
      "                , loss2: 1340.267333984375\n",
      "=================================\n",
      "in 197 epoch, average loss: -389184.09375\n",
      "                , loss1: -7813.64111328125\n",
      "                , loss2: 1497.9560546875\n",
      "=================================\n",
      "in 198 epoch, average loss: -390309.5625\n",
      "                , loss1: -7816.447265625\n",
      "                , loss2: 512.8025512695312\n",
      "=================================\n",
      "in 199 epoch, average loss: -391449.6875\n",
      "                , loss1: -7838.970703125\n",
      "                , loss2: 498.8492126464844\n",
      "=================================\n",
      "in 200 epoch, average loss: -392241.65625\n",
      "                , loss1: -7850.75146484375\n",
      "                , loss2: 295.8940734863281\n",
      "=================================\n",
      "in 201 epoch, average loss: -392697.15625\n",
      "                , loss1: -7862.6708984375\n",
      "                , loss2: 436.38153076171875\n",
      "=================================\n",
      "in 202 epoch, average loss: -393519.59375\n",
      "                , loss1: -7890.8076171875\n",
      "                , loss2: 1020.7662353515625\n",
      "=================================\n",
      "in 203 epoch, average loss: -394723.71875\n",
      "                , loss1: -7904.0048828125\n",
      "                , loss2: 476.5243225097656\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m temp_loss_total,temp_loss1,temp_loss2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[226], line 42\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     41\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "for epoch in range(15000):\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    # train\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 1}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 1}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 1}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = torch.sum(outs_straight, dim = 0)\n",
    "# bs = torch.sum(outs, dim = 0)\n",
    "bs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
