{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 3\n",
    "\n",
    "weight = 0.93\n",
    "lr = 4e-3\n",
    "sub = 0.009\n",
    "limit = 0.01\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\":256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers141\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers142\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers143\"] = {\"in_channels\": 256, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer113\"] = {\"in_channels\":2048, \"out_channels\":2048, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":3, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12704, 242)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/primary\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): ReLU()\n",
       "  (19): Dropout(p=0.05, inplace=False)\n",
       "  (20): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (21): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): ReLU()\n",
       "  (23): Dropout(p=0.05, inplace=False)\n",
       "  (24): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (25): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -5.52623291015625\n",
      "                , loss1: -143.9299072265625\n",
      "                , loss2: 127.03321533203125\n",
      "                , weight: 0.921\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: -3384.069921875\n",
      "                , loss1: -4098.695703125\n",
      "                , loss2: 131.0505615234375\n",
      "                , weight: 0.831\n",
      "=================================\n",
      "in 20 epoch, average loss: -6639.075\n",
      "                , loss1: -8690.7921875\n",
      "                , loss2: 142.1551025390625\n",
      "                , weight: 0.7409999999999999\n",
      "=================================\n",
      "in 30 epoch, average loss: -6438.916015625\n",
      "                , loss1: -9365.21875\n",
      "                , loss2: 35.830352783203125\n",
      "                , weight: 0.6509999999999998\n",
      "=================================\n",
      "in 40 epoch, average loss: -5680.587109375\n",
      "                , loss1: -9473.390625\n",
      "                , loss2: 17.43085174560547\n",
      "                , weight: 0.5609999999999997\n",
      "=================================\n",
      "in 50 epoch, average loss: -4844.1515625\n",
      "                , loss1: -9502.425\n",
      "                , loss2: 16.14903564453125\n",
      "                , weight: 0.47099999999999964\n",
      "=================================\n",
      "in 60 epoch, average loss: -3989.727734375\n",
      "                , loss1: -9503.6578125\n",
      "                , loss2: 15.755010986328125\n",
      "                , weight: 0.38099999999999956\n",
      "=================================\n",
      "in 70 epoch, average loss: -3131.9240234375\n",
      "                , loss1: -9506.59375\n",
      "                , loss2: 19.868191528320313\n",
      "                , weight: 0.2909999999999995\n",
      "=================================\n",
      "in 80 epoch, average loss: -2279.381640625\n",
      "                , loss1: -9498.1703125\n",
      "                , loss2: 14.655250549316406\n",
      "                , weight: 0.2009999999999994\n",
      "=================================\n",
      "in 90 epoch, average loss: -1417.107421875\n",
      "                , loss1: -9479.93203125\n",
      "                , loss2: 19.446658325195312\n",
      "                , weight: 0.11099999999999935\n",
      "=================================\n",
      "in 100 epoch, average loss: -571.763623046875\n",
      "                , loss1: -9501.3453125\n",
      "                , loss2: 12.825175476074218\n",
      "                , weight: 0.020999999999999373\n",
      "=================================\n",
      "in 110 epoch, average loss: -34.332156372070315\n",
      "                , loss1: -9401.5171875\n",
      "                , loss2: 2.3643653869628904\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 120 epoch, average loss: -25.889462280273438\n",
      "                , loss1: -9456.8015625\n",
      "                , loss2: 2.4809381484985353\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 130 epoch, average loss: -23.766517639160156\n",
      "                , loss1: -9459.453125\n",
      "                , loss2: 4.611842346191406\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 140 epoch, average loss: -21.300418090820312\n",
      "                , loss1: -9480.6078125\n",
      "                , loss2: 7.1414039611816404\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 150 epoch, average loss: -23.16156768798828\n",
      "                , loss1: -9443.20546875\n",
      "                , loss2: 5.168047332763672\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 160 epoch, average loss: -27.076901245117188\n",
      "                , loss1: -9471.66875\n",
      "                , loss2: 1.3381057739257813\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 170 epoch, average loss: -27.507632446289062\n",
      "                , loss1: -9475.059375\n",
      "                , loss2: 0.9175473213195801\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 180 epoch, average loss: -27.376617431640625\n",
      "                , loss1: -9477.18203125\n",
      "                , loss2: 1.0549277305603026\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 190 epoch, average loss: -27.795562744140625\n",
      "                , loss1: -9483.15390625\n",
      "                , loss2: 0.6538993358612061\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 200 epoch, average loss: -27.991232299804686\n",
      "                , loss1: -9493.83125\n",
      "                , loss2: 0.4902608871459961\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 210 epoch, average loss: -28.033041381835936\n",
      "                , loss1: -9496.1296875\n",
      "                , loss2: 0.45534791946411135\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 220 epoch, average loss: -28.03374328613281\n",
      "                , loss1: -9492.80546875\n",
      "                , loss2: 0.44467644691467284\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 230 epoch, average loss: -28.01062927246094\n",
      "                , loss1: -9484.9734375\n",
      "                , loss2: 0.4442889213562012\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 240 epoch, average loss: -28.172390747070313\n",
      "                , loss1: -9497.7359375\n",
      "                , loss2: 0.3208178520202637\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 250 epoch, average loss: -28.03857421875\n",
      "                , loss1: -9491.0328125\n",
      "                , loss2: 0.4345265865325928\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 260 epoch, average loss: -28.1302490234375\n",
      "                , loss1: -9491.1875\n",
      "                , loss2: 0.34331488609313965\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 270 epoch, average loss: -28.079705810546876\n",
      "                , loss1: -9495.57734375\n",
      "                , loss2: 0.4070286750793457\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 280 epoch, average loss: -28.206802368164062\n",
      "                , loss1: -9493.8265625\n",
      "                , loss2: 0.2746774673461914\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 290 epoch, average loss: -27.957196044921876\n",
      "                , loss1: -9490.6015625\n",
      "                , loss2: 0.5146083831787109\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 300 epoch, average loss: -28.141928100585936\n",
      "                , loss1: -9482.7265625\n",
      "                , loss2: 0.3062558174133301\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 310 epoch, average loss: -28.16618957519531\n",
      "                , loss1: -9492.0765625\n",
      "                , loss2: 0.31004040241241454\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 320 epoch, average loss: -28.170819091796876\n",
      "                , loss1: -9496.3421875\n",
      "                , loss2: 0.31820504665374755\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 330 epoch, average loss: -28.140097045898436\n",
      "                , loss1: -9495.05078125\n",
      "                , loss2: 0.34505467414855956\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 340 epoch, average loss: -27.94925537109375\n",
      "                , loss1: -9488.09375\n",
      "                , loss2: 0.5150303840637207\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 350 epoch, average loss: -27.58049621582031\n",
      "                , loss1: -9476.21328125\n",
      "                , loss2: 0.8481430053710938\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 360 epoch, average loss: -28.1154296875\n",
      "                , loss1: -9492.678125\n",
      "                , loss2: 0.3626027345657349\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 370 epoch, average loss: -27.80634765625\n",
      "                , loss1: -9478.8\n",
      "                , loss2: 0.6300524711608887\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 380 epoch, average loss: -28.1792724609375\n",
      "                , loss1: -9497.8703125\n",
      "                , loss2: 0.3143414258956909\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 390 epoch, average loss: -28.17080078125\n",
      "                , loss1: -9499.81328125\n",
      "                , loss2: 0.32864227294921877\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 400 epoch, average loss: -27.991302490234375\n",
      "                , loss1: -9498.225\n",
      "                , loss2: 0.5033763408660888\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 410 epoch, average loss: -27.9490478515625\n",
      "                , loss1: -9493.09765625\n",
      "                , loss2: 0.5302458763122558\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 420 epoch, average loss: -28.168185424804687\n",
      "                , loss1: -9497.7015625\n",
      "                , loss2: 0.3249200344085693\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 430 epoch, average loss: -28.161898803710937\n",
      "                , loss1: -9495.8796875\n",
      "                , loss2: 0.325740909576416\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 440 epoch, average loss: -28.170559692382813\n",
      "                , loss1: -9498.2015625\n",
      "                , loss2: 0.32404584884643556\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 450 epoch, average loss: -28.163482666015625\n",
      "                , loss1: -9497.80078125\n",
      "                , loss2: 0.3299201011657715\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 460 epoch, average loss: -28.170925903320313\n",
      "                , loss1: -9498.36484375\n",
      "                , loss2: 0.3241683006286621\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 470 epoch, average loss: -28.176132202148438\n",
      "                , loss1: -9499.559375\n",
      "                , loss2: 0.32254881858825685\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 480 epoch, average loss: -28.17513427734375\n",
      "                , loss1: -9499.4265625\n",
      "                , loss2: 0.32314794063568114\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 490 epoch, average loss: -28.174420166015626\n",
      "                , loss1: -9499.725\n",
      "                , loss2: 0.3247537136077881\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 500 epoch, average loss: -28.174725341796876\n",
      "                , loss1: -9498.634375\n",
      "                , loss2: 0.3211804389953613\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 510 epoch, average loss: -28.17330322265625\n",
      "                , loss1: -9499.50703125\n",
      "                , loss2: 0.3252218008041382\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 520 epoch, average loss: -28.179058837890626\n",
      "                , loss1: -9499.05703125\n",
      "                , loss2: 0.318110990524292\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 530 epoch, average loss: -28.1550048828125\n",
      "                , loss1: -9495.4671875\n",
      "                , loss2: 0.33139748573303224\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 540 epoch, average loss: -28.164849853515626\n",
      "                , loss1: -9494.525\n",
      "                , loss2: 0.31872382164001467\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 550 epoch, average loss: -28.182644653320313\n",
      "                , loss1: -9498.1546875\n",
      "                , loss2: 0.31182198524475097\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 560 epoch, average loss: -28.182855224609376\n",
      "                , loss1: -9499.121875\n",
      "                , loss2: 0.3145103931427002\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 570 epoch, average loss: -28.21497497558594\n",
      "                , loss1: -9495.47109375\n",
      "                , loss2: 0.2714385986328125\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 580 epoch, average loss: -28.14642333984375\n",
      "                , loss1: -9494.97890625\n",
      "                , loss2: 0.33851146697998047\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 590 epoch, average loss: -28.236697387695312\n",
      "                , loss1: -9493.63671875\n",
      "                , loss2: 0.2442138671875\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 600 epoch, average loss: -28.23524169921875\n",
      "                , loss1: -9490.98671875\n",
      "                , loss2: 0.23771660327911376\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 610 epoch, average loss: -28.157321166992187\n",
      "                , loss1: -9489.11015625\n",
      "                , loss2: 0.3100125789642334\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 620 epoch, average loss: -28.160586547851562\n",
      "                , loss1: -9499.296875\n",
      "                , loss2: 0.3373052358627319\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 630 epoch, average loss: -28.155819702148438\n",
      "                , loss1: -9499.47265625\n",
      "                , loss2: 0.3426025390625\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 640 epoch, average loss: -28.06661376953125\n",
      "                , loss1: -9498.2125\n",
      "                , loss2: 0.42802734375\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 650 epoch, average loss: -28.174395751953124\n",
      "                , loss1: -9499.675\n",
      "                , loss2: 0.3246279001235962\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 660 epoch, average loss: -28.16954650878906\n",
      "                , loss1: -9497.00703125\n",
      "                , loss2: 0.3214732646942139\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 670 epoch, average loss: -28.175961303710938\n",
      "                , loss1: -9499.078125\n",
      "                , loss2: 0.32127335071563723\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 680 epoch, average loss: -28.12468566894531\n",
      "                , loss1: -9495.9484375\n",
      "                , loss2: 0.3631599426269531\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 690 epoch, average loss: -28.180325317382813\n",
      "                , loss1: -9500.2046875\n",
      "                , loss2: 0.320289158821106\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 700 epoch, average loss: -28.181253051757814\n",
      "                , loss1: -9499.09140625\n",
      "                , loss2: 0.31602418422698975\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 710 epoch, average loss: -28.083099365234375\n",
      "                , loss1: -9492.70625\n",
      "                , loss2: 0.39501678943634033\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 720 epoch, average loss: -28.19703063964844\n",
      "                , loss1: -9497.4421875\n",
      "                , loss2: 0.2952988862991333\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 730 epoch, average loss: -28.189816284179688\n",
      "                , loss1: -9485.3203125\n",
      "                , loss2: 0.2661504507064819\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 740 epoch, average loss: -28.24285888671875\n",
      "                , loss1: -9494.01015625\n",
      "                , loss2: 0.23917107582092284\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 750 epoch, average loss: -28.204168701171874\n",
      "                , loss1: -9491.46484375\n",
      "                , loss2: 0.2702262163162231\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 760 epoch, average loss: -28.1532958984375\n",
      "                , loss1: -9495.4984375\n",
      "                , loss2: 0.333197808265686\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 770 epoch, average loss: -28.17908630371094\n",
      "                , loss1: -9499.8390625\n",
      "                , loss2: 0.32043192386627195\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 780 epoch, average loss: -28.180770874023438\n",
      "                , loss1: -9499.42890625\n",
      "                , loss2: 0.3175178050994873\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 790 epoch, average loss: -28.180270385742187\n",
      "                , loss1: -9500.0609375\n",
      "                , loss2: 0.3199113130569458\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 800 epoch, average loss: -28.115191650390624\n",
      "                , loss1: -9496.49921875\n",
      "                , loss2: 0.3743075132369995\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 810 epoch, average loss: -28.176657104492186\n",
      "                , loss1: -9498.2359375\n",
      "                , loss2: 0.3180493116378784\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 820 epoch, average loss: -28.18017578125\n",
      "                , loss1: -9499.10390625\n",
      "                , loss2: 0.3171307325363159\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 830 epoch, average loss: -28.180783081054688\n",
      "                , loss1: -9499.60625\n",
      "                , loss2: 0.318035364151001\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 840 epoch, average loss: -28.180328369140625\n",
      "                , loss1: -9498.6953125\n",
      "                , loss2: 0.31575794219970704\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 850 epoch, average loss: -28.18218078613281\n",
      "                , loss1: -9499.90078125\n",
      "                , loss2: 0.31752121448516846\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 860 epoch, average loss: -28.1727294921875\n",
      "                , loss1: -9496.99765625\n",
      "                , loss2: 0.3182631731033325\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 870 epoch, average loss: -28.18860778808594\n",
      "                , loss1: -9499.6484375\n",
      "                , loss2: 0.3103381633758545\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 880 epoch, average loss: -28.1991455078125\n",
      "                , loss1: -9499.0375\n",
      "                , loss2: 0.2979664087295532\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 890 epoch, average loss: -28.25096740722656\n",
      "                , loss1: -9491.38359375\n",
      "                , loss2: 0.22318317890167236\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 900 epoch, average loss: -28.254367065429687\n",
      "                , loss1: -9490.2296875\n",
      "                , loss2: 0.2163227081298828\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 910 epoch, average loss: -28.197906494140625\n",
      "                , loss1: -9489.3203125\n",
      "                , loss2: 0.27005462646484374\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 920 epoch, average loss: -28.19603271484375\n",
      "                , loss1: -9498.959375\n",
      "                , loss2: 0.3008429050445557\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 930 epoch, average loss: -28.215023803710938\n",
      "                , loss1: -9493.1875\n",
      "                , loss2: 0.2645429611206055\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 940 epoch, average loss: -28.15655517578125\n",
      "                , loss1: -9495.3359375\n",
      "                , loss2: 0.32945220470428466\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 950 epoch, average loss: -28.23658447265625\n",
      "                , loss1: -9492.0\n",
      "                , loss2: 0.23941447734832763\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 960 epoch, average loss: -28.254818725585938\n",
      "                , loss1: -9492.8875\n",
      "                , loss2: 0.22384321689605713\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 970 epoch, average loss: -28.130548095703126\n",
      "                , loss1: -9494.78046875\n",
      "                , loss2: 0.3537916898727417\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 980 epoch, average loss: -28.158322143554688\n",
      "                , loss1: -9499.715625\n",
      "                , loss2: 0.340827488899231\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 990 epoch, average loss: -28.177975463867188\n",
      "                , loss1: -9500.70625\n",
      "                , loss2: 0.3241452217102051\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1000 epoch, average loss: -28.180908203125\n",
      "                , loss1: -9500.290625\n",
      "                , loss2: 0.31996405124664307\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1010 epoch, average loss: -28.183538818359374\n",
      "                , loss1: -9500.3796875\n",
      "                , loss2: 0.3176008939743042\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1020 epoch, average loss: -28.111691284179688\n",
      "                , loss1: -9498.1671875\n",
      "                , loss2: 0.3828099727630615\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1030 epoch, average loss: -28.182666015625\n",
      "                , loss1: -9500.1078125\n",
      "                , loss2: 0.31765878200531006\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1040 epoch, average loss: -28.18732604980469\n",
      "                , loss1: -9499.99765625\n",
      "                , loss2: 0.3126645565032959\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1050 epoch, average loss: -28.18671875\n",
      "                , loss1: -9499.0578125\n",
      "                , loss2: 0.31045329570770264\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1060 epoch, average loss: -28.190863037109374\n",
      "                , loss1: -9499.3375\n",
      "                , loss2: 0.30714690685272217\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1070 epoch, average loss: -28.195022583007812\n",
      "                , loss1: -9495.4046875\n",
      "                , loss2: 0.2911923885345459\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1080 epoch, average loss: -28.248797607421874\n",
      "                , loss1: -9494.096875\n",
      "                , loss2: 0.23349363803863527\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1090 epoch, average loss: -28.2400634765625\n",
      "                , loss1: -9494.753125\n",
      "                , loss2: 0.24419806003570557\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1100 epoch, average loss: -28.265625\n",
      "                , loss1: -9488.47421875\n",
      "                , loss2: 0.19979583024978637\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1110 epoch, average loss: -28.200732421875\n",
      "                , loss1: -9499.49921875\n",
      "                , loss2: 0.29776582717895506\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1120 epoch, average loss: -28.188677978515624\n",
      "                , loss1: -9500.25859375\n",
      "                , loss2: 0.3120992183685303\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1130 epoch, average loss: -28.1921875\n",
      "                , loss1: -9500.48203125\n",
      "                , loss2: 0.30925688743591306\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1140 epoch, average loss: -28.21161804199219\n",
      "                , loss1: -9497.8875\n",
      "                , loss2: 0.2820427417755127\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1150 epoch, average loss: -28.191552734375\n",
      "                , loss1: -9500.4421875\n",
      "                , loss2: 0.309773325920105\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1160 epoch, average loss: -28.1972412109375\n",
      "                , loss1: -9500.046875\n",
      "                , loss2: 0.30289402008056643\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1170 epoch, average loss: -28.189682006835938\n",
      "                , loss1: -9498.47734375\n",
      "                , loss2: 0.30574922561645507\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1180 epoch, average loss: -28.2344970703125\n",
      "                , loss1: -9496.30078125\n",
      "                , loss2: 0.2544013261795044\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1190 epoch, average loss: -28.23258056640625\n",
      "                , loss1: -9493.959375\n",
      "                , loss2: 0.24929454326629638\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1200 epoch, average loss: -28.21451416015625\n",
      "                , loss1: -9488.9140625\n",
      "                , loss2: 0.2522254467010498\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1210 epoch, average loss: -28.011929321289063\n",
      "                , loss1: -9499.16171875\n",
      "                , loss2: 0.4855555534362793\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1220 epoch, average loss: -28.241607666015625\n",
      "                , loss1: -9491.7296875\n",
      "                , loss2: 0.23358628749847413\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1230 epoch, average loss: -28.257073974609376\n",
      "                , loss1: -9489.7859375\n",
      "                , loss2: 0.21228489875793458\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1240 epoch, average loss: -28.219708251953126\n",
      "                , loss1: -9497.87421875\n",
      "                , loss2: 0.2739166259765625\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1250 epoch, average loss: -28.24609375\n",
      "                , loss1: -9495.43984375\n",
      "                , loss2: 0.24022457599639893\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1260 epoch, average loss: -28.239276123046874\n",
      "                , loss1: -9492.35\n",
      "                , loss2: 0.237772536277771\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1270 epoch, average loss: -28.252166748046875\n",
      "                , loss1: -9495.4015625\n",
      "                , loss2: 0.234039568901062\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1280 epoch, average loss: -28.251242065429686\n",
      "                , loss1: -9490.9984375\n",
      "                , loss2: 0.22175199985504152\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1290 epoch, average loss: -28.207244873046875\n",
      "                , loss1: -9499.22109375\n",
      "                , loss2: 0.2904229640960693\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1300 epoch, average loss: -28.245733642578124\n",
      "                , loss1: -9495.53359375\n",
      "                , loss2: 0.2408681869506836\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1310 epoch, average loss: -28.17687072753906\n",
      "                , loss1: -9492.54765625\n",
      "                , loss2: 0.3007746696472168\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1320 epoch, average loss: -28.190798950195312\n",
      "                , loss1: -9500.81171875\n",
      "                , loss2: 0.31163573265075684\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1330 epoch, average loss: -28.187225341796875\n",
      "                , loss1: -9500.42578125\n",
      "                , loss2: 0.3140486478805542\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1340 epoch, average loss: -28.18889465332031\n",
      "                , loss1: -9501.0\n",
      "                , loss2: 0.31410536766052244\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1350 epoch, average loss: -28.19023742675781\n",
      "                , loss1: -9500.7\n",
      "                , loss2: 0.3118643283843994\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1360 epoch, average loss: -28.190652465820314\n",
      "                , loss1: -9501.4796875\n",
      "                , loss2: 0.3137859582901001\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1370 epoch, average loss: -28.186996459960938\n",
      "                , loss1: -9501.36953125\n",
      "                , loss2: 0.3171106815338135\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1380 epoch, average loss: -28.190185546875\n",
      "                , loss1: -9500.83515625\n",
      "                , loss2: 0.31231725215911865\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n",
      "in 1390 epoch, average loss: -28.193441772460936\n",
      "                , loss1: -9500.62265625\n",
      "                , loss2: 0.30842435359954834\n",
      "                , weight: 0.0029999999999993747\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m sub\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[28], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4094"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([81., 81., 80.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004132231404958678"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
