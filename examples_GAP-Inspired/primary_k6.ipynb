{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed) \n",
    "np.random.seed(seed)  \n",
    "random.seed(seed)  \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 5\n",
    "\n",
    "weight = 0.93\n",
    "lr = 4e-3\n",
    "sub = 0.002\n",
    "limit = 0.01\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 242, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\":256, \"out_channels\": 256, \"use_bn\":  True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers141\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\":  True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers142\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers143\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": True, \"drop_rate\": 0.05}\n",
    "\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer1231\"] = {\"in_channels\":256, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05} \n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1213\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":6, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  \n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12704, 242)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/primary\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=242, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (13): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut():\n",
    "    hgnn_trainer.eval()\n",
    "    outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "    outs_straight = StraightThroughEstimator.apply(outs)\n",
    "    G_clone = G.clone()\n",
    "    edges, _  = G_clone.e\n",
    "    cut = 0\n",
    "    for vertices in edges:\n",
    "        if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "            cut += 1\n",
    "        else:\n",
    "            G_clone.remove_hyperedges(vertices)\n",
    "    assert cut == G_clone.num_e\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: -1927.341796875\n",
      "                , loss1: -2268.8123046875\n",
      "                , loss2: 148.61162109375\n",
      "                , weight: 0.91\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 20 epoch, average loss: -5010.0484375\n",
      "                , loss1: -5927.089453125\n",
      "                , loss2: 315.93623046875\n",
      "                , weight: 0.89\n",
      "=================================\n",
      "in 30 epoch, average loss: -6426.838671875\n",
      "                , loss1: -8088.6015625\n",
      "                , loss2: 680.7044921875\n",
      "                , weight: 0.87\n",
      "=================================\n",
      "in 40 epoch, average loss: -6656.94609375\n",
      "                , loss1: -8629.984375\n",
      "                , loss2: 755.867919921875\n",
      "                , weight: 0.85\n",
      "=================================\n",
      "in 50 epoch, average loss: -6582.59921875\n",
      "                , loss1: -8751.41953125\n",
      "                , loss2: 759.776171875\n",
      "                , weight: 0.83\n",
      "=================================\n",
      "in 60 epoch, average loss: -6427.019921875\n",
      "                , loss1: -8776.328125\n",
      "                , loss2: 760.771923828125\n",
      "                , weight: 0.8099999999999999\n",
      "=================================\n",
      "in 70 epoch, average loss: -6257.79921875\n",
      "                , loss1: -8784.5359375\n",
      "                , loss2: 761.040087890625\n",
      "                , weight: 0.7899999999999999\n",
      "=================================\n",
      "in 80 epoch, average loss: -6085.51484375\n",
      "                , loss1: -8789.1296875\n",
      "                , loss2: 761.2140625\n",
      "                , weight: 0.7699999999999999\n",
      "=================================\n",
      "in 90 epoch, average loss: -5911.55078125\n",
      "                , loss1: -8791.5984375\n",
      "                , loss2: 761.2681640625\n",
      "                , weight: 0.7499999999999999\n",
      "=================================\n",
      "in 100 epoch, average loss: -5736.74375\n",
      "                , loss1: -8793.03828125\n",
      "                , loss2: 761.308544921875\n",
      "                , weight: 0.7299999999999999\n",
      "=================================\n",
      "in 110 epoch, average loss: -5561.702734375\n",
      "                , loss1: -8794.353125\n",
      "                , loss2: 761.43310546875\n",
      "                , weight: 0.7099999999999999\n",
      "=================================\n",
      "in 120 epoch, average loss: -5386.499609375\n",
      "                , loss1: -8795.3171875\n",
      "                , loss2: 761.4232421875\n",
      "                , weight: 0.6899999999999998\n",
      "=================================\n",
      "in 130 epoch, average loss: -5210.471875\n",
      "                , loss1: -8795.3421875\n",
      "                , loss2: 761.563720703125\n",
      "                , weight: 0.6699999999999998\n",
      "=================================\n",
      "in 140 epoch, average loss: -5035.56484375\n",
      "                , loss1: -8796.72890625\n",
      "                , loss2: 761.477099609375\n",
      "                , weight: 0.6499999999999998\n",
      "=================================\n",
      "in 150 epoch, average loss: -4859.97734375\n",
      "                , loss1: -8797.2953125\n",
      "                , loss2: 761.492041015625\n",
      "                , weight: 0.6299999999999998\n",
      "=================================\n",
      "in 160 epoch, average loss: -4683.99609375\n",
      "                , loss1: -8797.38671875\n",
      "                , loss2: 761.584375\n",
      "                , weight: 0.6099999999999998\n",
      "=================================\n",
      "in 170 epoch, average loss: -4508.5765625\n",
      "                , loss1: -8798.25859375\n",
      "                , loss2: 761.5787109375\n",
      "                , weight: 0.5899999999999997\n",
      "=================================\n",
      "in 180 epoch, average loss: -4332.75234375\n",
      "                , loss1: -8798.57109375\n",
      "                , loss2: 761.620654296875\n",
      "                , weight: 0.5699999999999997\n",
      "=================================\n",
      "in 190 epoch, average loss: -4156.946484375\n",
      "                , loss1: -8798.77734375\n",
      "                , loss2: 761.57021484375\n",
      "                , weight: 0.5499999999999997\n",
      "=================================\n",
      "in 200 epoch, average loss: -3981.254296875\n",
      "                , loss1: -8799.3078125\n",
      "                , loss2: 761.5703125\n",
      "                , weight: 0.5299999999999997\n",
      "=================================\n",
      "in 210 epoch, average loss: -3805.3640625\n",
      "                , loss1: -8799.546875\n",
      "                , loss2: 761.60087890625\n",
      "                , weight: 0.5099999999999997\n",
      "=================================\n",
      "in 220 epoch, average loss: -3629.384375\n",
      "                , loss1: -8799.68359375\n",
      "                , loss2: 761.6568359375\n",
      "                , weight: 0.48999999999999966\n",
      "=================================\n",
      "in 230 epoch, average loss: -3453.619140625\n",
      "                , loss1: -8800.0546875\n",
      "                , loss2: 761.607568359375\n",
      "                , weight: 0.46999999999999964\n",
      "=================================\n",
      "in 240 epoch, average loss: -3277.7\n",
      "                , loss1: -8800.2984375\n",
      "                , loss2: 761.6373046875\n",
      "                , weight: 0.4499999999999996\n",
      "=================================\n",
      "in 250 epoch, average loss: -3101.955078125\n",
      "                , loss1: -8800.87578125\n",
      "                , loss2: 761.629248046875\n",
      "                , weight: 0.4299999999999996\n",
      "=================================\n",
      "in 260 epoch, average loss: -2925.9705078125\n",
      "                , loss1: -8800.92578125\n",
      "                , loss2: 761.617431640625\n",
      "                , weight: 0.4099999999999996\n",
      "=================================\n",
      "in 270 epoch, average loss: -2749.7923828125\n",
      "                , loss1: -8800.61171875\n",
      "                , loss2: 761.650146484375\n",
      "                , weight: 0.38999999999999957\n",
      "=================================\n",
      "in 280 epoch, average loss: -2573.824609375\n",
      "                , loss1: -8800.7328125\n",
      "                , loss2: 761.6533203125\n",
      "                , weight: 0.36999999999999955\n",
      "=================================\n",
      "in 290 epoch, average loss: -2397.9470703125\n",
      "                , loss1: -8801.084375\n",
      "                , loss2: 761.641455078125\n",
      "                , weight: 0.34999999999999953\n",
      "=================================\n",
      "in 300 epoch, average loss: -2255.14296875\n",
      "                , loss1: -8701.8796875\n",
      "                , loss2: 695.484765625\n",
      "                , weight: 0.3299999999999995\n",
      "=================================\n",
      "in 310 epoch, average loss: -2296.2296875\n",
      "                , loss1: -8528.68828125\n",
      "                , loss2: 424.33916015625\n",
      "                , weight: 0.3099999999999995\n",
      "=================================\n",
      "in 320 epoch, average loss: -2132.3314453125\n",
      "                , loss1: -8554.60390625\n",
      "                , loss2: 425.479296875\n",
      "                , weight: 0.2899999999999995\n",
      "=================================\n",
      "in 330 epoch, average loss: -1962.830859375\n",
      "                , loss1: -8560.7484375\n",
      "                , loss2: 425.6138671875\n",
      "                , weight: 0.26999999999999946\n",
      "=================================\n",
      "in 340 epoch, average loss: -1792.2171875\n",
      "                , loss1: -8563.17421875\n",
      "                , loss2: 425.6423828125\n",
      "                , weight: 0.24999999999999944\n",
      "=================================\n",
      "in 350 epoch, average loss: -1621.128515625\n",
      "                , loss1: -8563.9953125\n",
      "                , loss2: 425.66611328125\n",
      "                , weight: 0.22999999999999943\n",
      "=================================\n",
      "in 360 epoch, average loss: -1449.98837890625\n",
      "                , loss1: -8564.678125\n",
      "                , loss2: 425.6779296875\n",
      "                , weight: 0.2099999999999994\n",
      "=================================\n",
      "in 370 epoch, average loss: -1278.663671875\n",
      "                , loss1: -8564.69296875\n",
      "                , loss2: 425.710205078125\n",
      "                , weight: 0.1899999999999994\n",
      "=================================\n",
      "in 380 epoch, average loss: -1107.6\n",
      "                , loss1: -8565.934375\n",
      "                , loss2: 425.702978515625\n",
      "                , weight: 0.16999999999999937\n",
      "=================================\n",
      "in 390 epoch, average loss: -936.2734375\n",
      "                , loss1: -8565.8765625\n",
      "                , loss2: 425.702490234375\n",
      "                , weight: 0.14999999999999936\n",
      "=================================\n",
      "in 400 epoch, average loss: -764.954443359375\n",
      "                , loss1: -8565.98203125\n",
      "                , loss2: 425.715234375\n",
      "                , weight: 0.12999999999999934\n",
      "=================================\n",
      "in 410 epoch, average loss: -593.650537109375\n",
      "                , loss1: -8566.046875\n",
      "                , loss2: 425.707861328125\n",
      "                , weight: 0.10999999999999932\n",
      "=================================\n",
      "in 420 epoch, average loss: -422.368359375\n",
      "                , loss1: -8566.146875\n",
      "                , loss2: 425.68310546875\n",
      "                , weight: 0.0899999999999993\n",
      "=================================\n",
      "in 430 epoch, average loss: -251.0261962890625\n",
      "                , loss1: -8566.06796875\n",
      "                , loss2: 425.691796875\n",
      "                , weight: 0.06999999999999929\n",
      "=================================\n",
      "in 440 epoch, average loss: -79.7134033203125\n",
      "                , loss1: -8566.215625\n",
      "                , loss2: 425.69345703125\n",
      "                , weight: 0.04999999999999927\n",
      "=================================\n",
      "in 450 epoch, average loss: 91.621533203125\n",
      "                , loss1: -8565.38203125\n",
      "                , loss2: 425.675244140625\n",
      "                , weight: 0.02999999999999925\n",
      "=================================\n",
      "in 460 epoch, average loss: 262.891650390625\n",
      "                , loss1: -8565.6421875\n",
      "                , loss2: 425.63740234375\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 470 epoch, average loss: 337.2219482421875\n",
      "                , loss1: -8489.553125\n",
      "                , loss2: 422.117431640625\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 480 epoch, average loss: 307.9022216796875\n",
      "                , loss1: -8255.6625\n",
      "                , loss2: 390.458837890625\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 490 epoch, average loss: 87.29383544921875\n",
      "                , loss1: -6449.92109375\n",
      "                , loss2: 151.7930419921875\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 500 epoch, average loss: -51.050759887695314\n",
      "                , loss1: -6467.12265625\n",
      "                , loss2: 13.620468139648438\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 510 epoch, average loss: -61.86424560546875\n",
      "                , loss1: -6967.2578125\n",
      "                , loss2: 7.808330535888672\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 520 epoch, average loss: -71.103857421875\n",
      "                , loss1: -7327.653125\n",
      "                , loss2: 2.172672462463379\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 530 epoch, average loss: -73.2531494140625\n",
      "                , loss1: -7474.05\n",
      "                , loss2: 1.4873523712158203\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 540 epoch, average loss: -74.02833251953125\n",
      "                , loss1: -7486.90625\n",
      "                , loss2: 0.8407383918762207\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 550 epoch, average loss: -74.40440673828125\n",
      "                , loss1: -7507.40625\n",
      "                , loss2: 0.6696553230285645\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 560 epoch, average loss: -74.5546142578125\n",
      "                , loss1: -7527.021875\n",
      "                , loss2: 0.7156037807464599\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 570 epoch, average loss: -74.67997436523437\n",
      "                , loss1: -7529.41171875\n",
      "                , loss2: 0.6141379833221435\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 580 epoch, average loss: -74.6947265625\n",
      "                , loss1: -7521.2671875\n",
      "                , loss2: 0.5179329872131347\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 590 epoch, average loss: -74.87839965820312\n",
      "                , loss1: -7539.3703125\n",
      "                , loss2: 0.5152970790863037\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 600 epoch, average loss: -74.94503784179688\n",
      "                , loss1: -7545.309375\n",
      "                , loss2: 0.5080629825592041\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 610 epoch, average loss: -74.96735229492188\n",
      "                , loss1: -7549.97421875\n",
      "                , loss2: 0.532387638092041\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 620 epoch, average loss: -74.99420776367188\n",
      "                , loss1: -7551.6640625\n",
      "                , loss2: 0.5224336624145508\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 630 epoch, average loss: -75.03416748046875\n",
      "                , loss1: -7550.05703125\n",
      "                , loss2: 0.466402530670166\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 640 epoch, average loss: -75.0352783203125\n",
      "                , loss1: -7553.52578125\n",
      "                , loss2: 0.4999758243560791\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 650 epoch, average loss: -75.02243041992188\n",
      "                , loss1: -7558.6046875\n",
      "                , loss2: 0.563612985610962\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 660 epoch, average loss: -75.06153564453125\n",
      "                , loss1: -7548.309375\n",
      "                , loss2: 0.4215558052062988\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 670 epoch, average loss: -75.09508056640625\n",
      "                , loss1: -7550.4046875\n",
      "                , loss2: 0.408966588973999\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 680 epoch, average loss: -75.17760620117187\n",
      "                , loss1: -7550.9875\n",
      "                , loss2: 0.3322632074356079\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 690 epoch, average loss: -75.14434814453125\n",
      "                , loss1: -7549.1234375\n",
      "                , loss2: 0.34688358306884765\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 700 epoch, average loss: -75.15316772460938\n",
      "                , loss1: -7551.73828125\n",
      "                , loss2: 0.3642066717147827\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 710 epoch, average loss: -75.22274169921874\n",
      "                , loss1: -7551.303125\n",
      "                , loss2: 0.2902890920639038\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 720 epoch, average loss: -75.24462890625\n",
      "                , loss1: -7549.265625\n",
      "                , loss2: 0.2480222225189209\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 730 epoch, average loss: -75.259521484375\n",
      "                , loss1: -7550.4453125\n",
      "                , loss2: 0.2449409246444702\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 740 epoch, average loss: -75.27964477539062\n",
      "                , loss1: -7553.62890625\n",
      "                , loss2: 0.2566411018371582\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 750 epoch, average loss: -75.33263549804687\n",
      "                , loss1: -7559.3140625\n",
      "                , loss2: 0.2605046510696411\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 760 epoch, average loss: -75.33510131835938\n",
      "                , loss1: -7555.8609375\n",
      "                , loss2: 0.2235102415084839\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 770 epoch, average loss: -75.3141845703125\n",
      "                , loss1: -7561.0421875\n",
      "                , loss2: 0.29623589515686033\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 780 epoch, average loss: -75.32803344726562\n",
      "                , loss1: -7560.0640625\n",
      "                , loss2: 0.2726144790649414\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 790 epoch, average loss: -75.37219848632813\n",
      "                , loss1: -7563.66015625\n",
      "                , loss2: 0.26440062522888186\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 800 epoch, average loss: -75.33546142578125\n",
      "                , loss1: -7555.709375\n",
      "                , loss2: 0.22163987159729004\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 810 epoch, average loss: -75.38445434570312\n",
      "                , loss1: -7564.93984375\n",
      "                , loss2: 0.26494832038879396\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 820 epoch, average loss: -75.41492919921875\n",
      "                , loss1: -7562.15546875\n",
      "                , loss2: 0.20663061141967773\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 830 epoch, average loss: -75.41182250976563\n",
      "                , loss1: -7564.2765625\n",
      "                , loss2: 0.23094913959503174\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 840 epoch, average loss: -75.4302490234375\n",
      "                , loss1: -7565.19921875\n",
      "                , loss2: 0.22174737453460694\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 850 epoch, average loss: -75.43818969726563\n",
      "                , loss1: -7564.9359375\n",
      "                , loss2: 0.2111670970916748\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 860 epoch, average loss: -75.44393310546874\n",
      "                , loss1: -7563.48984375\n",
      "                , loss2: 0.19095369577407836\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 870 epoch, average loss: -75.47289428710937\n",
      "                , loss1: -7567.5171875\n",
      "                , loss2: 0.20227112770080566\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 880 epoch, average loss: -75.4349609375\n",
      "                , loss1: -7569.31875\n",
      "                , loss2: 0.2582282304763794\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 890 epoch, average loss: -75.49349975585938\n",
      "                , loss1: -7568.971875\n",
      "                , loss2: 0.1962154507637024\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 900 epoch, average loss: -75.46121826171876\n",
      "                , loss1: -7569.53359375\n",
      "                , loss2: 0.23410844802856445\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 910 epoch, average loss: -75.45698852539063\n",
      "                , loss1: -7570.53125\n",
      "                , loss2: 0.24831976890563964\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 920 epoch, average loss: -75.48339233398437\n",
      "                , loss1: -7567.33125\n",
      "                , loss2: 0.1899195671081543\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 930 epoch, average loss: -75.46296997070313\n",
      "                , loss1: -7572.43515625\n",
      "                , loss2: 0.26137828826904297\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 940 epoch, average loss: -75.46350708007813\n",
      "                , loss1: -7572.89375\n",
      "                , loss2: 0.26543145179748534\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 950 epoch, average loss: -75.512548828125\n",
      "                , loss1: -7569.15625\n",
      "                , loss2: 0.17900556325912476\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 960 epoch, average loss: -75.53934936523437\n",
      "                , loss1: -7571.871875\n",
      "                , loss2: 0.17936980724334717\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 970 epoch, average loss: -75.52213134765626\n",
      "                , loss1: -7573.8109375\n",
      "                , loss2: 0.2159660816192627\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 980 epoch, average loss: -75.50429077148438\n",
      "                , loss1: -7571.8234375\n",
      "                , loss2: 0.21395909786224365\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 990 epoch, average loss: -75.55357666015625\n",
      "                , loss1: -7573.196875\n",
      "                , loss2: 0.17838642597198487\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1000 epoch, average loss: -75.52122802734375\n",
      "                , loss1: -7573.00234375\n",
      "                , loss2: 0.20879895687103273\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1010 epoch, average loss: -75.54192504882812\n",
      "                , loss1: -7573.90234375\n",
      "                , loss2: 0.1971001386642456\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1020 epoch, average loss: -75.5145263671875\n",
      "                , loss1: -7574.41171875\n",
      "                , loss2: 0.2295919179916382\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1030 epoch, average loss: -75.5577392578125\n",
      "                , loss1: -7573.31171875\n",
      "                , loss2: 0.17538543939590454\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1040 epoch, average loss: -75.56150512695312\n",
      "                , loss1: -7575.64375\n",
      "                , loss2: 0.19492552280426026\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1050 epoch, average loss: -75.55144653320312\n",
      "                , loss1: -7574.46015625\n",
      "                , loss2: 0.19314998388290405\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1060 epoch, average loss: -75.58607177734375\n",
      "                , loss1: -7575.7828125\n",
      "                , loss2: 0.17174817323684693\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1070 epoch, average loss: -75.577783203125\n",
      "                , loss1: -7576.165625\n",
      "                , loss2: 0.1838827133178711\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1080 epoch, average loss: -75.5786376953125\n",
      "                , loss1: -7576.66015625\n",
      "                , loss2: 0.18796205520629883\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1090 epoch, average loss: -75.581689453125\n",
      "                , loss1: -7577.6015625\n",
      "                , loss2: 0.1943185806274414\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1100 epoch, average loss: -75.57965698242188\n",
      "                , loss1: -7579.00859375\n",
      "                , loss2: 0.21043045520782472\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1110 epoch, average loss: -75.57584228515626\n",
      "                , loss1: -7578.05078125\n",
      "                , loss2: 0.2046589136123657\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1120 epoch, average loss: -75.54285888671875\n",
      "                , loss1: -7578.959375\n",
      "                , loss2: 0.2467484951019287\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1130 epoch, average loss: -75.59976196289062\n",
      "                , loss1: -7578.96328125\n",
      "                , loss2: 0.1898744821548462\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1140 epoch, average loss: -75.59215087890625\n",
      "                , loss1: -7579.4453125\n",
      "                , loss2: 0.2023020029067993\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1150 epoch, average loss: -75.63917236328125\n",
      "                , loss1: -7581.70859375\n",
      "                , loss2: 0.17790921926498413\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1160 epoch, average loss: -75.63151245117187\n",
      "                , loss1: -7579.72109375\n",
      "                , loss2: 0.165693199634552\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1170 epoch, average loss: -75.65380859375\n",
      "                , loss1: -7581.196875\n",
      "                , loss2: 0.15815590620040892\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1180 epoch, average loss: -75.63302001953124\n",
      "                , loss1: -7581.48984375\n",
      "                , loss2: 0.1818734288215637\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1190 epoch, average loss: -75.6321533203125\n",
      "                , loss1: -7583.415625\n",
      "                , loss2: 0.2020055055618286\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1200 epoch, average loss: -75.63344116210938\n",
      "                , loss1: -7583.35703125\n",
      "                , loss2: 0.20012426376342773\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1210 epoch, average loss: -75.66383666992188\n",
      "                , loss1: -7584.1546875\n",
      "                , loss2: 0.17770732641220094\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1220 epoch, average loss: -75.67916870117188\n",
      "                , loss1: -7586.9921875\n",
      "                , loss2: 0.19074597358703613\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1230 epoch, average loss: -75.72799682617188\n",
      "                , loss1: -7588.628125\n",
      "                , loss2: 0.15827873945236207\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1240 epoch, average loss: -75.72478637695312\n",
      "                , loss1: -7588.859375\n",
      "                , loss2: 0.16380399465560913\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1250 epoch, average loss: -75.70537109375\n",
      "                , loss1: -7589.20625\n",
      "                , loss2: 0.1866831064224243\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1260 epoch, average loss: -75.73413696289063\n",
      "                , loss1: -7590.12421875\n",
      "                , loss2: 0.16710227727890015\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1270 epoch, average loss: -75.731103515625\n",
      "                , loss1: -7589.42578125\n",
      "                , loss2: 0.16314886808395385\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1280 epoch, average loss: -75.7614501953125\n",
      "                , loss1: -7591.14375\n",
      "                , loss2: 0.1499936103820801\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1290 epoch, average loss: -75.75444946289062\n",
      "                , loss1: -7591.03828125\n",
      "                , loss2: 0.15592823028564454\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1300 epoch, average loss: -75.7532958984375\n",
      "                , loss1: -7591.3015625\n",
      "                , loss2: 0.15971485376358033\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1310 epoch, average loss: -75.76259765625\n",
      "                , loss1: -7591.953125\n",
      "                , loss2: 0.15694072246551513\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1320 epoch, average loss: -75.75819091796875\n",
      "                , loss1: -7591.60625\n",
      "                , loss2: 0.157867693901062\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1330 epoch, average loss: -75.77229614257813\n",
      "                , loss1: -7592.709375\n",
      "                , loss2: 0.15479789972305297\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1340 epoch, average loss: -75.77648315429687\n",
      "                , loss1: -7593.33125\n",
      "                , loss2: 0.1568300247192383\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1350 epoch, average loss: -75.78077392578125\n",
      "                , loss1: -7593.8171875\n",
      "                , loss2: 0.1573876142501831\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1360 epoch, average loss: -75.76918334960938\n",
      "                , loss1: -7593.78828125\n",
      "                , loss2: 0.16870651245117188\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1370 epoch, average loss: -75.77350463867188\n",
      "                , loss1: -7592.53984375\n",
      "                , loss2: 0.1518934488296509\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1380 epoch, average loss: -75.79267578125\n",
      "                , loss1: -7594.00078125\n",
      "                , loss2: 0.14733242988586426\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1390 epoch, average loss: -75.78558959960938\n",
      "                , loss1: -7593.578125\n",
      "                , loss2: 0.15019567012786866\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1400 epoch, average loss: -75.78187866210938\n",
      "                , loss1: -7593.440625\n",
      "                , loss2: 0.15252985954284667\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1410 epoch, average loss: -75.77645263671874\n",
      "                , loss1: -7592.7109375\n",
      "                , loss2: 0.15065231323242187\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1420 epoch, average loss: -75.79560546875\n",
      "                , loss1: -7595.28359375\n",
      "                , loss2: 0.15723059177398682\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1430 epoch, average loss: -75.79034423828125\n",
      "                , loss1: -7594.68984375\n",
      "                , loss2: 0.15654296875\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1440 epoch, average loss: -75.7944580078125\n",
      "                , loss1: -7594.1015625\n",
      "                , loss2: 0.14655523300170897\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1450 epoch, average loss: -75.79202270507812\n",
      "                , loss1: -7594.81640625\n",
      "                , loss2: 0.15613340139389037\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1460 epoch, average loss: -75.77838134765625\n",
      "                , loss1: -7593.1765625\n",
      "                , loss2: 0.15337889194488524\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1470 epoch, average loss: -75.79431762695313\n",
      "                , loss1: -7595.09921875\n",
      "                , loss2: 0.15667541027069093\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1480 epoch, average loss: -75.800732421875\n",
      "                , loss1: -7595.44609375\n",
      "                , loss2: 0.1537257432937622\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1490 epoch, average loss: -75.79360961914062\n",
      "                , loss1: -7595.19375\n",
      "                , loss2: 0.15833507776260375\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1500 epoch, average loss: -75.7967529296875\n",
      "                , loss1: -7594.4296875\n",
      "                , loss2: 0.14754505157470704\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1510 epoch, average loss: -75.79707641601563\n",
      "                , loss1: -7594.5859375\n",
      "                , loss2: 0.14877867698669434\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1520 epoch, average loss: -75.79601440429687\n",
      "                , loss1: -7594.4796875\n",
      "                , loss2: 0.14879209995269777\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1530 epoch, average loss: -75.79336547851562\n",
      "                , loss1: -7595.91640625\n",
      "                , loss2: 0.1657949924468994\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1540 epoch, average loss: -75.79268798828124\n",
      "                , loss1: -7595.4890625\n",
      "                , loss2: 0.1621963620185852\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1550 epoch, average loss: -75.79879760742188\n",
      "                , loss1: -7595.00234375\n",
      "                , loss2: 0.15122183561325073\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1560 epoch, average loss: -75.80498046875\n",
      "                , loss1: -7595.57421875\n",
      "                , loss2: 0.1507555365562439\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1570 epoch, average loss: -75.80007934570312\n",
      "                , loss1: -7595.58046875\n",
      "                , loss2: 0.1557191014289856\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1580 epoch, average loss: -75.8107666015625\n",
      "                , loss1: -7596.0609375\n",
      "                , loss2: 0.14984571933746338\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1590 epoch, average loss: -75.798046875\n",
      "                , loss1: -7595.22734375\n",
      "                , loss2: 0.15422525405883789\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1600 epoch, average loss: -75.811572265625\n",
      "                , loss1: -7596.5203125\n",
      "                , loss2: 0.1536329984664917\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1610 epoch, average loss: -75.80765991210937\n",
      "                , loss1: -7595.7328125\n",
      "                , loss2: 0.14965990781784058\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1620 epoch, average loss: -75.81768188476562\n",
      "                , loss1: -7596.940625\n",
      "                , loss2: 0.15171879529953003\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1630 epoch, average loss: -75.80645141601562\n",
      "                , loss1: -7596.33359375\n",
      "                , loss2: 0.15688284635543823\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1640 epoch, average loss: -75.8098876953125\n",
      "                , loss1: -7595.8703125\n",
      "                , loss2: 0.14882510900497437\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1650 epoch, average loss: -75.82261352539062\n",
      "                , loss1: -7596.83359375\n",
      "                , loss2: 0.1457115650177002\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1660 epoch, average loss: -75.8114990234375\n",
      "                , loss1: -7596.390625\n",
      "                , loss2: 0.15240302085876464\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1670 epoch, average loss: -75.81182861328125\n",
      "                , loss1: -7596.7875\n",
      "                , loss2: 0.1560459852218628\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1680 epoch, average loss: -75.81405029296874\n",
      "                , loss1: -7596.62890625\n",
      "                , loss2: 0.1522398352622986\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1690 epoch, average loss: -75.81384887695313\n",
      "                , loss1: -7597.14765625\n",
      "                , loss2: 0.15762970447540284\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1700 epoch, average loss: -75.81644287109376\n",
      "                , loss1: -7596.309375\n",
      "                , loss2: 0.14664660692214965\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1710 epoch, average loss: -75.81821899414062\n",
      "                , loss1: -7596.7109375\n",
      "                , loss2: 0.1488816738128662\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1720 epoch, average loss: -75.818505859375\n",
      "                , loss1: -7597.021875\n",
      "                , loss2: 0.15170596837997435\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1730 epoch, average loss: -75.82012939453125\n",
      "                , loss1: -7597.1859375\n",
      "                , loss2: 0.151739764213562\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1740 epoch, average loss: -75.812451171875\n",
      "                , loss1: -7596.08828125\n",
      "                , loss2: 0.1484357714653015\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1750 epoch, average loss: -75.8192138671875\n",
      "                , loss1: -7596.7609375\n",
      "                , loss2: 0.1483907699584961\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1760 epoch, average loss: -75.82119750976562\n",
      "                , loss1: -7597.26796875\n",
      "                , loss2: 0.15148142576217652\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1770 epoch, average loss: -75.811474609375\n",
      "                , loss1: -7596.16796875\n",
      "                , loss2: 0.15019758939743041\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1780 epoch, average loss: -75.825732421875\n",
      "                , loss1: -7598.903125\n",
      "                , loss2: 0.16329993009567262\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1790 epoch, average loss: -75.81832275390624\n",
      "                , loss1: -7597.3875\n",
      "                , loss2: 0.15554227828979492\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1800 epoch, average loss: -75.827880859375\n",
      "                , loss1: -7597.30625\n",
      "                , loss2: 0.1451771378517151\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1810 epoch, average loss: -75.82734375\n",
      "                , loss1: -7597.90078125\n",
      "                , loss2: 0.15165889263153076\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1820 epoch, average loss: -75.82041625976562\n",
      "                , loss1: -7597.61953125\n",
      "                , loss2: 0.15577541589736937\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1830 epoch, average loss: -75.82747192382813\n",
      "                , loss1: -7598.8203125\n",
      "                , loss2: 0.16071858406066894\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1840 epoch, average loss: -75.82791137695312\n",
      "                , loss1: -7596.93828125\n",
      "                , loss2: 0.14146419763565063\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1850 epoch, average loss: -75.8318603515625\n",
      "                , loss1: -7597.65859375\n",
      "                , loss2: 0.14472603797912598\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1860 epoch, average loss: -75.79778442382812\n",
      "                , loss1: -7598.02734375\n",
      "                , loss2: 0.18248220682144164\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1870 epoch, average loss: -75.8248779296875\n",
      "                , loss1: -7597.6125\n",
      "                , loss2: 0.15124391317367553\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1880 epoch, average loss: -75.81611938476563\n",
      "                , loss1: -7597.57265625\n",
      "                , loss2: 0.15961328744888306\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1890 epoch, average loss: -75.82739868164063\n",
      "                , loss1: -7597.896875\n",
      "                , loss2: 0.15156763792037964\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1900 epoch, average loss: -75.8312744140625\n",
      "                , loss1: -7597.79375\n",
      "                , loss2: 0.1466511607170105\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1910 epoch, average loss: -75.83117065429687\n",
      "                , loss1: -7597.8671875\n",
      "                , loss2: 0.1474888801574707\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1920 epoch, average loss: -75.83092041015625\n",
      "                , loss1: -7598.85859375\n",
      "                , loss2: 0.15765656232833863\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1930 epoch, average loss: -75.83078002929688\n",
      "                , loss1: -7598.328125\n",
      "                , loss2: 0.15250082015991212\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1940 epoch, average loss: -75.82333984375\n",
      "                , loss1: -7598.09140625\n",
      "                , loss2: 0.15756771564483643\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1950 epoch, average loss: -75.8312255859375\n",
      "                , loss1: -7597.42421875\n",
      "                , loss2: 0.14301743507385253\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1960 epoch, average loss: -75.83831787109375\n",
      "                , loss1: -7598.98671875\n",
      "                , loss2: 0.15154438018798827\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1970 epoch, average loss: -75.83424072265625\n",
      "                , loss1: -7598.365625\n",
      "                , loss2: 0.14941773414611817\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1980 epoch, average loss: -75.84205932617188\n",
      "                , loss1: -7598.815625\n",
      "                , loss2: 0.14609493017196656\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 1990 epoch, average loss: -75.828125\n",
      "                , loss1: -7598.4890625\n",
      "                , loss2: 0.15675684213638305\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2000 epoch, average loss: -75.8248779296875\n",
      "                , loss1: -7598.4640625\n",
      "                , loss2: 0.15976706743240357\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2010 epoch, average loss: -75.83964233398437\n",
      "                , loss1: -7598.721875\n",
      "                , loss2: 0.14757294654846193\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2020 epoch, average loss: -75.83396606445312\n",
      "                , loss1: -7597.72578125\n",
      "                , loss2: 0.14328938722610474\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2030 epoch, average loss: -75.84510498046875\n",
      "                , loss1: -7599.090625\n",
      "                , loss2: 0.14580425024032592\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2040 epoch, average loss: -75.84591674804688\n",
      "                , loss1: -7599.396875\n",
      "                , loss2: 0.14803805351257324\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2050 epoch, average loss: -75.8490966796875\n",
      "                , loss1: -7599.634375\n",
      "                , loss2: 0.1472452998161316\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2060 epoch, average loss: -75.83990478515625\n",
      "                , loss1: -7599.521875\n",
      "                , loss2: 0.15531431436538695\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2070 epoch, average loss: -75.8626708984375\n",
      "                , loss1: -7600.33984375\n",
      "                , loss2: 0.14072893857955932\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2080 epoch, average loss: -75.85556640625\n",
      "                , loss1: -7599.50546875\n",
      "                , loss2: 0.13949248790740967\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2090 epoch, average loss: -75.85081787109375\n",
      "                , loss1: -7600.1046875\n",
      "                , loss2: 0.15023115873336793\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2100 epoch, average loss: -75.84970703125\n",
      "                , loss1: -7599.80859375\n",
      "                , loss2: 0.14837915897369386\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2110 epoch, average loss: -75.86175537109375\n",
      "                , loss1: -7599.87890625\n",
      "                , loss2: 0.13703351020812987\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2120 epoch, average loss: -75.86495971679688\n",
      "                , loss1: -7599.83671875\n",
      "                , loss2: 0.13340020179748535\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2130 epoch, average loss: -75.85849609375\n",
      "                , loss1: -7600.28984375\n",
      "                , loss2: 0.14440916776657103\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2140 epoch, average loss: -75.86752319335938\n",
      "                , loss1: -7599.66875\n",
      "                , loss2: 0.12916734218597412\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2150 epoch, average loss: -75.86401977539063\n",
      "                , loss1: -7599.94375\n",
      "                , loss2: 0.13541951179504394\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2160 epoch, average loss: -75.86137084960937\n",
      "                , loss1: -7600.13828125\n",
      "                , loss2: 0.14001092910766602\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2170 epoch, average loss: -75.86510009765625\n",
      "                , loss1: -7600.74765625\n",
      "                , loss2: 0.14238240718841552\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2180 epoch, average loss: -75.86403198242188\n",
      "                , loss1: -7600.515625\n",
      "                , loss2: 0.14111671447753907\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2190 epoch, average loss: -75.85894165039062\n",
      "                , loss1: -7599.91875\n",
      "                , loss2: 0.1402478575706482\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2200 epoch, average loss: -75.860302734375\n",
      "                , loss1: -7600.165625\n",
      "                , loss2: 0.14134688377380372\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2210 epoch, average loss: -75.87058715820312\n",
      "                , loss1: -7600.14140625\n",
      "                , loss2: 0.13082966804504395\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2220 epoch, average loss: -75.8708984375\n",
      "                , loss1: -7600.2640625\n",
      "                , loss2: 0.13173856735229492\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2230 epoch, average loss: -75.8717529296875\n",
      "                , loss1: -7599.96640625\n",
      "                , loss2: 0.12790613174438475\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2240 epoch, average loss: -75.864111328125\n",
      "                , loss1: -7599.8453125\n",
      "                , loss2: 0.13434419631958008\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2250 epoch, average loss: -75.87359619140625\n",
      "                , loss1: -7600.54765625\n",
      "                , loss2: 0.13188191652297973\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2260 epoch, average loss: -75.86913452148437\n",
      "                , loss1: -7600.4359375\n",
      "                , loss2: 0.13521728515625\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2270 epoch, average loss: -75.86937866210937\n",
      "                , loss1: -7600.6078125\n",
      "                , loss2: 0.13669952154159545\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2280 epoch, average loss: -75.8739990234375\n",
      "                , loss1: -7600.59921875\n",
      "                , loss2: 0.13198339939117432\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2290 epoch, average loss: -75.871533203125\n",
      "                , loss1: -7601.0671875\n",
      "                , loss2: 0.139146625995636\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2300 epoch, average loss: -75.86747436523437\n",
      "                , loss1: -7600.57421875\n",
      "                , loss2: 0.13826597929000856\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2310 epoch, average loss: -75.87739868164063\n",
      "                , loss1: -7600.84453125\n",
      "                , loss2: 0.1310509204864502\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2320 epoch, average loss: -75.8806396484375\n",
      "                , loss1: -7600.87265625\n",
      "                , loss2: 0.12808358669281006\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2330 epoch, average loss: -75.879052734375\n",
      "                , loss1: -7601.03203125\n",
      "                , loss2: 0.1312634229660034\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2340 epoch, average loss: -75.87259521484376\n",
      "                , loss1: -7600.6921875\n",
      "                , loss2: 0.13432189226150512\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2350 epoch, average loss: -75.87463989257813\n",
      "                , loss1: -7600.71875\n",
      "                , loss2: 0.13254964351654053\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2360 epoch, average loss: -75.88016357421876\n",
      "                , loss1: -7600.9578125\n",
      "                , loss2: 0.12940679788589476\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2370 epoch, average loss: -75.8751953125\n",
      "                , loss1: -7600.43515625\n",
      "                , loss2: 0.12915877103805543\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2380 epoch, average loss: -75.88046875\n",
      "                , loss1: -7601.24375\n",
      "                , loss2: 0.13196455240249633\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2390 epoch, average loss: -75.87891845703125\n",
      "                , loss1: -7601.19921875\n",
      "                , loss2: 0.1330607771873474\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2400 epoch, average loss: -75.87963256835937\n",
      "                , loss1: -7600.74609375\n",
      "                , loss2: 0.1278325915336609\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2410 epoch, average loss: -75.87863159179688\n",
      "                , loss1: -7600.71328125\n",
      "                , loss2: 0.12849681377410888\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2420 epoch, average loss: -75.87889404296875\n",
      "                , loss1: -7600.959375\n",
      "                , loss2: 0.13070328235626222\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2430 epoch, average loss: -75.87942504882812\n",
      "                , loss1: -7600.609375\n",
      "                , loss2: 0.12666947841644288\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2440 epoch, average loss: -75.880517578125\n",
      "                , loss1: -7600.8390625\n",
      "                , loss2: 0.12786332368850709\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2450 epoch, average loss: -75.87858276367187\n",
      "                , loss1: -7600.83359375\n",
      "                , loss2: 0.12974377870559692\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2460 epoch, average loss: -75.87877807617187\n",
      "                , loss1: -7601.0515625\n",
      "                , loss2: 0.13172847032546997\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2470 epoch, average loss: -75.88008422851563\n",
      "                , loss1: -7601.02109375\n",
      "                , loss2: 0.13012484312057496\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2480 epoch, average loss: -75.87941284179688\n",
      "                , loss1: -7600.73203125\n",
      "                , loss2: 0.12790485620498657\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2490 epoch, average loss: -75.88329467773437\n",
      "                , loss1: -7601.21640625\n",
      "                , loss2: 0.12887561321258545\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2500 epoch, average loss: -75.880322265625\n",
      "                , loss1: -7601.51953125\n",
      "                , loss2: 0.13487344980239868\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2510 epoch, average loss: -75.87453002929688\n",
      "                , loss1: -7601.5671875\n",
      "                , loss2: 0.14113794565200805\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2520 epoch, average loss: -75.876953125\n",
      "                , loss1: -7601.340625\n",
      "                , loss2: 0.13645458221435547\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2530 epoch, average loss: -75.88151245117187\n",
      "                , loss1: -7601.03984375\n",
      "                , loss2: 0.1288878083229065\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2540 epoch, average loss: -75.88063354492188\n",
      "                , loss1: -7600.5390625\n",
      "                , loss2: 0.12475070953369141\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2550 epoch, average loss: -75.88814697265624\n",
      "                , loss1: -7601.57890625\n",
      "                , loss2: 0.12764474153518676\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2560 epoch, average loss: -75.88460693359374\n",
      "                , loss1: -7601.565625\n",
      "                , loss2: 0.13104258775711058\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2570 epoch, average loss: -75.879833984375\n",
      "                , loss1: -7601.178125\n",
      "                , loss2: 0.13195401430130005\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2580 epoch, average loss: -75.88132934570312\n",
      "                , loss1: -7601.5421875\n",
      "                , loss2: 0.1340821385383606\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2590 epoch, average loss: -75.88864135742188\n",
      "                , loss1: -7601.14921875\n",
      "                , loss2: 0.12284486293792725\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2600 epoch, average loss: -75.875830078125\n",
      "                , loss1: -7600.95703125\n",
      "                , loss2: 0.13374444246292114\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2610 epoch, average loss: -75.88580932617188\n",
      "                , loss1: -7601.23828125\n",
      "                , loss2: 0.12656745910644532\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n",
      "in 2620 epoch, average loss: -75.88225708007812\n",
      "                , loss1: -7601.2296875\n",
      "                , loss2: 0.13003318309783934\n",
      "                , weight: 0.009999999999999237\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m sub\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[266], line 30\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[266], line 22\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     24\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition/examples/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1656\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e2v_drop_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[0;32m-> 1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2e_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2e_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv2e_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v(X, e2v_aggr, e2v_weight, drop_rate\u001b[38;5;241m=\u001b[39me2v_drop_rate)\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1419\u001b[0m, in \u001b[0;36mHypergraph.v2e\u001b[0;34m(self, X, aggr, v2e_weight, e_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``vertices to hyperedges``. The combination of ``v2e_aggregation`` and ``v2e_update``.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \n\u001b[1;32m   1411\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;124;03m    ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e_aggregation(X, aggr, v2e_weight, drop_rate\u001b[38;5;241m=\u001b[39mdrop_rate)\n\u001b[0;32m-> 1419\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2e_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1373\u001b[0m, in \u001b[0;36mHypergraph.v2e_update\u001b[0;34m(self, X, e_weight)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(X\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     e_weight \u001b[38;5;241m=\u001b[39m e_weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(1,20000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "    hgnn_trainer.train()\n",
    "    # if loss_1 < -8500 and loss_2 < 2 and cut() < 5072:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6484"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([40., 40., 41., 40., 40., 40.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004149377593360996"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
