{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 6\n",
    "#1304\n",
    "\n",
    "#  这是1420个超边的结果\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 4096, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 512, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "# h_hyper_prmts[\"convlayers122\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers123\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":6, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "\n",
    "# h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 7403, \"out_channels\": 4096, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# # h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 4096, \"out_channels\": 4096, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 4096, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# # h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers17\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# # l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":7403, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":512, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# # l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":512, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的矩阵形式.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7403, 2755)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dhg \n",
    "from dhg.data import Cooking200\n",
    "data = Cooking200()\n",
    "e_list = data[\"edge_list\"]\n",
    "num_v = data[\"num_vertices\"]\n",
    "G = dhg.Hypergraph(data[\"num_vertices\"],data[\"edge_list\"])\n",
    "num_v, data[\"num_edges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (17): ReLU()\n",
       "  (18): Dropout(p=0.05, inplace=False)\n",
       "  (19): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (20): ReLU()\n",
       "  (21): Dropout(p=0.05, inplace=False)\n",
       "  (22): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (23): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "\n",
    "hgnn_trainer.weight = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -1877.3966796875\n",
      "                , loss1: -126.3563232421875\n",
      "                , loss2: 6.828763580322265\n",
      "                , weight: 14.911999999976425\n",
      "=================================\n",
      "in 10 epoch, average loss: -16890.553125\n",
      "                , loss1: -1257.7578125\n",
      "                , loss2: 1255.85693359375\n",
      "                , weight: 14.031999999976433\n",
      "=================================\n",
      "in 20 epoch, average loss: -16781.8796875\n",
      "                , loss1: -1260.38046875\n",
      "                , loss2: 293.6218505859375\n",
      "                , weight: 13.151999999976441\n",
      "=================================\n",
      "in 30 epoch, average loss: -15898.3515625\n",
      "                , loss1: -1262.90361328125\n",
      "                , loss2: 99.93917846679688\n",
      "                , weight: 12.27199999997645\n",
      "=================================\n",
      "in 40 epoch, average loss: -14874.7734375\n",
      "                , loss1: -1264.7095703125\n",
      "                , loss2: 33.559774780273436\n",
      "                , weight: 11.391999999976457\n",
      "=================================\n",
      "in 50 epoch, average loss: -13778.9359375\n",
      "                , loss1: -1265.3216796875\n",
      "                , loss2: 23.183242797851562\n",
      "                , weight: 10.511999999976466\n",
      "=================================\n",
      "in 60 epoch, average loss: -12673.8171875\n",
      "                , loss1: -1265.5091796875\n",
      "                , loss2: 16.755621337890624\n",
      "                , weight: 9.631999999976474\n",
      "=================================\n",
      "in 70 epoch, average loss: -11552.85078125\n",
      "                , loss1: -1265.91875\n",
      "                , loss2: 27.7473388671875\n",
      "                , weight: 8.751999999976482\n",
      "=================================\n",
      "in 80 epoch, average loss: -10441.953125\n",
      "                , loss1: -1265.9689453125\n",
      "                , loss2: 25.077943420410158\n",
      "                , weight: 7.871999999976488\n",
      "=================================\n",
      "in 90 epoch, average loss: -9331.6125\n",
      "                , loss1: -1266.0119140625\n",
      "                , loss2: 21.664205932617186\n",
      "                , weight: 6.991999999976487\n",
      "=================================\n",
      "in 100 epoch, average loss: -8222.0625\n",
      "                , loss1: -1265.9685546875\n",
      "                , loss2: 16.845370483398437\n",
      "                , weight: 6.1119999999764865\n",
      "=================================\n",
      "in 110 epoch, average loss: -7108.58359375\n",
      "                , loss1: -1266.141796875\n",
      "                , loss2: 17.257012939453126\n",
      "                , weight: 5.231999999976486\n",
      "=================================\n",
      "in 120 epoch, average loss: -6299.587109375\n",
      "                , loss1: -1266.0294921875\n",
      "                , loss2: 23.471865844726562\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 130 epoch, average loss: -6271.094921875\n",
      "                , loss1: -1266.012890625\n",
      "                , loss2: 18.457115173339844\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 140 epoch, average loss: -6273.289453125\n",
      "                , loss1: -1265.8818359375\n",
      "                , loss2: 15.610768127441407\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 150 epoch, average loss: -6275.5578125\n",
      "                , loss1: -1265.8958984375\n",
      "                , loss2: 13.412741088867188\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 160 epoch, average loss: -6270.63515625\n",
      "                , loss1: -1266.041796875\n",
      "                , loss2: 19.060328674316406\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 170 epoch, average loss: -6262.9640625\n",
      "                , loss1: -1266.02265625\n",
      "                , loss2: 26.6361572265625\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 180 epoch, average loss: -6274.312109375\n",
      "                , loss1: -1266.076953125\n",
      "                , loss2: 15.558901977539062\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 190 epoch, average loss: -6281.787109375\n",
      "                , loss1: -1266.0875\n",
      "                , loss2: 8.136226654052734\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 200 epoch, average loss: -6274.146875\n",
      "                , loss1: -1266.00830078125\n",
      "                , loss2: 15.382344055175782\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 210 epoch, average loss: -6279.9078125\n",
      "                , loss1: -1266.00791015625\n",
      "                , loss2: 9.619404602050782\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 220 epoch, average loss: -6270.249609375\n",
      "                , loss1: -1266.149609375\n",
      "                , loss2: 19.981243896484376\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 230 epoch, average loss: -6275.3625\n",
      "                , loss1: -1266.21259765625\n",
      "                , loss2: 15.181216430664062\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 240 epoch, average loss: -6275.7\n",
      "                , loss1: -1266.19853515625\n",
      "                , loss2: 14.774452209472656\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 250 epoch, average loss: -6277.61328125\n",
      "                , loss1: -1265.973046875\n",
      "                , loss2: 11.7406005859375\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 260 epoch, average loss: -6277.362109375\n",
      "                , loss1: -1266.25322265625\n",
      "                , loss2: 13.383363342285156\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 270 epoch, average loss: -6274.46171875\n",
      "                , loss1: -1266.19951171875\n",
      "                , loss2: 16.017550659179687\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 280 epoch, average loss: -6274.714453125\n",
      "                , loss1: -1266.13046875\n",
      "                , loss2: 15.422171020507813\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 290 epoch, average loss: -6283.29296875\n",
      "                , loss1: -1266.22265625\n",
      "                , loss2: 7.30157699584961\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 300 epoch, average loss: -6280.60703125\n",
      "                , loss1: -1266.2640625\n",
      "                , loss2: 10.191690063476562\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 310 epoch, average loss: -6278.28359375\n",
      "                , loss1: -1266.2908203125\n",
      "                , loss2: 12.649095153808593\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 320 epoch, average loss: -6279.96484375\n",
      "                , loss1: -1266.33369140625\n",
      "                , loss2: 11.181210327148438\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 330 epoch, average loss: -6277.225390625\n",
      "                , loss1: -1266.30908203125\n",
      "                , loss2: 13.798361206054688\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 340 epoch, average loss: -6283.315234375\n",
      "                , loss1: -1266.33623046875\n",
      "                , loss2: 7.8424560546875\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 350 epoch, average loss: -6284.941015625\n",
      "                , loss1: -1266.3119140625\n",
      "                , loss2: 6.096029281616211\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 360 epoch, average loss: -6285.075390625\n",
      "                , loss1: -1266.3171875\n",
      "                , loss2: 5.9881141662597654\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 370 epoch, average loss: -6284.101171875\n",
      "                , loss1: -1266.194921875\n",
      "                , loss2: 6.355011367797852\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 380 epoch, average loss: -6286.8296875\n",
      "                , loss1: -1266.2462890625\n",
      "                , loss2: 3.8808971405029298\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 390 epoch, average loss: -6278.627734375\n",
      "                , loss1: -1266.20947265625\n",
      "                , loss2: 11.900494384765626\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 400 epoch, average loss: -6279.900390625\n",
      "                , loss1: -1266.3595703125\n",
      "                , loss2: 11.374371337890626\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 410 epoch, average loss: -6279.7625\n",
      "                , loss1: -1266.3005859375\n",
      "                , loss2: 11.219041442871093\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 420 epoch, average loss: -6282.65390625\n",
      "                , loss1: -1266.29091796875\n",
      "                , loss2: 8.278988647460938\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 430 epoch, average loss: -6279.4453125\n",
      "                , loss1: -1266.31513671875\n",
      "                , loss2: 11.608099365234375\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 440 epoch, average loss: -6273.641015625\n",
      "                , loss1: -1266.2423828125\n",
      "                , loss2: 17.050740051269532\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 450 epoch, average loss: -6284.12734375\n",
      "                , loss1: -1266.28984375\n",
      "                , loss2: 6.800627899169922\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 460 epoch, average loss: -6282.794140625\n",
      "                , loss1: -1266.1958984375\n",
      "                , loss2: 7.667748260498047\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 470 epoch, average loss: -6282.6875\n",
      "                , loss1: -1266.21689453125\n",
      "                , loss2: 7.877821350097657\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 480 epoch, average loss: -6284.721484375\n",
      "                , loss1: -1266.28095703125\n",
      "                , loss2: 6.162632751464844\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 490 epoch, average loss: -6283.617578125\n",
      "                , loss1: -1266.2998046875\n",
      "                , loss2: 7.359589385986328\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 500 epoch, average loss: -6278.929296875\n",
      "                , loss1: -1266.30703125\n",
      "                , loss2: 12.084844970703125\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 510 epoch, average loss: -6283.6078125\n",
      "                , loss1: -1266.32490234375\n",
      "                , loss2: 7.494132232666016\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 520 epoch, average loss: -6284.142578125\n",
      "                , loss1: -1266.2265625\n",
      "                , loss2: 6.470319366455078\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 530 epoch, average loss: -6287.18828125\n",
      "                , loss1: -1266.34599609375\n",
      "                , loss2: 4.018160629272461\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 540 epoch, average loss: -6287.45859375\n",
      "                , loss1: -1266.3423828125\n",
      "                , loss2: 3.7300437927246093\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 550 epoch, average loss: -6286.70859375\n",
      "                , loss1: -1266.2744140625\n",
      "                , loss2: 4.142311477661133\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 560 epoch, average loss: -6288.144140625\n",
      "                , loss1: -1266.357421875\n",
      "                , loss2: 3.1200576782226563\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 570 epoch, average loss: -6288.75625\n",
      "                , loss1: -1266.36162109375\n",
      "                , loss2: 2.5285682678222656\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 580 epoch, average loss: -6288.891015625\n",
      "                , loss1: -1266.3578125\n",
      "                , loss2: 2.3744380950927733\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 590 epoch, average loss: -6289.041796875\n",
      "                , loss1: -1266.33525390625\n",
      "                , loss2: 2.111772155761719\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 600 epoch, average loss: -6289.42109375\n",
      "                , loss1: -1266.36376953125\n",
      "                , loss2: 1.8741153717041015\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 610 epoch, average loss: -6289.616015625\n",
      "                , loss1: -1266.31240234375\n",
      "                , loss2: 1.4236090660095215\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 620 epoch, average loss: -6289.262109375\n",
      "                , loss1: -1266.33046875\n",
      "                , loss2: 1.8689170837402345\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 630 epoch, average loss: -6289.605078125\n",
      "                , loss1: -1266.360546875\n",
      "                , loss2: 1.673843002319336\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 640 epoch, average loss: -6290.03359375\n",
      "                , loss1: -1266.3693359375\n",
      "                , loss2: 1.2889623641967773\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 650 epoch, average loss: -6289.385546875\n",
      "                , loss1: -1266.36435546875\n",
      "                , loss2: 1.9125352859497071\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 660 epoch, average loss: -6290.1671875\n",
      "                , loss1: -1266.3783203125\n",
      "                , loss2: 1.2003714561462402\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 670 epoch, average loss: -6289.5265625\n",
      "                , loss1: -1266.37861328125\n",
      "                , loss2: 1.8422700881958007\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 680 epoch, average loss: -6289.81796875\n",
      "                , loss1: -1266.36826171875\n",
      "                , loss2: 1.498801040649414\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 690 epoch, average loss: -6290.030078125\n",
      "                , loss1: -1266.37919921875\n",
      "                , loss2: 1.3418946266174316\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 700 epoch, average loss: -6290.15703125\n",
      "                , loss1: -1266.3689453125\n",
      "                , loss2: 1.162827205657959\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 710 epoch, average loss: -6289.568359375\n",
      "                , loss1: -1266.36845703125\n",
      "                , loss2: 1.7506759643554688\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 720 epoch, average loss: -6290.0234375\n",
      "                , loss1: -1266.38466796875\n",
      "                , loss2: 1.3756203651428223\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 730 epoch, average loss: -6290.626953125\n",
      "                , loss1: -1266.3837890625\n",
      "                , loss2: 0.7674681186676026\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 740 epoch, average loss: -6290.3109375\n",
      "                , loss1: -1266.385546875\n",
      "                , loss2: 1.0928643226623536\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 750 epoch, average loss: -6290.432421875\n",
      "                , loss1: -1266.3779296875\n",
      "                , loss2: 0.9328201293945313\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 760 epoch, average loss: -6290.323828125\n",
      "                , loss1: -1266.36640625\n",
      "                , loss2: 0.9851781845092773\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 770 epoch, average loss: -6290.479296875\n",
      "                , loss1: -1266.3826171875\n",
      "                , loss2: 0.9097565650939942\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 780 epoch, average loss: -6290.5296875\n",
      "                , loss1: -1266.38603515625\n",
      "                , loss2: 0.8759886741638183\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 790 epoch, average loss: -6290.187890625\n",
      "                , loss1: -1266.37763671875\n",
      "                , loss2: 1.176200771331787\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 800 epoch, average loss: -6290.391796875\n",
      "                , loss1: -1266.38232421875\n",
      "                , loss2: 0.9956939697265625\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n",
      "in 810 epoch, average loss: -6288.88046875\n",
      "                , loss1: -1266.2638671875\n",
      "                , loss2: 1.9184967041015626\n",
      "                , weight: 4.9679999999764854\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.088\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[28], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(9000):\n",
    "    if hgnn_trainer.weight > 5:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.088\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1511"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1233., 1234., 1234., 1234., 1234., 1234.], device='cuda:1',\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00013508037282182898"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes:Tensor = outs_straight.sum(dim=0)\n",
    "print(num_nodes.item())\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
