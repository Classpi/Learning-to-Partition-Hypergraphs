{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 2\n",
    "\n",
    "weight = 0.93\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 8192, \"out_channels\": 4096, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 4096, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers141\"] = {\"in_channels\": 4096, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer113\"] = {\"in_channels\":2048, \"out_channels\":2048, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer13\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":2, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  \n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12704, 242)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/primary\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): ReLU()\n",
       "  (19): Dropout(p=0.05, inplace=False)\n",
       "  (20): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (21): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): ReLU()\n",
       "  (23): Dropout(p=0.05, inplace=False)\n",
       "  (24): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (25): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 64.3364013671875\n",
      "                , loss1: -343.460009765625\n",
      "                , loss2: 383.4450927734375\n",
      "                , weight: 0.9291\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: -2371.040234375\n",
      "                , loss1: -3738.822265625\n",
      "                , loss2: 1083.3318359375\n",
      "                , weight: 0.9200999999999999\n",
      "=================================\n",
      "in 20 epoch, average loss: -5003.951171875\n",
      "                , loss1: -5577.88125\n",
      "                , loss2: 98.9876708984375\n",
      "                , weight: 0.9110999999999998\n",
      "=================================\n",
      "in 30 epoch, average loss: -6267.28125\n",
      "                , loss1: -7089.9109375\n",
      "                , loss2: 156.63729248046874\n",
      "                , weight: 0.9020999999999997\n",
      "=================================\n",
      "in 40 epoch, average loss: -6798.61875\n",
      "                , loss1: -7679.46640625\n",
      "                , loss2: 90.64806518554687\n",
      "                , weight: 0.8930999999999996\n",
      "=================================\n",
      "in 50 epoch, average loss: -7171.79375\n",
      "                , loss1: -8183.3859375\n",
      "                , loss2: 95.97600708007812\n",
      "                , weight: 0.8840999999999994\n",
      "=================================\n",
      "in 60 epoch, average loss: -7455.8796875\n",
      "                , loss1: -8574.30390625\n",
      "                , loss2: 81.9582763671875\n",
      "                , weight: 0.8750999999999993\n",
      "=================================\n",
      "in 70 epoch, average loss: -7667.9875\n",
      "                , loss1: -8866.709375\n",
      "                , loss2: 47.17549133300781\n",
      "                , weight: 0.8660999999999992\n",
      "=================================\n",
      "in 80 epoch, average loss: -7812.36328125\n",
      "                , loss1: -9112.365625\n",
      "                , loss2: 34.55940246582031\n",
      "                , weight: 0.8570999999999991\n",
      "=================================\n",
      "in 90 epoch, average loss: -7915.5703125\n",
      "                , loss1: -9317.8484375\n",
      "                , loss2: 24.515797424316407\n",
      "                , weight: 0.848099999999999\n",
      "=================================\n",
      "in 100 epoch, average loss: -7980.5703125\n",
      "                , loss1: -9486.03125\n",
      "                , loss2: 17.464993286132813\n",
      "                , weight: 0.8390999999999988\n",
      "=================================\n",
      "in 110 epoch, average loss: -8008.48984375\n",
      "                , loss1: -9616.490625\n",
      "                , loss2: 13.026611328125\n",
      "                , weight: 0.8300999999999987\n",
      "=================================\n",
      "in 120 epoch, average loss: -8024.65078125\n",
      "                , loss1: -9737.10859375\n",
      "                , loss2: 9.845545196533203\n",
      "                , weight: 0.8210999999999986\n",
      "=================================\n",
      "in 130 epoch, average loss: -8017.50703125\n",
      "                , loss1: -9834.43515625\n",
      "                , loss2: 8.800535583496094\n",
      "                , weight: 0.8120999999999985\n",
      "=================================\n",
      "in 140 epoch, average loss: -8015.28671875\n",
      "                , loss1: -9947.096875\n",
      "                , loss2: 13.439048767089844\n",
      "                , weight: 0.8030999999999984\n",
      "=================================\n",
      "in 150 epoch, average loss: -7994.97890625\n",
      "                , loss1: -10040.07265625\n",
      "                , loss2: 18.43974609375\n",
      "                , weight: 0.7940999999999983\n",
      "=================================\n",
      "in 160 epoch, average loss: -7954.65078125\n",
      "                , loss1: -10101.70078125\n",
      "                , loss2: 17.06445770263672\n",
      "                , weight: 0.7850999999999981\n",
      "=================================\n",
      "in 170 epoch, average loss: -7913.48828125\n",
      "                , loss1: -10163.040625\n",
      "                , loss2: 15.170240783691407\n",
      "                , weight: 0.776099999999998\n",
      "=================================\n",
      "in 180 epoch, average loss: -7858.528125\n",
      "                , loss1: -10208.63984375\n",
      "                , loss2: 13.832403564453125\n",
      "                , weight: 0.7670999999999979\n",
      "=================================\n",
      "in 190 epoch, average loss: -7797.1\n",
      "                , loss1: -10246.78828125\n",
      "                , loss2: 12.462532043457031\n",
      "                , weight: 0.7580999999999978\n",
      "=================================\n",
      "in 200 epoch, average loss: -7735.12734375\n",
      "                , loss1: -10285.890625\n",
      "                , loss2: 11.67468490600586\n",
      "                , weight: 0.7490999999999977\n",
      "=================================\n",
      "in 210 epoch, average loss: -7666.4421875\n",
      "                , loss1: -10316.18359375\n",
      "                , loss2: 10.330757904052735\n",
      "                , weight: 0.7400999999999975\n",
      "=================================\n",
      "in 220 epoch, average loss: -7591.7703125\n",
      "                , loss1: -10340.2\n",
      "                , loss2: 9.812065124511719\n",
      "                , weight: 0.7310999999999974\n",
      "=================================\n",
      "in 230 epoch, average loss: -7520.61484375\n",
      "                , loss1: -10369.1875\n",
      "                , loss2: 8.945597076416016\n",
      "                , weight: 0.7220999999999973\n",
      "=================================\n",
      "in 240 epoch, average loss: -7443.67578125\n",
      "                , loss1: -10391.11171875\n",
      "                , loss2: 8.299017333984375\n",
      "                , weight: 0.7130999999999972\n",
      "=================================\n",
      "in 250 epoch, average loss: -7363.36796875\n",
      "                , loss1: -10409.09140625\n",
      "                , loss2: 7.817710113525391\n",
      "                , weight: 0.7040999999999971\n",
      "=================================\n",
      "in 260 epoch, average loss: -7281.58046875\n",
      "                , loss1: -10425.5578125\n",
      "                , loss2: 7.437418365478516\n",
      "                , weight: 0.6950999999999969\n",
      "=================================\n",
      "in 270 epoch, average loss: -7199.15859375\n",
      "                , loss1: -10441.36875\n",
      "                , loss2: 6.9350532531738285\n",
      "                , weight: 0.6860999999999968\n",
      "=================================\n",
      "in 280 epoch, average loss: -7116.771875\n",
      "                , loss1: -10457.771875\n",
      "                , loss2: 6.528904724121094\n",
      "                , weight: 0.6770999999999967\n",
      "=================================\n",
      "in 290 epoch, average loss: -7034.5\n",
      "                , loss1: -10474.84453125\n",
      "                , loss2: 6.159754943847656\n",
      "                , weight: 0.6680999999999966\n",
      "=================================\n",
      "in 300 epoch, average loss: -6945.15234375\n",
      "                , loss1: -10482.16328125\n",
      "                , loss2: 6.087787246704101\n",
      "                , weight: 0.6590999999999965\n",
      "=================================\n",
      "in 310 epoch, average loss: -6855.7984375\n",
      "                , loss1: -10489.34453125\n",
      "                , loss2: 5.805991744995117\n",
      "                , weight: 0.6500999999999963\n",
      "=================================\n",
      "in 320 epoch, average loss: -6770.14765625\n",
      "                , loss1: -10502.6828125\n",
      "                , loss2: 5.656948852539062\n",
      "                , weight: 0.6410999999999962\n",
      "=================================\n",
      "in 330 epoch, average loss: -6679.8875\n",
      "                , loss1: -10508.92109375\n",
      "                , loss2: 5.353770065307617\n",
      "                , weight: 0.6320999999999961\n",
      "=================================\n",
      "in 340 epoch, average loss: -6591.8859375\n",
      "                , loss1: -10519.14140625\n",
      "                , loss2: 5.1818279266357425\n",
      "                , weight: 0.623099999999996\n",
      "=================================\n",
      "in 350 epoch, average loss: -6501.40703125\n",
      "                , loss1: -10525.7421875\n",
      "                , loss2: 5.075881576538086\n",
      "                , weight: 0.6140999999999959\n",
      "=================================\n",
      "in 360 epoch, average loss: -6411.2515625\n",
      "                , loss1: -10532.890625\n",
      "                , loss2: 4.855956649780273\n",
      "                , weight: 0.6050999999999958\n",
      "=================================\n",
      "in 370 epoch, average loss: -6320.68046875\n",
      "                , loss1: -10539.7046875\n",
      "                , loss2: 4.718907165527344\n",
      "                , weight: 0.5960999999999956\n",
      "=================================\n",
      "in 380 epoch, average loss: -6229.405078125\n",
      "                , loss1: -10545.81015625\n",
      "                , loss2: 4.747801208496094\n",
      "                , weight: 0.5870999999999955\n",
      "=================================\n",
      "in 390 epoch, average loss: -6138.10703125\n",
      "                , loss1: -10551.74921875\n",
      "                , loss2: 4.591329956054688\n",
      "                , weight: 0.5780999999999954\n",
      "=================================\n",
      "in 400 epoch, average loss: -6047.33515625\n",
      "                , loss1: -10558.68046875\n",
      "                , loss2: 4.3680988311767575\n",
      "                , weight: 0.5690999999999953\n",
      "=================================\n",
      "in 410 epoch, average loss: -5955.3828125\n",
      "                , loss1: -10563.89765625\n",
      "                , loss2: 4.234854507446289\n",
      "                , weight: 0.5600999999999952\n",
      "=================================\n",
      "in 420 epoch, average loss: -5864.21875\n",
      "                , loss1: -10570.7625\n",
      "                , loss2: 4.138655471801758\n",
      "                , weight: 0.551099999999995\n",
      "=================================\n",
      "in 430 epoch, average loss: -5769.834765625\n",
      "                , loss1: -10572.1296875\n",
      "                , loss2: 4.131130218505859\n",
      "                , weight: 0.5420999999999949\n",
      "=================================\n",
      "in 440 epoch, average loss: -5677.410546875\n",
      "                , loss1: -10576.928125\n",
      "                , loss2: 3.987034225463867\n",
      "                , weight: 0.5330999999999948\n",
      "=================================\n",
      "in 450 epoch, average loss: -5584.49375\n",
      "                , loss1: -10581.01484375\n",
      "                , loss2: 3.8697017669677733\n",
      "                , weight: 0.5240999999999947\n",
      "=================================\n",
      "in 460 epoch, average loss: -5491.570703125\n",
      "                , loss1: -10585.4296875\n",
      "                , loss2: 3.8518398284912108\n",
      "                , weight: 0.5150999999999946\n",
      "=================================\n",
      "in 470 epoch, average loss: -5397.943359375\n",
      "                , loss1: -10588.571875\n",
      "                , loss2: 3.810491943359375\n",
      "                , weight: 0.5060999999999944\n",
      "=================================\n",
      "in 480 epoch, average loss: -5304.230078125\n",
      "                , loss1: -10591.56171875\n",
      "                , loss2: 3.7268096923828127\n",
      "                , weight: 0.4970999999999943\n",
      "=================================\n",
      "in 490 epoch, average loss: -5210.383984375\n",
      "                , loss1: -10594.43046875\n",
      "                , loss2: 3.6632694244384765\n",
      "                , weight: 0.4880999999999942\n",
      "=================================\n",
      "in 500 epoch, average loss: -5115.7421875\n",
      "                , loss1: -10595.95625\n",
      "                , loss2: 3.6917137145996093\n",
      "                , weight: 0.4790999999999941\n",
      "=================================\n",
      "in 510 epoch, average loss: -5021.45234375\n",
      "                , loss1: -10598.0484375\n",
      "                , loss2: 3.606694793701172\n",
      "                , weight: 0.47009999999999397\n",
      "=================================\n",
      "in 520 epoch, average loss: -4928.8453125\n",
      "                , loss1: -10603.7140625\n",
      "                , loss2: 3.4675876617431642\n",
      "                , weight: 0.46109999999999385\n",
      "=================================\n",
      "in 530 epoch, average loss: -4834.38125\n",
      "                , loss1: -10605.78125\n",
      "                , loss2: 3.449567413330078\n",
      "                , weight: 0.45209999999999373\n",
      "=================================\n",
      "in 540 epoch, average loss: -4740.094921875\n",
      "                , loss1: -10608.4171875\n",
      "                , loss2: 3.4565750122070313\n",
      "                , weight: 0.4430999999999936\n",
      "=================================\n",
      "in 550 epoch, average loss: -4644.94140625\n",
      "                , loss1: -10609.15234375\n",
      "                , loss2: 3.4557544708251955\n",
      "                , weight: 0.4340999999999935\n",
      "=================================\n",
      "in 560 epoch, average loss: -4550.72265625\n",
      "                , loss1: -10611.7546875\n",
      "                , loss2: 3.309254837036133\n",
      "                , weight: 0.42509999999999337\n",
      "=================================\n",
      "in 570 epoch, average loss: -4455.841015625\n",
      "                , loss1: -10613.43359375\n",
      "                , loss2: 3.393549346923828\n",
      "                , weight: 0.41609999999999325\n",
      "=================================\n",
      "in 580 epoch, average loss: -4361.391796875\n",
      "                , loss1: -10615.87890625\n",
      "                , loss2: 3.321257400512695\n",
      "                , weight: 0.40709999999999313\n",
      "=================================\n",
      "in 590 epoch, average loss: -4266.621875\n",
      "                , loss1: -10617.66171875\n",
      "                , loss2: 3.267071533203125\n",
      "                , weight: 0.398099999999993\n",
      "=================================\n",
      "in 600 epoch, average loss: -4171.5671875\n",
      "                , loss1: -10618.9625\n",
      "                , loss2: 3.2755252838134767\n",
      "                , weight: 0.3890999999999929\n",
      "=================================\n",
      "in 610 epoch, average loss: -4077.03828125\n",
      "                , loss1: -10621.4203125\n",
      "                , loss2: 3.1779687881469725\n",
      "                , weight: 0.3800999999999928\n",
      "=================================\n",
      "in 620 epoch, average loss: -3982.151171875\n",
      "                , loss1: -10623.253125\n",
      "                , loss2: 3.1629222869873046\n",
      "                , weight: 0.37109999999999266\n",
      "=================================\n",
      "in 630 epoch, average loss: -3887.00234375\n",
      "                , loss1: -10624.51875\n",
      "                , loss2: 3.161087226867676\n",
      "                , weight: 0.36209999999999254\n",
      "=================================\n",
      "in 640 epoch, average loss: -3792.116015625\n",
      "                , loss1: -10626.42578125\n",
      "                , loss2: 3.1113822937011717\n",
      "                , weight: 0.3530999999999924\n",
      "=================================\n",
      "in 650 epoch, average loss: -3696.92109375\n",
      "                , loss1: -10627.73046875\n",
      "                , loss2: 3.1225542068481444\n",
      "                , weight: 0.3440999999999923\n",
      "=================================\n",
      "in 660 epoch, average loss: -3601.96953125\n",
      "                , loss1: -10629.63203125\n",
      "                , loss2: 3.070766067504883\n",
      "                , weight: 0.3350999999999922\n",
      "=================================\n",
      "in 670 epoch, average loss: -3506.54453125\n",
      "                , loss1: -10630.25859375\n",
      "                , loss2: 3.036033821105957\n",
      "                , weight: 0.32609999999999206\n",
      "=================================\n",
      "in 680 epoch, average loss: -3410.8765625\n",
      "                , loss1: -10630.2828125\n",
      "                , loss2: 3.0400171279907227\n",
      "                , weight: 0.31709999999999194\n",
      "=================================\n",
      "in 690 epoch, average loss: -3315.650390625\n",
      "                , loss1: -10631.6625\n",
      "                , loss2: 3.0222410202026366\n",
      "                , weight: 0.3080999999999918\n",
      "=================================\n",
      "in 700 epoch, average loss: -3220.362109375\n",
      "                , loss1: -10633.04296875\n",
      "                , loss2: 3.0432771682739257\n",
      "                , weight: 0.2990999999999917\n",
      "=================================\n",
      "in 710 epoch, average loss: -3125.0703125\n",
      "                , loss1: -10634.184375\n",
      "                , loss2: 2.975138854980469\n",
      "                , weight: 0.2900999999999916\n",
      "=================================\n",
      "in 720 epoch, average loss: -3029.5953125\n",
      "                , loss1: -10634.915625\n",
      "                , loss2: 2.9497209548950196\n",
      "                , weight: 0.28109999999999147\n",
      "=================================\n",
      "in 730 epoch, average loss: -2934.113671875\n",
      "                , loss1: -10635.796875\n",
      "                , loss2: 2.9640989303588867\n",
      "                , weight: 0.27209999999999135\n",
      "=================================\n",
      "in 740 epoch, average loss: -2838.713671875\n",
      "                , loss1: -10636.915625\n",
      "                , loss2: 2.9380266189575197\n",
      "                , weight: 0.26309999999999123\n",
      "=================================\n",
      "in 750 epoch, average loss: -2743.26328125\n",
      "                , loss1: -10637.953125\n",
      "                , loss2: 2.924136734008789\n",
      "                , weight: 0.2540999999999911\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.0009\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[32], line 30\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | 设置为训练模式\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     24\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/Hyper-Graph-Partition/examples/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1466\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_neg_1, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(20000):\n",
    "    if hgnn_trainer.weight > 0.01:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.0009\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([120., 122.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008264462809917356"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9000)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(torch.tensor([53.,56,53,54,56,55]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
