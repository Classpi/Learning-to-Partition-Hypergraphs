{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 4\n",
    "\n",
    "limit = 20\n",
    "weight = 900\n",
    "sub = 0.044\n",
    "\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 512, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "# h_hyper_prmts[\"convlayers122\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "# h_hyper_prmts[\"convlayers123\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer0\"] = {\"in_channels\":2048, \"out_channels\":2048, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer01\"] = {\"in_channels\":2048, \"out_channels\":1024, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":1024, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12334\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":512, \"out_channels\":256, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":128, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer121\"] = {\"in_channels\":128, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":4, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "\n",
    "# h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 7403, \"out_channels\": 4096, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# # h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 4096, \"out_channels\": 4096, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 4096, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# # h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 2048, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers15\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers16\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "# h_hyper_prmts[\"convlayers17\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":512, \"out_channels\":512, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# # l_hyper_prmts[\"linerlayer12\"] = {\"in_channels\":7403, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":512, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# # l_hyper_prmts[\"linerlayer32\"] = {\"in_channels\":512, \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "# l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":3, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device,weight):\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "    nn = torch.matmul(outs, (1 - torch.transpose(outs, 0, 1)))\n",
    "    ne_k = torch.matmul(nn, H)\n",
    "    ne_k = ne_k.mul(H)\n",
    "\n",
    "    H_degree = torch.sum(H, dim=0)\n",
    "    H_degree = H_degree\n",
    "\n",
    "    H_1 = ne_k / H_degree\n",
    "    a2 = 1 - H_1\n",
    "    a3 = torch.prod(a2, dim=0)\n",
    "    a3 = a3.sum()\n",
    "    loss_1 = -1 * a3\n",
    "\n",
    "    # pun = torch.mul(ne_k, H)\n",
    "\n",
    "    # loss_1 = pun.sum()\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  \n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7403, 2755)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dhg \n",
    "from dhg.data import Cooking200\n",
    "data = Cooking200()\n",
    "e_list = data[\"edge_list\"]\n",
    "num_v = data[\"num_vertices\"]\n",
    "G = dhg.Hypergraph(data[\"num_vertices\"],data[\"edge_list\"])\n",
    "num_v, data[\"num_edges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (18): ReLU()\n",
       "  (19): Dropout(p=0.05, inplace=False)\n",
       "  (20): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): ReLU()\n",
       "  (23): Dropout(p=0.05, inplace=False)\n",
       "  (24): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (25): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): ReLU()\n",
       "  (27): Dropout(p=0.05, inplace=False)\n",
       "  (28): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (29): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (30): ReLU()\n",
       "  (31): Dropout(p=0.05, inplace=False)\n",
       "  (32): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (33): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: -5642.474609375\n",
      "                , loss1: -135.4869873046875\n",
      "                , loss2: 0.2869319200515747\n",
      "                , weight: 41.64800000022653\n",
      "=================================\n",
      "in 10 epoch, average loss: -55939.79375\n",
      "                , loss1: -1353.80556640625\n",
      "                , loss2: 115.999072265625\n",
      "                , weight: 41.20800000022656\n",
      "=================================\n",
      "in 20 epoch, average loss: -55302.34375\n",
      "                , loss1: -1350.9064453125\n",
      "                , loss2: 38.951010131835936\n",
      "                , weight: 40.76800000022659\n",
      "=================================\n",
      "in 30 epoch, average loss: -54690.18125\n",
      "                , loss1: -1350.826171875\n",
      "                , loss2: 53.390618896484376\n",
      "                , weight: 40.32800000022662\n",
      "=================================\n",
      "in 40 epoch, average loss: -54130.36875\n",
      "                , loss1: -1351.68291015625\n",
      "                , loss2: 53.13873291015625\n",
      "                , weight: 39.888000000226654\n",
      "=================================\n",
      "in 50 epoch, average loss: -53576.225\n",
      "                , loss1: -1352.173046875\n",
      "                , loss2: 32.017642211914065\n",
      "                , weight: 39.448000000226685\n",
      "=================================\n",
      "in 60 epoch, average loss: -53008.75625\n",
      "                , loss1: -1352.7984375\n",
      "                , loss2: 28.974398803710937\n",
      "                , weight: 39.008000000226716\n",
      "=================================\n",
      "in 70 epoch, average loss: -52414.125\n",
      "                , loss1: -1353.7369140625\n",
      "                , loss2: 64.82365112304687\n",
      "                , weight: 38.568000000226746\n",
      "=================================\n",
      "in 80 epoch, average loss: -51864.3625\n",
      "                , loss1: -1353.75625\n",
      "                , loss2: 19.69879913330078\n",
      "                , weight: 38.12800000022678\n",
      "=================================\n",
      "in 90 epoch, average loss: -51259.496875\n",
      "                , loss1: -1353.6072265625\n",
      "                , loss2: 23.270945739746093\n",
      "                , weight: 37.68800000022681\n",
      "=================================\n",
      "in 100 epoch, average loss: -50692.56875\n",
      "                , loss1: -1354.31181640625\n",
      "                , loss2: 20.950311279296876\n",
      "                , weight: 37.24800000022684\n",
      "=================================\n",
      "in 110 epoch, average loss: -50170.675\n",
      "                , loss1: -1356.555078125\n",
      "                , loss2: 29.950534057617187\n",
      "                , weight: 36.80800000022687\n",
      "=================================\n",
      "in 120 epoch, average loss: -49653.975\n",
      "                , loss1: -1358.62685546875\n",
      "                , loss2: 25.49273223876953\n",
      "                , weight: 36.3680000002269\n",
      "=================================\n",
      "in 130 epoch, average loss: -49071.0875\n",
      "                , loss1: -1359.00322265625\n",
      "                , loss2: 24.25525665283203\n",
      "                , weight: 35.92800000022693\n",
      "=================================\n",
      "in 140 epoch, average loss: -48486.046875\n",
      "                , loss1: -1359.05322265625\n",
      "                , loss2: 13.127890014648438\n",
      "                , weight: 35.48800000022696\n",
      "=================================\n",
      "in 150 epoch, average loss: -47882.6875\n",
      "                , loss1: -1359.0044921875\n",
      "                , loss2: 16.78007354736328\n",
      "                , weight: 35.04800000022699\n",
      "=================================\n",
      "in 160 epoch, average loss: -47296.09375\n",
      "                , loss1: -1359.1091796875\n",
      "                , loss2: 9.063771057128907\n",
      "                , weight: 34.60800000022702\n",
      "=================================\n",
      "in 170 epoch, average loss: -46698.21875\n",
      "                , loss1: -1359.12890625\n",
      "                , loss2: 9.610450744628906\n",
      "                , weight: 34.16800000022705\n",
      "=================================\n",
      "in 180 epoch, average loss: -46100.396875\n",
      "                , loss1: -1359.1123046875\n",
      "                , loss2: 8.848661804199219\n",
      "                , weight: 33.728000000227084\n",
      "=================================\n",
      "in 190 epoch, average loss: -45501.390625\n",
      "                , loss1: -1359.0765625\n",
      "                , loss2: 8.651808166503907\n",
      "                , weight: 33.288000000227115\n",
      "=================================\n",
      "in 200 epoch, average loss: -44901.6\n",
      "                , loss1: -1359.11611328125\n",
      "                , loss2: 11.746070861816406\n",
      "                , weight: 32.848000000227145\n",
      "=================================\n",
      "in 210 epoch, average loss: -44311.965625\n",
      "                , loss1: -1359.1193359375\n",
      "                , loss2: 3.482269287109375\n",
      "                , weight: 32.408000000227176\n",
      "=================================\n",
      "in 220 epoch, average loss: -43715.728125\n",
      "                , loss1: -1359.1349609375\n",
      "                , loss2: 2.207118034362793\n",
      "                , weight: 31.968000000227203\n",
      "=================================\n",
      "in 230 epoch, average loss: -43117.75625\n",
      "                , loss1: -1359.1283203125\n",
      "                , loss2: 1.9457508087158204\n",
      "                , weight: 31.5280000002272\n",
      "=================================\n",
      "in 240 epoch, average loss: -42520.328125\n",
      "                , loss1: -1359.13154296875\n",
      "                , loss2: 1.4574942588806152\n",
      "                , weight: 31.088000000227193\n",
      "=================================\n",
      "in 250 epoch, average loss: -41922.028125\n",
      "                , loss1: -1359.135546875\n",
      "                , loss2: 1.8664636611938477\n",
      "                , weight: 30.64800000022719\n",
      "=================================\n",
      "in 260 epoch, average loss: -41324.73125\n",
      "                , loss1: -1359.1359375\n",
      "                , loss2: 1.1542325973510743\n",
      "                , weight: 30.208000000227184\n",
      "=================================\n",
      "in 270 epoch, average loss: -40738.678125\n",
      "                , loss1: -1359.56005859375\n",
      "                , loss2: 1.8476394653320312\n",
      "                , weight: 29.76800000022718\n",
      "=================================\n",
      "in 280 epoch, average loss: -40154.675\n",
      "                , loss1: -1360.0466796875\n",
      "                , loss2: 2.0667442321777343\n",
      "                , weight: 29.328000000227174\n",
      "=================================\n",
      "in 290 epoch, average loss: -39552.715625\n",
      "                , loss1: -1359.961328125\n",
      "                , loss2: 3.1033252716064452\n",
      "                , weight: 28.88800000022717\n",
      "=================================\n",
      "in 300 epoch, average loss: -38951.29375\n",
      "                , loss1: -1360.021875\n",
      "                , loss2: 7.886869812011719\n",
      "                , weight: 28.448000000227164\n",
      "=================================\n",
      "in 310 epoch, average loss: -38345.709375\n",
      "                , loss1: -1360.0474609375\n",
      "                , loss2: 15.792741394042968\n",
      "                , weight: 28.00800000022716\n",
      "=================================\n",
      "in 320 epoch, average loss: -37756.990625\n",
      "                , loss1: -1360.0498046875\n",
      "                , loss2: 6.155020141601563\n",
      "                , weight: 27.568000000227155\n",
      "=================================\n",
      "in 330 epoch, average loss: -37161.53125\n",
      "                , loss1: -1360.04990234375\n",
      "                , loss2: 3.192388916015625\n",
      "                , weight: 27.12800000022715\n",
      "=================================\n",
      "in 340 epoch, average loss: -36562.6875\n",
      "                , loss1: -1360.01796875\n",
      "                , loss2: 2.756109046936035\n",
      "                , weight: 26.688000000227145\n",
      "=================================\n",
      "in 350 epoch, average loss: -35966.103125\n",
      "                , loss1: -1360.052734375\n",
      "                , loss2: 1.8518699645996093\n",
      "                , weight: 26.24800000022714\n",
      "=================================\n",
      "in 360 epoch, average loss: -35365.315625\n",
      "                , loss1: -1360.046875\n",
      "                , loss2: 4.066373825073242\n",
      "                , weight: 25.808000000227135\n",
      "=================================\n",
      "in 370 epoch, average loss: -34768.559375\n",
      "                , loss1: -1360.04375\n",
      "                , loss2: 2.3203557968139648\n",
      "                , weight: 25.36800000022713\n",
      "=================================\n",
      "in 380 epoch, average loss: -34169.221875\n",
      "                , loss1: -1360.04765625\n",
      "                , loss2: 3.3361366271972654\n",
      "                , weight: 24.928000000227126\n",
      "=================================\n",
      "in 390 epoch, average loss: -33569.09375\n",
      "                , loss1: -1360.0416015625\n",
      "                , loss2: 4.893566131591797\n",
      "                , weight: 24.48800000022712\n",
      "=================================\n",
      "in 400 epoch, average loss: -32973.8375\n",
      "                , loss1: -1360.04853515625\n",
      "                , loss2: 1.9016656875610352\n",
      "                , weight: 24.048000000227116\n",
      "=================================\n",
      "in 410 epoch, average loss: -32374.9\n",
      "                , loss1: -1360.04345703125\n",
      "                , loss2: 2.2956043243408204\n",
      "                , weight: 23.60800000022711\n",
      "=================================\n",
      "in 420 epoch, average loss: -31776.13125\n",
      "                , loss1: -1360.0427734375\n",
      "                , loss2: 2.623399353027344\n",
      "                , weight: 23.168000000227106\n",
      "=================================\n",
      "in 430 epoch, average loss: -31179.1625\n",
      "                , loss1: -1360.05126953125\n",
      "                , loss2: 1.3777482986450196\n",
      "                , weight: 22.7280000002271\n",
      "=================================\n",
      "in 440 epoch, average loss: -30578.44375\n",
      "                , loss1: -1360.0490234375\n",
      "                , loss2: 3.6180118560791015\n",
      "                , weight: 22.288000000227097\n",
      "=================================\n",
      "in 450 epoch, average loss: -29982.440625\n",
      "                , loss1: -1360.04892578125\n",
      "                , loss2: 1.1984254837036132\n",
      "                , weight: 21.848000000227092\n",
      "=================================\n",
      "in 460 epoch, average loss: -29384.103125\n",
      "                , loss1: -1360.04970703125\n",
      "                , loss2: 1.1302217483520507\n",
      "                , weight: 21.408000000227087\n",
      "=================================\n",
      "in 470 epoch, average loss: -28786.090625\n",
      "                , loss1: -1360.04873046875\n",
      "                , loss2: 0.7039882659912109\n",
      "                , weight: 20.968000000227082\n",
      "=================================\n",
      "in 480 epoch, average loss: -28187.84375\n",
      "                , loss1: -1360.04970703125\n",
      "                , loss2: 0.5474457740783691\n",
      "                , weight: 20.528000000227077\n",
      "=================================\n",
      "in 490 epoch, average loss: -27587.190625\n",
      "                , loss1: -1359.96025390625\n",
      "                , loss2: 0.9529861450195313\n",
      "                , weight: 20.088000000227073\n",
      "=================================\n",
      "in 500 epoch, average loss: -27158.50625\n",
      "                , loss1: -1360.05419921875\n",
      "                , loss2: 0.6877796173095703\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 510 epoch, average loss: -27140.875\n",
      "                , loss1: -1360.05\n",
      "                , loss2: 0.2813223123550415\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 520 epoch, average loss: -27140.728125\n",
      "                , loss1: -1360.04931640625\n",
      "                , loss2: 0.4129011631011963\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 530 epoch, average loss: -27140.725\n",
      "                , loss1: -1360.05\n",
      "                , loss2: 0.43032541275024416\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 540 epoch, average loss: -27140.821875\n",
      "                , loss1: -1360.04951171875\n",
      "                , loss2: 0.3253838300704956\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 550 epoch, average loss: -27140.74375\n",
      "                , loss1: -1360.0498046875\n",
      "                , loss2: 0.40852508544921873\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 560 epoch, average loss: -27133.5625\n",
      "                , loss1: -1359.94140625\n",
      "                , loss2: 5.429111099243164\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 570 epoch, average loss: -27133.2875\n",
      "                , loss1: -1359.87451171875\n",
      "                , loss2: 4.364097213745117\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 580 epoch, average loss: -27138.315625\n",
      "                , loss1: -1359.962890625\n",
      "                , loss2: 1.1015320777893067\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 590 epoch, average loss: -27138.0625\n",
      "                , loss1: -1359.9703125\n",
      "                , loss2: 1.5051947593688966\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 600 epoch, average loss: -27138.096875\n",
      "                , loss1: -1359.96162109375\n",
      "                , loss2: 1.2948564529418944\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 610 epoch, average loss: -27140.390625\n",
      "                , loss1: -1360.0509765625\n",
      "                , loss2: 0.7840394973754883\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 620 epoch, average loss: -27137.446875\n",
      "                , loss1: -1359.88271484375\n",
      "                , loss2: 0.3722066879272461\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 630 epoch, average loss: -27138.74375\n",
      "                , loss1: -1359.955078125\n",
      "                , loss2: 0.5180164337158203\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 640 epoch, average loss: -27138.878125\n",
      "                , loss1: -1359.96123046875\n",
      "                , loss2: 0.5071788311004639\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 650 epoch, average loss: -27138.725\n",
      "                , loss1: -1359.96083984375\n",
      "                , loss2: 0.6493974685668945\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 660 epoch, average loss: -27136.684375\n",
      "                , loss1: -1359.86298828125\n",
      "                , loss2: 0.741248369216919\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 670 epoch, average loss: -27138.8\n",
      "                , loss1: -1359.96123046875\n",
      "                , loss2: 0.5882838726043701\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 680 epoch, average loss: -27138.48125\n",
      "                , loss1: -1359.9607421875\n",
      "                , loss2: 0.8955164909362793\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 690 epoch, average loss: -27138.815625\n",
      "                , loss1: -1359.96162109375\n",
      "                , loss2: 0.577738094329834\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 700 epoch, average loss: -27137.565625\n",
      "                , loss1: -1359.90390625\n",
      "                , loss2: 0.6751725196838378\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 710 epoch, average loss: -27132.284375\n",
      "                , loss1: -1359.9619140625\n",
      "                , loss2: 7.113697052001953\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 720 epoch, average loss: -27131.546875\n",
      "                , loss1: -1359.86845703125\n",
      "                , loss2: 5.99132080078125\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 730 epoch, average loss: -27138.11875\n",
      "                , loss1: -1359.9822265625\n",
      "                , loss2: 1.686432456970215\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 740 epoch, average loss: -27138.596875\n",
      "                , loss1: -1360.00009765625\n",
      "                , loss2: 1.565423583984375\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 750 epoch, average loss: -27137.78125\n",
      "                , loss1: -1360.099609375\n",
      "                , loss2: 4.367205810546875\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 760 epoch, average loss: -27118.553125\n",
      "                , loss1: -1360.79345703125\n",
      "                , loss2: 37.440399169921875\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 770 epoch, average loss: -27128.671875\n",
      "                , loss1: -1361.75439453125\n",
      "                , loss2: 46.498602294921874\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 780 epoch, average loss: -26941.996875\n",
      "                , loss1: -1355.99140625\n",
      "                , loss2: 118.16619873046875\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 790 epoch, average loss: -26798.9\n",
      "                , loss1: -1354.3015625\n",
      "                , loss2: 227.54365234375\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 800 epoch, average loss: -26746.41875\n",
      "                , loss1: -1351.52109375\n",
      "                , loss2: 224.5319580078125\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 810 epoch, average loss: -26876.003125\n",
      "                , loss1: -1351.8607421875\n",
      "                , loss2: 101.72936401367187\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 820 epoch, average loss: -26957.721875\n",
      "                , loss1: -1353.36923828125\n",
      "                , loss2: 50.11164855957031\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 830 epoch, average loss: -26998.01875\n",
      "                , loss1: -1355.1091796875\n",
      "                , loss2: 44.54445190429688\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 840 epoch, average loss: -27021.26875\n",
      "                , loss1: -1355.56884765625\n",
      "                , loss2: 30.458602905273438\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 850 epoch, average loss: -27034.5375\n",
      "                , loss1: -1356.54189453125\n",
      "                , loss2: 36.611993408203126\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 860 epoch, average loss: -26997.0\n",
      "                , loss1: -1355.9609375\n",
      "                , loss2: 62.5523681640625\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 870 epoch, average loss: -27020.490625\n",
      "                , loss1: -1356.4552734375\n",
      "                , loss2: 48.926751708984376\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 880 epoch, average loss: -27015.453125\n",
      "                , loss1: -1357.01337890625\n",
      "                , loss2: 65.10450439453125\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 890 epoch, average loss: -27037.25\n",
      "                , loss1: -1356.39091796875\n",
      "                , loss2: 30.8814697265625\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 900 epoch, average loss: -27034.790625\n",
      "                , loss1: -1356.405078125\n",
      "                , loss2: 33.62865295410156\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 910 epoch, average loss: -27054.41875\n",
      "                , loss1: -1357.189453125\n",
      "                , loss2: 29.656439208984374\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 920 epoch, average loss: -27079.496875\n",
      "                , loss1: -1357.29375\n",
      "                , loss2: 6.657063293457031\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 930 epoch, average loss: -27074.940625\n",
      "                , loss1: -1357.5005859375\n",
      "                , loss2: 15.34371337890625\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 940 epoch, average loss: -27087.59375\n",
      "                , loss1: -1357.641796875\n",
      "                , loss2: 5.507233810424805\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 950 epoch, average loss: -27087.65\n",
      "                , loss1: -1357.68681640625\n",
      "                , loss2: 6.349092102050781\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 960 epoch, average loss: -27092.7875\n",
      "                , loss1: -1357.8287109375\n",
      "                , loss2: 4.039706420898438\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 970 epoch, average loss: -27092.2625\n",
      "                , loss1: -1358.22626953125\n",
      "                , loss2: 12.499622344970703\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 980 epoch, average loss: -27103.15\n",
      "                , loss1: -1358.4658203125\n",
      "                , loss2: 6.393694305419922\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 990 epoch, average loss: -27098.471875\n",
      "                , loss1: -1358.37890625\n",
      "                , loss2: 9.333054351806641\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1000 epoch, average loss: -27106.69375\n",
      "                , loss1: -1358.47890625\n",
      "                , loss2: 3.1102657318115234\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1010 epoch, average loss: -27107.865625\n",
      "                , loss1: -1358.5703125\n",
      "                , loss2: 3.7619197845458983\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1020 epoch, average loss: -27120.359375\n",
      "                , loss1: -1359.1771484375\n",
      "                , loss2: 3.3795948028564453\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1030 epoch, average loss: -27122.371875\n",
      "                , loss1: -1359.3353515625\n",
      "                , loss2: 4.522456359863281\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1040 epoch, average loss: -27146.353125\n",
      "                , loss1: -1360.53955078125\n",
      "                , loss2: 4.573913955688477\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1050 epoch, average loss: -27144.459375\n",
      "                , loss1: -1360.7443359375\n",
      "                , loss2: 10.555352783203125\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1060 epoch, average loss: -27145.203125\n",
      "                , loss1: -1360.8005859375\n",
      "                , loss2: 10.935289001464843\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1070 epoch, average loss: -27150.3125\n",
      "                , loss1: -1360.83125\n",
      "                , loss2: 6.432671356201172\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1080 epoch, average loss: -27149.453125\n",
      "                , loss1: -1360.791796875\n",
      "                , loss2: 6.508123779296875\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1090 epoch, average loss: -27150.340625\n",
      "                , loss1: -1360.7724609375\n",
      "                , loss2: 5.229570388793945\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1100 epoch, average loss: -27148.715625\n",
      "                , loss1: -1360.62880859375\n",
      "                , loss2: 3.9891006469726564\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1110 epoch, average loss: -27153.55625\n",
      "                , loss1: -1360.8064453125\n",
      "                , loss2: 2.694194793701172\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1120 epoch, average loss: -27154.671875\n",
      "                , loss1: -1360.80556640625\n",
      "                , loss2: 1.5623696327209473\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1130 epoch, average loss: -27154.56875\n",
      "                , loss1: -1360.80380859375\n",
      "                , loss2: 1.631855583190918\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1140 epoch, average loss: -27153.7\n",
      "                , loss1: -1360.80146484375\n",
      "                , loss2: 2.4523910522460937\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1150 epoch, average loss: -27154.003125\n",
      "                , loss1: -1360.79638671875\n",
      "                , loss2: 2.0492856979370115\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1160 epoch, average loss: -27164.28125\n",
      "                , loss1: -1361.298046875\n",
      "                , loss2: 1.7841442108154297\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1170 epoch, average loss: -27170.984375\n",
      "                , loss1: -1361.63994140625\n",
      "                , loss2: 1.9032915115356446\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1180 epoch, average loss: -27171.709375\n",
      "                , loss1: -1361.642578125\n",
      "                , loss2: 1.232103157043457\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1190 epoch, average loss: -27171.88125\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 1.1301905632019043\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1200 epoch, average loss: -27171.975\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 1.0328150749206544\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1210 epoch, average loss: -27171.53125\n",
      "                , loss1: -1361.645703125\n",
      "                , loss2: 1.4699060440063476\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1220 epoch, average loss: -27172.20625\n",
      "                , loss1: -1361.6455078125\n",
      "                , loss2: 0.7884210586547852\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1230 epoch, average loss: -27172.615625\n",
      "                , loss1: -1361.64287109375\n",
      "                , loss2: 0.3234731674194336\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1240 epoch, average loss: -27172.65\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.36338038444519044\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1250 epoch, average loss: -27172.834375\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.17560176849365233\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1260 epoch, average loss: -27172.4875\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.5207982063293457\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1270 epoch, average loss: -27172.790625\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 0.2170398473739624\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1280 epoch, average loss: -27172.703125\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.30871195793151857\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1290 epoch, average loss: -27172.540625\n",
      "                , loss1: -1361.64599609375\n",
      "                , loss2: 0.46286873817443847\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1300 epoch, average loss: -27171.575\n",
      "                , loss1: -1361.5935546875\n",
      "                , loss2: 0.38400251865386964\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1310 epoch, average loss: -27171.33125\n",
      "                , loss1: -1361.64638671875\n",
      "                , loss2: 1.684140396118164\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1320 epoch, average loss: -27171.740625\n",
      "                , loss1: -1361.6458984375\n",
      "                , loss2: 1.2623268127441407\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1330 epoch, average loss: -27172.003125\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 1.0060285568237304\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1340 epoch, average loss: -27172.265625\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.7470378398895263\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1350 epoch, average loss: -27172.75625\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.2510129690170288\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1360 epoch, average loss: -27172.440625\n",
      "                , loss1: -1361.64560546875\n",
      "                , loss2: 0.5580325126647949\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1370 epoch, average loss: -27172.66875\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.3413506746292114\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1380 epoch, average loss: -27172.446875\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 0.5628461360931396\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1390 epoch, average loss: -27172.44375\n",
      "                , loss1: -1361.6458984375\n",
      "                , loss2: 0.560100793838501\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1400 epoch, average loss: -27172.046875\n",
      "                , loss1: -1361.6458984375\n",
      "                , loss2: 0.9567700386047363\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1410 epoch, average loss: -27172.6125\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.3998396396636963\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1420 epoch, average loss: -27172.46875\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.5433517932891846\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1430 epoch, average loss: -27172.725\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.2830721378326416\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1440 epoch, average loss: -27172.7125\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 0.2890004634857178\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1450 epoch, average loss: -27172.63125\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.37603800296783446\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1460 epoch, average loss: -27172.43125\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 0.5753665447235108\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1470 epoch, average loss: -27172.6625\n",
      "                , loss1: -1361.64619140625\n",
      "                , loss2: 0.34586682319641116\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1480 epoch, average loss: -27172.571875\n",
      "                , loss1: -1361.64609375\n",
      "                , loss2: 0.43166818618774416\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n",
      "in 1490 epoch, average loss: -27172.665625\n",
      "                , loss1: -1361.6462890625\n",
      "                , loss2: 0.346425986289978\n",
      "                , weight: 19.95600000022707\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m sub\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[82], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/graph-partition-with-gcn/.env-HGP/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=4e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(30000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1851., 1850., 1851., 1851.], device='cuda:1', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "num_nodes:Tensor = outs_straight.sum(dim=0)\n",
    "num_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
