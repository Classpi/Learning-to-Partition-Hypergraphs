{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exs/.conda/envs/partition/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) \n",
    "np.random.seed(seed) \n",
    "random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "weight = 10\n",
    "\n",
    "\n",
    "partitions = 5\n",
    "\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 1330, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": True, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers2\"] = {\"in_channels\": 512, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers3\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": True, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers5\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers51\"] = {\"in_channels\": 128, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers52\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers53\"] = {\"in_channels\": 256, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "#h_hyper_prmts[\"convlayers54\"] = {\"in_channels\": 256, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers55\"] = {\"in_channels\": 64, \"out_channels\": 5, \"use_bn\": True, \"drop_rate\": 0.1}\n",
    "\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":32, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "# l_hyper_prmts[\"linerlayer4\"] = {\"in_channels\":32, \"out_channels\":5, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device, weight):\n",
    "\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "\n",
    "    X_ = outs.t().unsqueeze(-1)\n",
    "    H_ = H.unsqueeze(0)\n",
    "    xweight = H.sum(dim=0)\n",
    "    mid = X_.mul(H_)\n",
    "    sum = (mid * (1 / xweight)).sum()\n",
    "    sub = (mid + (1 - H)).prod(dim=1).sum()\n",
    "    loss_1 = sum - sub\n",
    "\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "\n",
    "    return loss, loss_1, loss_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  \n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1413, 1330)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/cora\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=1330, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (5): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (6): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (7): HGNNPConv(\n",
       "        (bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (theta): Linear(in_features=64, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 9.335382843017578\n",
      "                , loss1: 23.301171875\n",
      "                , loss2: 0.04287581741809845\n",
      "                , weight: 0.398799999999337\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 91.81322021484375\n",
      "                , loss1: 233.009765625\n",
      "                , loss2: 0.42677717208862304\n",
      "                , weight: 0.38679999999933723\n",
      "=================================\n",
      "in 20 epoch, average loss: 89.02931518554688\n",
      "                , loss1: 233.038037109375\n",
      "                , loss2: 0.4283750534057617\n",
      "                , weight: 0.37479999999933744\n",
      "=================================\n",
      "in 30 epoch, average loss: 86.182666015625\n",
      "                , loss1: 233.045263671875\n",
      "                , loss2: 0.375431227684021\n",
      "                , weight: 0.36279999999933765\n",
      "=================================\n",
      "in 40 epoch, average loss: 83.35079345703124\n",
      "                , loss1: 233.023388671875\n",
      "                , loss2: 0.347866153717041\n",
      "                , weight: 0.35079999999933786\n",
      "=================================\n",
      "in 50 epoch, average loss: 80.57400512695312\n",
      "                , loss1: 233.019384765625\n",
      "                , loss2: 0.3687335729598999\n",
      "                , weight: 0.3387999999993381\n",
      "=================================\n",
      "in 60 epoch, average loss: 77.76036376953125\n",
      "                , loss1: 233.013525390625\n",
      "                , loss2: 0.3532604217529297\n",
      "                , weight: 0.3267999999993383\n",
      "=================================\n",
      "in 70 epoch, average loss: 74.98084716796875\n",
      "                , loss1: 233.028564453125\n",
      "                , loss2: 0.36515853404998777\n",
      "                , weight: 0.3147999999993385\n",
      "=================================\n",
      "in 80 epoch, average loss: 72.215185546875\n",
      "                , loss1: 233.0076171875\n",
      "                , loss2: 0.4022349834442139\n",
      "                , weight: 0.3027999999993387\n",
      "=================================\n",
      "in 90 epoch, average loss: 70.16799926757812\n",
      "                , loss1: 233.005419921875\n",
      "                , loss2: 0.3689075469970703\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 100 epoch, average loss: 70.08078002929688\n",
      "                , loss1: 233.008251953125\n",
      "                , loss2: 0.36471405029296877\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 110 epoch, average loss: 70.07915649414062\n",
      "                , loss1: 233.0105712890625\n",
      "                , loss2: 0.36239964962005616\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 120 epoch, average loss: 70.077783203125\n",
      "                , loss1: 233.06025390625\n",
      "                , loss2: 0.34614930152893064\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 130 epoch, average loss: 70.06137084960938\n",
      "                , loss1: 233.14970703125\n",
      "                , loss2: 0.3029845952987671\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 140 epoch, average loss: 70.06679077148438\n",
      "                , loss1: 233.01005859375\n",
      "                , loss2: 0.350180459022522\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 150 epoch, average loss: 70.10486450195313\n",
      "                , loss1: 233.0021728515625\n",
      "                , loss2: 0.39060220718383787\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 160 epoch, average loss: 70.07527465820313\n",
      "                , loss1: 233.014892578125\n",
      "                , loss2: 0.35722172260284424\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 170 epoch, average loss: 70.06146240234375\n",
      "                , loss1: 233.0099853515625\n",
      "                , loss2: 0.3448739528656006\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 180 epoch, average loss: 70.07486572265626\n",
      "                , loss1: 233.010302734375\n",
      "                , loss2: 0.3581823825836182\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 190 epoch, average loss: 70.064501953125\n",
      "                , loss1: 233.009716796875\n",
      "                , loss2: 0.34799799919128416\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 200 epoch, average loss: 70.08641357421875\n",
      "                , loss1: 233.016943359375\n",
      "                , loss2: 0.3677358627319336\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 210 epoch, average loss: 70.05293579101563\n",
      "                , loss1: 233.008203125\n",
      "                , loss2: 0.3368802785873413\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 220 epoch, average loss: 70.07799682617187\n",
      "                , loss1: 233.0174072265625\n",
      "                , loss2: 0.3591862916946411\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 230 epoch, average loss: 70.07278442382812\n",
      "                , loss1: 233.0197509765625\n",
      "                , loss2: 0.35327322483062745\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 240 epoch, average loss: 70.09732666015626\n",
      "                , loss1: 233.016259765625\n",
      "                , loss2: 0.37884979248046874\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 250 epoch, average loss: 70.10146484375\n",
      "                , loss1: 233.0105712890625\n",
      "                , loss2: 0.3847014904022217\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 260 epoch, average loss: 70.06749877929687\n",
      "                , loss1: 233.026025390625\n",
      "                , loss2: 0.34610536098480227\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 270 epoch, average loss: 70.2738525390625\n",
      "                , loss1: 233.1498291015625\n",
      "                , loss2: 0.515428876876831\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 280 epoch, average loss: 70.099365234375\n",
      "                , loss1: 233.18564453125\n",
      "                , loss2: 0.33021466732025145\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 290 epoch, average loss: 70.08896484375\n",
      "                , loss1: 233.00849609375\n",
      "                , loss2: 0.37282092571258546\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 300 epoch, average loss: 70.08160400390625\n",
      "                , loss1: 233.0102294921875\n",
      "                , loss2: 0.3649484872817993\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 310 epoch, average loss: 70.06422119140625\n",
      "                , loss1: 233.01845703125\n",
      "                , loss2: 0.34510245323181155\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 320 epoch, average loss: 70.0698486328125\n",
      "                , loss1: 233.0499267578125\n",
      "                , loss2: 0.3413138151168823\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 330 epoch, average loss: 70.06025390625\n",
      "                , loss1: 233.01552734375\n",
      "                , loss2: 0.34200587272644045\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 340 epoch, average loss: 70.08848876953125\n",
      "                , loss1: 233.020849609375\n",
      "                , loss2: 0.36864557266235354\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 350 epoch, average loss: 70.06361694335938\n",
      "                , loss1: 233.011279296875\n",
      "                , loss2: 0.3466477632522583\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 360 epoch, average loss: 70.10122680664062\n",
      "                , loss1: 233.0091064453125\n",
      "                , loss2: 0.384899640083313\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 370 epoch, average loss: 70.11890258789063\n",
      "                , loss1: 233.0087646484375\n",
      "                , loss2: 0.4026798725128174\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 380 epoch, average loss: 70.12286987304688\n",
      "                , loss1: 233.0094970703125\n",
      "                , loss2: 0.4064267635345459\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 390 epoch, average loss: 70.111572265625\n",
      "                , loss1: 233.0080078125\n",
      "                , loss2: 0.3955854415893555\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 400 epoch, average loss: 70.07770385742188\n",
      "                , loss1: 233.03447265625\n",
      "                , loss2: 0.35379934310913086\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 410 epoch, average loss: 70.07999877929687\n",
      "                , loss1: 233.0536376953125\n",
      "                , loss2: 0.3503526210784912\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 420 epoch, average loss: 70.09075927734375\n",
      "                , loss1: 233.008203125\n",
      "                , loss2: 0.3747056722640991\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 430 epoch, average loss: 70.08189086914062\n",
      "                , loss1: 233.0072509765625\n",
      "                , loss2: 0.3661231756210327\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 440 epoch, average loss: 70.081787109375\n",
      "                , loss1: 233.01044921875\n",
      "                , loss2: 0.3650657653808594\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 450 epoch, average loss: 70.08529663085938\n",
      "                , loss1: 233.0114013671875\n",
      "                , loss2: 0.3682847261428833\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 460 epoch, average loss: 70.11132202148437\n",
      "                , loss1: 233.01748046875\n",
      "                , loss2: 0.39248759746551515\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 470 epoch, average loss: 70.080908203125\n",
      "                , loss1: 233.00908203125\n",
      "                , loss2: 0.3645843744277954\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 480 epoch, average loss: 70.0786865234375\n",
      "                , loss1: 233.034033203125\n",
      "                , loss2: 0.3549030780792236\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 490 epoch, average loss: 70.07202758789063\n",
      "                , loss1: 233.009375\n",
      "                , loss2: 0.35562906265258787\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 500 epoch, average loss: 70.09783935546875\n",
      "                , loss1: 233.0132080078125\n",
      "                , loss2: 0.3802831172943115\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 510 epoch, average loss: 70.06366577148438\n",
      "                , loss1: 233.0169677734375\n",
      "                , loss2: 0.3449913740158081\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 520 epoch, average loss: 70.0793212890625\n",
      "                , loss1: 233.008740234375\n",
      "                , loss2: 0.3631101369857788\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 530 epoch, average loss: 70.10015258789062\n",
      "                , loss1: 233.007958984375\n",
      "                , loss2: 0.3841735601425171\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 540 epoch, average loss: 70.0879150390625\n",
      "                , loss1: 233.0080078125\n",
      "                , loss2: 0.3719334602355957\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 550 epoch, average loss: 70.0823486328125\n",
      "                , loss1: 233.0091064453125\n",
      "                , loss2: 0.3660294532775879\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 560 epoch, average loss: 70.07530517578125\n",
      "                , loss1: 233.009228515625\n",
      "                , loss2: 0.3589505195617676\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 570 epoch, average loss: 70.078759765625\n",
      "                , loss1: 233.0091796875\n",
      "                , loss2: 0.3624154806137085\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 580 epoch, average loss: 70.07542724609375\n",
      "                , loss1: 233.00888671875\n",
      "                , loss2: 0.35917370319366454\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 590 epoch, average loss: 70.07750854492187\n",
      "                , loss1: 233.01787109375\n",
      "                , loss2: 0.3585689067840576\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 600 epoch, average loss: 70.10496215820312\n",
      "                , loss1: 233.019921875\n",
      "                , loss2: 0.3853968620300293\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 610 epoch, average loss: 70.0481689453125\n",
      "                , loss1: 233.2897705078125\n",
      "                , loss2: 0.24787051677703859\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 620 epoch, average loss: 70.03455200195313\n",
      "                , loss1: 233.4322998046875\n",
      "                , loss2: 0.1916080355644226\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 630 epoch, average loss: 70.02305297851562\n",
      "                , loss1: 233.7169189453125\n",
      "                , loss2: 0.09495829343795777\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 640 epoch, average loss: 70.07510986328126\n",
      "                , loss1: 233.4656494140625\n",
      "                , loss2: 0.22218642234802247\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 650 epoch, average loss: 70.0539794921875\n",
      "                , loss1: 233.85869140625\n",
      "                , loss2: 0.08346163034439087\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 660 epoch, average loss: 70.02675170898438\n",
      "                , loss1: 233.8781982421875\n",
      "                , loss2: 0.05039171576499939\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 670 epoch, average loss: 70.02064208984375\n",
      "                , loss1: 233.95439453125\n",
      "                , loss2: 0.021486859023571014\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 680 epoch, average loss: 69.99921875\n",
      "                , loss1: 233.810498046875\n",
      "                , loss2: 0.04312310516834259\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 690 epoch, average loss: 70.00771484375\n",
      "                , loss1: 233.9732421875\n",
      "                , loss2: 0.0029286120086908342\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 700 epoch, average loss: 70.00296630859376\n",
      "                , loss1: 233.702685546875\n",
      "                , loss2: 0.07911295294761658\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 710 epoch, average loss: 70.0016845703125\n",
      "                , loss1: 233.6693359375\n",
      "                , loss2: 0.0878239095211029\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 720 epoch, average loss: 70.02686767578125\n",
      "                , loss1: 234.0198974609375\n",
      "                , loss2: 0.008117570728063583\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 730 epoch, average loss: 69.99197387695312\n",
      "                , loss1: 233.7494140625\n",
      "                , loss2: 0.05415334701538086\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 740 epoch, average loss: 69.97761840820313\n",
      "                , loss1: 233.7304443359375\n",
      "                , loss2: 0.04546526372432709\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 750 epoch, average loss: 70.01103515625\n",
      "                , loss1: 233.798828125\n",
      "                , loss2: 0.05843202471733093\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 760 epoch, average loss: 70.031591796875\n",
      "                , loss1: 233.9796875\n",
      "                , loss2: 0.024866405129432678\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 770 epoch, average loss: 70.05003662109375\n",
      "                , loss1: 233.667626953125\n",
      "                , loss2: 0.13668190240859984\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 780 epoch, average loss: 70.10042724609374\n",
      "                , loss1: 233.842822265625\n",
      "                , loss2: 0.13465549945831298\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 790 epoch, average loss: 70.0090576171875\n",
      "                , loss1: 233.922900390625\n",
      "                , loss2: 0.01934065520763397\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 800 epoch, average loss: 70.02161865234375\n",
      "                , loss1: 234.0197509765625\n",
      "                , loss2: 0.0029058044776320456\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 810 epoch, average loss: 69.99556884765624\n",
      "                , loss1: 233.787548828125\n",
      "                , loss2: 0.046329337358474734\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 820 epoch, average loss: 69.98439331054688\n",
      "                , loss1: 233.8699462890625\n",
      "                , loss2: 0.010509888082742691\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 830 epoch, average loss: 70.02979736328125\n",
      "                , loss1: 233.714013671875\n",
      "                , loss2: 0.1025656819343567\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 840 epoch, average loss: 69.98682861328125\n",
      "                , loss1: 233.85185546875\n",
      "                , loss2: 0.01835700571537018\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 850 epoch, average loss: 69.99413452148437\n",
      "                , loss1: 233.7569580078125\n",
      "                , loss2: 0.054052734375\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 860 epoch, average loss: 69.9753173828125\n",
      "                , loss1: 233.693017578125\n",
      "                , loss2: 0.05436440706253052\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 870 epoch, average loss: 70.01383056640626\n",
      "                , loss1: 233.639501953125\n",
      "                , loss2: 0.10890015363693237\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 880 epoch, average loss: 69.98773193359375\n",
      "                , loss1: 233.7525634765625\n",
      "                , loss2: 0.04896307587623596\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 890 epoch, average loss: 69.9987060546875\n",
      "                , loss1: 233.727197265625\n",
      "                , loss2: 0.06752508282661437\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 900 epoch, average loss: 69.9912353515625\n",
      "                , loss1: 233.707470703125\n",
      "                , loss2: 0.06595982909202576\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 910 epoch, average loss: 69.97695922851562\n",
      "                , loss1: 233.79248046875\n",
      "                , loss2: 0.02624874413013458\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 920 epoch, average loss: 69.98442993164062\n",
      "                , loss1: 233.6608642578125\n",
      "                , loss2: 0.07309504747390747\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 930 epoch, average loss: 69.99364624023437\n",
      "                , loss1: 233.8703857421875\n",
      "                , loss2: 0.019634026288986205\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 940 epoch, average loss: 69.98750610351563\n",
      "                , loss1: 233.752978515625\n",
      "                , loss2: 0.04861759841442108\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 950 epoch, average loss: 69.98517456054688\n",
      "                , loss1: 233.653173828125\n",
      "                , loss2: 0.0761483907699585\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 960 epoch, average loss: 70.0268798828125\n",
      "                , loss1: 233.8949951171875\n",
      "                , loss2: 0.04549243450164795\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 970 epoch, average loss: 69.99261474609375\n",
      "                , loss1: 233.798388671875\n",
      "                , loss2: 0.04014317393302917\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 980 epoch, average loss: 69.98199462890625\n",
      "                , loss1: 233.792529296875\n",
      "                , loss2: 0.031265759468078615\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 990 epoch, average loss: 70.00556640625\n",
      "                , loss1: 233.791455078125\n",
      "                , loss2: 0.05516711473464966\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1000 epoch, average loss: 69.99758911132812\n",
      "                , loss1: 233.9364990234375\n",
      "                , loss2: 0.0037866324186325074\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1010 epoch, average loss: 69.98281860351562\n",
      "                , loss1: 233.6677490234375\n",
      "                , loss2: 0.06942391395568848\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1020 epoch, average loss: 69.98766479492187\n",
      "                , loss1: 233.8347900390625\n",
      "                , loss2: 0.024290476739406586\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1030 epoch, average loss: 69.98675537109375\n",
      "                , loss1: 233.6373046875\n",
      "                , loss2: 0.08247395753860473\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1040 epoch, average loss: 69.97767333984375\n",
      "                , loss1: 233.741748046875\n",
      "                , loss2: 0.04213890135288238\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1050 epoch, average loss: 69.98985595703125\n",
      "                , loss1: 233.7150390625\n",
      "                , loss2: 0.062302511930465695\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1060 epoch, average loss: 69.98148803710937\n",
      "                , loss1: 233.6775146484375\n",
      "                , loss2: 0.06517055630683899\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1070 epoch, average loss: 69.98098754882812\n",
      "                , loss1: 233.73759765625\n",
      "                , loss2: 0.0467057079076767\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n",
      "in 1080 epoch, average loss: 69.99010009765625\n",
      "                , loss1: 233.7995849609375\n",
      "                , loss2: 0.03727106750011444\n",
      "                , weight: 0.2991999999993388\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.3\u001b[39m:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.0012\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m      9\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | 设置为训练模式\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     24\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Hyper-Graph-Partition/examples_standardForm/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1466\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_neg_1, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=2.5e-4, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(30000):\n",
    "    if hgnn_trainer.weight > 0.3:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - 0.0012\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([266., 266., 266., 266., 266.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9000)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var(torch.tensor([53.,56,53,54,56,55]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
