{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 4\n",
    "\n",
    "weight = 1.03\n",
    "lr = 2.4e-3\n",
    "sub = 0.0005\n",
    "limit = 0.02\n",
    "\n",
    "h_hyper_prmts[\"convlayers11\"] = {\"in_channels\": 242, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.2}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\":256, \"out_channels\": 256, \"use_bn\":  False, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers141\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\":  True, \"drop_rate\": 0.05}\n",
    "# h_hyper_prmts[\"convlayers142\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0.05}\n",
    "h_hyper_prmts[\"convlayers143\"] = {\"in_channels\": 256, \"out_channels\": 256, \"use_bn\": True, \"drop_rate\": 0}\n",
    "\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer123\"] = {\"in_channels\":256, \"out_channels\":64, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer31\"] = {\"in_channels\":64, \"out_channels\":4, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device, weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的标准.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "\n",
    "    X_ = outs.t().unsqueeze(-1)\n",
    "    H_ = H.unsqueeze(0)\n",
    "    xweight = H.sum(dim=0)\n",
    "    mid = X_.mul(H_)\n",
    "    sum = (mid * (1 / xweight)).sum()\n",
    "    sub = (mid + (1 - H)).prod(dim=1).sum()\n",
    "    loss_1 = sum - sub\n",
    "\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "\n",
    "    return loss, loss_1, loss_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12704, 242)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/primary\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.2, inplace=False)\n",
       "        (theta): Linear(in_features=242, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0.05, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (9): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut():\n",
    "    hgnn_trainer.eval()\n",
    "    outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "    outs_straight = StraightThroughEstimator.apply(outs)\n",
    "    G_clone = G.clone()\n",
    "    edges, _  = G_clone.e\n",
    "    cut = 0\n",
    "    for vertices in edges:\n",
    "        if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "            cut += 1\n",
    "        else:\n",
    "            G_clone.remove_hyperedges(vertices)\n",
    "    assert cut == G_clone.num_e\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 20 epoch, average loss: 99.057763671875\n",
      "                , loss1: 5077.0234375\n",
      "                , loss2: 0.05580823421478272\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 40 epoch, average loss: 99.17092895507812\n",
      "                , loss1: 5077.27421875\n",
      "                , loss2: 0.16405707597732544\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 60 epoch, average loss: 99.0973388671875\n",
      "                , loss1: 5076.46796875\n",
      "                , loss2: 0.10620758533477784\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 80 epoch, average loss: 99.05733032226563\n",
      "                , loss1: 5076.77890625\n",
      "                , loss2: 0.060144835710525514\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 100 epoch, average loss: 99.05657958984375\n",
      "                , loss1: 5077.010546875\n",
      "                , loss2: 0.05486415028572082\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 120 epoch, average loss: 99.05621337890625\n",
      "                , loss1: 5077.115625\n",
      "                , loss2: 0.052458536624908444\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 140 epoch, average loss: 99.05607299804687\n",
      "                , loss1: 5077.040625\n",
      "                , loss2: 0.053796255588531496\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 160 epoch, average loss: 99.05595703125\n",
      "                , loss1: 5077.062109375\n",
      "                , loss2: 0.053224372863769534\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 180 epoch, average loss: 99.0564697265625\n",
      "                , loss1: 5077.10703125\n",
      "                , loss2: 0.05286964774131775\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 200 epoch, average loss: 99.05681762695312\n",
      "                , loss1: 5077.056640625\n",
      "                , loss2: 0.054209846258163455\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 220 epoch, average loss: 99.05677490234375\n",
      "                , loss1: 5077.1640625\n",
      "                , loss2: 0.052072614431381226\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 240 epoch, average loss: 99.055908203125\n",
      "                , loss1: 5077.084765625\n",
      "                , loss2: 0.05276365280151367\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 260 epoch, average loss: 99.055810546875\n",
      "                , loss1: 5077.123046875\n",
      "                , loss2: 0.051898181438446045\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 280 epoch, average loss: 99.05586547851563\n",
      "                , loss1: 5077.18125\n",
      "                , loss2: 0.050838547945022586\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 300 epoch, average loss: 99.0560302734375\n",
      "                , loss1: 5076.971484375\n",
      "                , loss2: 0.055091530084609985\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 320 epoch, average loss: 99.05621948242188\n",
      "                , loss1: 5077.10859375\n",
      "                , loss2: 0.052602946758270264\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 340 epoch, average loss: 99.0583251953125\n",
      "                , loss1: 5076.9796875\n",
      "                , loss2: 0.057224714756011964\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 360 epoch, average loss: 99.05723876953125\n",
      "                , loss1: 5077.1015625\n",
      "                , loss2: 0.05376977920532226\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 380 epoch, average loss: 99.05609130859375\n",
      "                , loss1: 5077.008984375\n",
      "                , loss2: 0.05440719127655029\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 400 epoch, average loss: 99.055908203125\n",
      "                , loss1: 5077.1484375\n",
      "                , loss2: 0.05151157379150391\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 420 epoch, average loss: 99.0559814453125\n",
      "                , loss1: 5077.048046875\n",
      "                , loss2: 0.05353807806968689\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 440 epoch, average loss: 99.05592651367188\n",
      "                , loss1: 5077.126953125\n",
      "                , loss2: 0.051952534914016725\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 460 epoch, average loss: 99.05548706054688\n",
      "                , loss1: 5077.1234375\n",
      "                , loss2: 0.0515899121761322\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 480 epoch, average loss: 99.0735595703125\n",
      "                , loss1: 5077.11171875\n",
      "                , loss2: 0.06986814141273498\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 500 epoch, average loss: 99.07939453125\n",
      "                , loss1: 5076.803125\n",
      "                , loss2: 0.08175327777862548\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 520 epoch, average loss: 99.05977172851563\n",
      "                , loss1: 5076.4765625\n",
      "                , loss2: 0.06846709251403808\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 540 epoch, average loss: 99.0565673828125\n",
      "                , loss1: 5077.05\n",
      "                , loss2: 0.05409134030342102\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 560 epoch, average loss: 99.05614624023437\n",
      "                , loss1: 5077.06640625\n",
      "                , loss2: 0.05335565805435181\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 580 epoch, average loss: 99.05604858398438\n",
      "                , loss1: 5077.1046875\n",
      "                , loss2: 0.052518045902252196\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 600 epoch, average loss: 99.05594482421876\n",
      "                , loss1: 5077.102734375\n",
      "                , loss2: 0.052436155080795285\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 620 epoch, average loss: 99.05611572265624\n",
      "                , loss1: 5077.14140625\n",
      "                , loss2: 0.051851123571395874\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 640 epoch, average loss: 99.05597534179688\n",
      "                , loss1: 5077.0984375\n",
      "                , loss2: 0.0525443434715271\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 660 epoch, average loss: 99.05599975585938\n",
      "                , loss1: 5077.1140625\n",
      "                , loss2: 0.0522727370262146\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 680 epoch, average loss: 99.05582885742187\n",
      "                , loss1: 5077.087890625\n",
      "                , loss2: 0.052599388360977176\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 700 epoch, average loss: 99.05592041015625\n",
      "                , loss1: 5077.058203125\n",
      "                , loss2: 0.053280651569366455\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 720 epoch, average loss: 99.0559814453125\n",
      "                , loss1: 5077.143359375\n",
      "                , loss2: 0.05168993473052978\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 740 epoch, average loss: 99.05580444335938\n",
      "                , loss1: 5077.133984375\n",
      "                , loss2: 0.051692080497741696\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 760 epoch, average loss: 99.05584716796875\n",
      "                , loss1: 5077.098046875\n",
      "                , loss2: 0.052449047565460205\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 780 epoch, average loss: 99.05595703125\n",
      "                , loss1: 5077.074609375\n",
      "                , loss2: 0.05299354791641235\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 800 epoch, average loss: 99.05580444335938\n",
      "                , loss1: 5077.10625\n",
      "                , loss2: 0.05222602486610413\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 820 epoch, average loss: 99.05569458007812\n",
      "                , loss1: 5077.126171875\n",
      "                , loss2: 0.05172836184501648\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 840 epoch, average loss: 99.05593872070312\n",
      "                , loss1: 5077.1921875\n",
      "                , loss2: 0.05068708062171936\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 860 epoch, average loss: 99.05575561523438\n",
      "                , loss1: 5077.09140625\n",
      "                , loss2: 0.05246749520301819\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 880 epoch, average loss: 99.05574951171874\n",
      "                , loss1: 5077.0578125\n",
      "                , loss2: 0.05313171744346619\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 900 epoch, average loss: 99.055712890625\n",
      "                , loss1: 5077.11328125\n",
      "                , loss2: 0.051994460821151736\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 920 epoch, average loss: 99.0557373046875\n",
      "                , loss1: 5077.1015625\n",
      "                , loss2: 0.05226203799247742\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 940 epoch, average loss: 99.05570068359376\n",
      "                , loss1: 5077.112890625\n",
      "                , loss2: 0.051985245943069455\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 960 epoch, average loss: 99.05570068359376\n",
      "                , loss1: 5077.172265625\n",
      "                , loss2: 0.05082942247390747\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 980 epoch, average loss: 99.05589599609375\n",
      "                , loss1: 5077.065234375\n",
      "                , loss2: 0.053141224384307864\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1000 epoch, average loss: 99.05620727539062\n",
      "                , loss1: 5077.08984375\n",
      "                , loss2: 0.05295224785804749\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1020 epoch, average loss: 99.05633544921875\n",
      "                , loss1: 5077.128515625\n",
      "                , loss2: 0.05231132507324219\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1040 epoch, average loss: 99.05635375976563\n",
      "                , loss1: 5077.009375\n",
      "                , loss2: 0.05465615391731262\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1060 epoch, average loss: 99.05610961914063\n",
      "                , loss1: 5077.14296875\n",
      "                , loss2: 0.05182027816772461\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1080 epoch, average loss: 99.05574951171874\n",
      "                , loss1: 5077.11796875\n",
      "                , loss2: 0.05194604992866516\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1100 epoch, average loss: 99.05577392578125\n",
      "                , loss1: 5077.090625\n",
      "                , loss2: 0.052508610486984256\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1120 epoch, average loss: 99.05582885742187\n",
      "                , loss1: 5077.14140625\n",
      "                , loss2: 0.05155721306800842\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1140 epoch, average loss: 99.05582275390626\n",
      "                , loss1: 5077.1046875\n",
      "                , loss2: 0.052271676063537595\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1160 epoch, average loss: 99.05575561523438\n",
      "                , loss1: 5077.1140625\n",
      "                , loss2: 0.05203545093536377\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1180 epoch, average loss: 99.05574951171874\n",
      "                , loss1: 5077.08828125\n",
      "                , loss2: 0.05252788662910461\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1200 epoch, average loss: 99.05584106445312\n",
      "                , loss1: 5077.1328125\n",
      "                , loss2: 0.05175924301147461\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1220 epoch, average loss: 99.05579833984375\n",
      "                , loss1: 5077.084375\n",
      "                , loss2: 0.05262346863746643\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1240 epoch, average loss: 99.05578002929687\n",
      "                , loss1: 5077.128125\n",
      "                , loss2: 0.051778435707092285\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1260 epoch, average loss: 99.05570678710937\n",
      "                , loss1: 5077.144140625\n",
      "                , loss2: 0.05140007734298706\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1280 epoch, average loss: 99.05580444335938\n",
      "                , loss1: 5077.078515625\n",
      "                , loss2: 0.052769559621810916\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1300 epoch, average loss: 99.05567626953125\n",
      "                , loss1: 5077.08984375\n",
      "                , loss2: 0.05242872834205627\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1320 epoch, average loss: 99.05577392578125\n",
      "                , loss1: 5077.133984375\n",
      "                , loss2: 0.05166805386543274\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1340 epoch, average loss: 99.05597534179688\n",
      "                , loss1: 5077.118359375\n",
      "                , loss2: 0.052159678936004636\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1360 epoch, average loss: 99.079150390625\n",
      "                , loss1: 5076.929296875\n",
      "                , loss2: 0.07902364730834961\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1380 epoch, average loss: 99.05940551757813\n",
      "                , loss1: 5076.630078125\n",
      "                , loss2: 0.06512280702590942\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1400 epoch, average loss: 99.0562255859375\n",
      "                , loss1: 5077.24921875\n",
      "                , loss2: 0.049857205152511595\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1420 epoch, average loss: 99.05580444335938\n",
      "                , loss1: 5077.0296875\n",
      "                , loss2: 0.053718411922454835\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1440 epoch, average loss: 99.05580444335938\n",
      "                , loss1: 5077.158203125\n",
      "                , loss2: 0.0512352705001831\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1460 epoch, average loss: 99.05579833984375\n",
      "                , loss1: 5077.099609375\n",
      "                , loss2: 0.05235384106636047\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1480 epoch, average loss: 99.05574340820313\n",
      "                , loss1: 5077.140625\n",
      "                , loss2: 0.05150209665298462\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1500 epoch, average loss: 99.0557373046875\n",
      "                , loss1: 5077.056640625\n",
      "                , loss2: 0.053132861852645874\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1520 epoch, average loss: 99.05572509765625\n",
      "                , loss1: 5077.140234375\n",
      "                , loss2: 0.05149206519126892\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1540 epoch, average loss: 99.05576171875\n",
      "                , loss1: 5077.090625\n",
      "                , loss2: 0.052479219436645505\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1560 epoch, average loss: 99.05567016601563\n",
      "                , loss1: 5077.090234375\n",
      "                , loss2: 0.0524019718170166\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1580 epoch, average loss: 99.05572509765625\n",
      "                , loss1: 5077.126171875\n",
      "                , loss2: 0.051765072345733645\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1600 epoch, average loss: 99.05575561523438\n",
      "                , loss1: 5077.071484375\n",
      "                , loss2: 0.0528644323348999\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1620 epoch, average loss: 99.05579833984375\n",
      "                , loss1: 5077.09609375\n",
      "                , loss2: 0.05240779519081116\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1640 epoch, average loss: 99.05565795898437\n",
      "                , loss1: 5077.1328125\n",
      "                , loss2: 0.051558852195739746\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1660 epoch, average loss: 99.0556396484375\n",
      "                , loss1: 5077.1015625\n",
      "                , loss2: 0.05214877724647522\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1680 epoch, average loss: 99.05578002929687\n",
      "                , loss1: 5077.1\n",
      "                , loss2: 0.05233317017555237\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1700 epoch, average loss: 99.05568237304688\n",
      "                , loss1: 5077.133203125\n",
      "                , loss2: 0.05157878398895264\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1720 epoch, average loss: 99.05570678710937\n",
      "                , loss1: 5077.0953125\n",
      "                , loss2: 0.052335327863693236\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1740 epoch, average loss: 99.05560302734375\n",
      "                , loss1: 5077.14921875\n",
      "                , loss2: 0.05120059847831726\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1760 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.084765625\n",
      "                , loss2: 0.05250387787818909\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1780 epoch, average loss: 99.055712890625\n",
      "                , loss1: 5077.115625\n",
      "                , loss2: 0.0519790768623352\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1800 epoch, average loss: 99.05584716796875\n",
      "                , loss1: 5077.12421875\n",
      "                , loss2: 0.051918411254882814\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1820 epoch, average loss: 99.05568237304688\n",
      "                , loss1: 5077.077734375\n",
      "                , loss2: 0.052658605575561526\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1840 epoch, average loss: 99.05582885742187\n",
      "                , loss1: 5077.12109375\n",
      "                , loss2: 0.051953500509262084\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1860 epoch, average loss: 99.05567016601563\n",
      "                , loss1: 5077.090625\n",
      "                , loss2: 0.052411395311355594\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1880 epoch, average loss: 99.0556884765625\n",
      "                , loss1: 5077.137890625\n",
      "                , loss2: 0.05149094462394714\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1900 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.110546875\n",
      "                , loss2: 0.05201020836830139\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1920 epoch, average loss: 99.05567016601563\n",
      "                , loss1: 5077.10078125\n",
      "                , loss2: 0.05220187902450561\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1940 epoch, average loss: 99.05563354492188\n",
      "                , loss1: 5077.116796875\n",
      "                , loss2: 0.05185021162033081\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1960 epoch, average loss: 99.05567626953125\n",
      "                , loss1: 5077.141015625\n",
      "                , loss2: 0.05143746733665466\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 1980 epoch, average loss: 99.05565795898437\n",
      "                , loss1: 5077.134765625\n",
      "                , loss2: 0.0515236496925354\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2000 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.0796875\n",
      "                , loss2: 0.052606767416000365\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2020 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.11953125\n",
      "                , loss2: 0.05183093547821045\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2040 epoch, average loss: 99.05562744140624\n",
      "                , loss1: 5077.071484375\n",
      "                , loss2: 0.05272391438484192\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2060 epoch, average loss: 99.0556396484375\n",
      "                , loss1: 5077.127734375\n",
      "                , loss2: 0.0516379714012146\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2080 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.1328125\n",
      "                , loss2: 0.051582664251327515\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2100 epoch, average loss: 99.05565795898437\n",
      "                , loss1: 5077.101953125\n",
      "                , loss2: 0.05215911269187927\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2120 epoch, average loss: 99.05559692382812\n",
      "                , loss1: 5077.128125\n",
      "                , loss2: 0.05157795548439026\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2140 epoch, average loss: 99.0556640625\n",
      "                , loss1: 5077.125\n",
      "                , loss2: 0.051730334758758545\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2160 epoch, average loss: 99.05563354492188\n",
      "                , loss1: 5077.086328125\n",
      "                , loss2: 0.052437156438827515\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2180 epoch, average loss: 99.055615234375\n",
      "                , loss1: 5077.150390625\n",
      "                , loss2: 0.051173198223114016\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2200 epoch, average loss: 94.25870971679687\n",
      "                , loss1: 4826.8984375\n",
      "                , loss2: 0.1341792345046997\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2220 epoch, average loss: 99.46134643554687\n",
      "                , loss1: 5086.771875\n",
      "                , loss2: 0.26929779052734376\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2240 epoch, average loss: 99.18285522460937\n",
      "                , loss1: 5078.746484375\n",
      "                , loss2: 0.147304904460907\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2260 epoch, average loss: 99.07938842773437\n",
      "                , loss1: 5076.469140625\n",
      "                , loss2: 0.08824002742767334\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2280 epoch, average loss: 99.06409301757813\n",
      "                , loss1: 5077.15546875\n",
      "                , loss2: 0.059551316499710086\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2300 epoch, average loss: 99.0607421875\n",
      "                , loss1: 5077.20546875\n",
      "                , loss2: 0.05523461699485779\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2320 epoch, average loss: 99.05901489257812\n",
      "                , loss1: 5077.15859375\n",
      "                , loss2: 0.054431354999542235\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2340 epoch, average loss: 99.06051025390624\n",
      "                , loss1: 5076.87890625\n",
      "                , loss2: 0.06136291027069092\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2360 epoch, average loss: 99.0580810546875\n",
      "                , loss1: 5077.1765625\n",
      "                , loss2: 0.05313774347305298\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2380 epoch, average loss: 99.05828247070312\n",
      "                , loss1: 5077.042578125\n",
      "                , loss2: 0.05594341158866882\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2400 epoch, average loss: 99.05931396484375\n",
      "                , loss1: 5077.12265625\n",
      "                , loss2: 0.05541766881942749\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2420 epoch, average loss: 99.05804443359375\n",
      "                , loss1: 5076.923828125\n",
      "                , loss2: 0.058015835285186765\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2440 epoch, average loss: 99.05809936523437\n",
      "                , loss1: 5077.198046875\n",
      "                , loss2: 0.052729558944702146\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2460 epoch, average loss: 99.05775146484375\n",
      "                , loss1: 5077.10078125\n",
      "                , loss2: 0.054285645484924316\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2480 epoch, average loss: 99.05731811523438\n",
      "                , loss1: 5077.1765625\n",
      "                , loss2: 0.05236449241638184\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2500 epoch, average loss: 99.05665893554688\n",
      "                , loss1: 5077.064453125\n",
      "                , loss2: 0.053900289535522464\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2520 epoch, average loss: 99.05821533203125\n",
      "                , loss1: 5077.159765625\n",
      "                , loss2: 0.05360526442527771\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2540 epoch, average loss: 99.05662841796875\n",
      "                , loss1: 5077.12578125\n",
      "                , loss2: 0.05269871354103088\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2560 epoch, average loss: 99.05645141601562\n",
      "                , loss1: 5077.0796875\n",
      "                , loss2: 0.05339809060096741\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2580 epoch, average loss: 99.13826293945313\n",
      "                , loss1: 5079.6484375\n",
      "                , loss2: 0.08511697649955749\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n",
      "in 2600 epoch, average loss: 99.0920166015625\n",
      "                , loss1: 5077.3984375\n",
      "                , loss2: 0.08274767398834229\n",
      "                , weight: 0.01950000000005797\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      6\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m sub\n\u001b[0;32m----> 7\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_1 \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5072\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(1,30000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    if loss_1 < 5072:\n",
    "        continue\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 20}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 20}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 20}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "    hgnn_trainer.train()\n",
    "    # if loss_1 < -8500 and loss_2 < 2 and cut() < 5072:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5072"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)\n",
    "(torch.max(num_nodes).item() - torch.min(num_nodes).item()) / num_nodes.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
