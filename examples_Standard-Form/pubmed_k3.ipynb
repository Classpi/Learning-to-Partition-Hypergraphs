{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) \n",
    "np.random.seed(seed)  \n",
    "random.seed(seed) \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 2\n",
    "\n",
    "# 1253 target\n",
    "weight = 50\n",
    "limit = 0.3\n",
    "sub = 0.022\n",
    "\n",
    "lr = 4e-3\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.9}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers131\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0}\n",
    "l_hyper_prmts[\"linerlayer42\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0}\n",
    "l_hyper_prmts[\"linerlayer421\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0}\n",
    "l_hyper_prmts[\"linerlayer422\"] = {\"in_channels\":16, \"out_channels\":8, \"use_bn\":False, \"drop_rate\":0}\n",
    "l_hyper_prmts[\"linerlayer423\"] = {\"in_channels\":8, \"out_channels\":3, \"use_bn\":True, \"drop_rate\":0}\n",
    "\n",
    "\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device, weight):\n",
    "\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "\n",
    "    X_ = outs.t().unsqueeze(-1)\n",
    "    H_ = H.unsqueeze(0)\n",
    "    xweight = H.sum(dim=0)\n",
    "    mid = X_.mul(H_)\n",
    "    sum = (mid * (1 / xweight)).sum()\n",
    "    sub = (mid + (1 - H)).prod(dim=1).sum()\n",
    "    loss_1 = sum - sub\n",
    "\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (9): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (12): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 35.04520263671875\n",
      "                , loss1: 116.8171875\n",
      "                , loss2: 4.5778354979120196e-05\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 10 epoch, average loss: 1014.62626953125\n",
      "                , loss1: 1196.49365234375\n",
      "                , loss2: 655.678076171875\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 20 epoch, average loss: 409.15205078125\n",
      "                , loss1: 1198.2732421875\n",
      "                , loss2: 49.670065307617186\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 30 epoch, average loss: 372.449755859375\n",
      "                , loss1: 1199.83818359375\n",
      "                , loss2: 12.498297119140625\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 40 epoch, average loss: 362.21572265625\n",
      "                , loss1: 1195.4662109375\n",
      "                , loss2: 3.5758567810058595\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 50 epoch, average loss: 358.88564453125\n",
      "                , loss1: 1192.5267578125\n",
      "                , loss2: 1.1275565147399902\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 60 epoch, average loss: 357.7085205078125\n",
      "                , loss1: 1190.526171875\n",
      "                , loss2: 0.5506796360015869\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 70 epoch, average loss: 357.162158203125\n",
      "                , loss1: 1189.91376953125\n",
      "                , loss2: 0.18801040649414064\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 80 epoch, average loss: 356.7476318359375\n",
      "                , loss1: 1188.9251953125\n",
      "                , loss2: 0.07001028060913086\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 90 epoch, average loss: 356.446484375\n",
      "                , loss1: 1187.9763671875\n",
      "                , loss2: 0.05357778072357178\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 100 epoch, average loss: 356.187451171875\n",
      "                , loss1: 1187.206640625\n",
      "                , loss2: 0.025430330634117128\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 110 epoch, average loss: 355.9135986328125\n",
      "                , loss1: 1186.2828125\n",
      "                , loss2: 0.02874211370944977\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 120 epoch, average loss: 355.577294921875\n",
      "                , loss1: 1185.14609375\n",
      "                , loss2: 0.03349561095237732\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 130 epoch, average loss: 355.1523681640625\n",
      "                , loss1: 1183.73662109375\n",
      "                , loss2: 0.031383785605430606\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 140 epoch, average loss: 354.71689453125\n",
      "                , loss1: 1182.3056640625\n",
      "                , loss2: 0.02516947388648987\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 150 epoch, average loss: 354.3472412109375\n",
      "                , loss1: 1181.09375\n",
      "                , loss2: 0.019108667969703674\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 160 epoch, average loss: 354.08125\n",
      "                , loss1: 1180.21669921875\n",
      "                , loss2: 0.01625092774629593\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 170 epoch, average loss: 353.897216796875\n",
      "                , loss1: 1179.60546875\n",
      "                , loss2: 0.015534710884094239\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 180 epoch, average loss: 353.7461181640625\n",
      "                , loss1: 1179.108203125\n",
      "                , loss2: 0.013644538819789886\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 190 epoch, average loss: 353.582373046875\n",
      "                , loss1: 1178.56552734375\n",
      "                , loss2: 0.012705154716968536\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 200 epoch, average loss: 353.37841796875\n",
      "                , loss1: 1177.88662109375\n",
      "                , loss2: 0.012454761564731598\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 210 epoch, average loss: 353.1812744140625\n",
      "                , loss1: 1177.23115234375\n",
      "                , loss2: 0.011910608410835266\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 220 epoch, average loss: 352.99873046875\n",
      "                , loss1: 1176.62724609375\n",
      "                , loss2: 0.01054358035326004\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 230 epoch, average loss: 352.8334716796875\n",
      "                , loss1: 1176.0720703125\n",
      "                , loss2: 0.011843234300613403\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 240 epoch, average loss: 352.685595703125\n",
      "                , loss1: 1175.58203125\n",
      "                , loss2: 0.010947734862565995\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 250 epoch, average loss: 352.5518798828125\n",
      "                , loss1: 1175.13359375\n",
      "                , loss2: 0.01175180748105049\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 260 epoch, average loss: 352.427880859375\n",
      "                , loss1: 1174.7216796875\n",
      "                , loss2: 0.011361049860715866\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 270 epoch, average loss: 352.3103271484375\n",
      "                , loss1: 1174.333203125\n",
      "                , loss2: 0.010374466329813004\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 280 epoch, average loss: 352.1995361328125\n",
      "                , loss1: 1173.9666015625\n",
      "                , loss2: 0.009587080776691436\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 290 epoch, average loss: 352.094482421875\n",
      "                , loss1: 1173.60859375\n",
      "                , loss2: 0.011900340765714645\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 300 epoch, average loss: 351.988037109375\n",
      "                , loss1: 1173.2470703125\n",
      "                , loss2: 0.013932633399963378\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 310 epoch, average loss: 351.8869140625\n",
      "                , loss1: 1172.921875\n",
      "                , loss2: 0.010351735353469848\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 320 epoch, average loss: 351.788037109375\n",
      "                , loss1: 1172.5953125\n",
      "                , loss2: 0.009465237706899643\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 330 epoch, average loss: 351.6827392578125\n",
      "                , loss1: 1172.2466796875\n",
      "                , loss2: 0.008715619146823884\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 340 epoch, average loss: 351.569091796875\n",
      "                , loss1: 1171.85478515625\n",
      "                , loss2: 0.012600331008434296\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 350 epoch, average loss: 351.4384765625\n",
      "                , loss1: 1171.4388671875\n",
      "                , loss2: 0.006809495389461517\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 360 epoch, average loss: 351.31318359375\n",
      "                , loss1: 1171.029296875\n",
      "                , loss2: 0.0043904207646846775\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 370 epoch, average loss: 351.199365234375\n",
      "                , loss1: 1170.6521484375\n",
      "                , loss2: 0.003719836473464966\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 380 epoch, average loss: 351.0921875\n",
      "                , loss1: 1170.29541015625\n",
      "                , loss2: 0.0035151444375514985\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 390 epoch, average loss: 350.99052734375\n",
      "                , loss1: 1169.95634765625\n",
      "                , loss2: 0.0036218490451574324\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 400 epoch, average loss: 350.890087890625\n",
      "                , loss1: 1169.62001953125\n",
      "                , loss2: 0.00405571348965168\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 410 epoch, average loss: 350.797509765625\n",
      "                , loss1: 1169.277734375\n",
      "                , loss2: 0.014180514216423034\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 420 epoch, average loss: 350.6895263671875\n",
      "                , loss1: 1168.9251953125\n",
      "                , loss2: 0.01197359710931778\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 430 epoch, average loss: 350.575830078125\n",
      "                , loss1: 1168.5587890625\n",
      "                , loss2: 0.008166196197271347\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 440 epoch, average loss: 350.47666015625\n",
      "                , loss1: 1168.1611328125\n",
      "                , loss2: 0.028287389874458314\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 450 epoch, average loss: 350.4159423828125\n",
      "                , loss1: 1167.69921875\n",
      "                , loss2: 0.10618109703063965\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 460 epoch, average loss: 350.2346435546875\n",
      "                , loss1: 1167.09921875\n",
      "                , loss2: 0.10488910675048828\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 470 epoch, average loss: 349.8887451171875\n",
      "                , loss1: 1166.15908203125\n",
      "                , loss2: 0.040969017148017886\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 480 epoch, average loss: 349.4899169921875\n",
      "                , loss1: 1164.87666015625\n",
      "                , loss2: 0.026927658915519716\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 490 epoch, average loss: 349.15810546875\n",
      "                , loss1: 1163.77138671875\n",
      "                , loss2: 0.02669943869113922\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 500 epoch, average loss: 349.057861328125\n",
      "                , loss1: 1163.1255859375\n",
      "                , loss2: 0.120158851146698\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 510 epoch, average loss: 348.852099609375\n",
      "                , loss1: 1162.57099609375\n",
      "                , loss2: 0.080778568983078\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 520 epoch, average loss: 348.6845458984375\n",
      "                , loss1: 1162.128515625\n",
      "                , loss2: 0.045976221561431885\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 530 epoch, average loss: 348.5572265625\n",
      "                , loss1: 1161.7314453125\n",
      "                , loss2: 0.03779956102371216\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 540 epoch, average loss: 348.456884765625\n",
      "                , loss1: 1161.39130859375\n",
      "                , loss2: 0.039500480890274046\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 550 epoch, average loss: 348.4040283203125\n",
      "                , loss1: 1161.09609375\n",
      "                , loss2: 0.07519826889038086\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 560 epoch, average loss: 348.522802734375\n",
      "                , loss1: 1160.890625\n",
      "                , loss2: 0.2556270122528076\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 570 epoch, average loss: 348.2619384765625\n",
      "                , loss1: 1160.6890625\n",
      "                , loss2: 0.05519660115242005\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 580 epoch, average loss: 348.1588134765625\n",
      "                , loss1: 1160.4185546875\n",
      "                , loss2: 0.033235573768615724\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 590 epoch, average loss: 348.073095703125\n",
      "                , loss1: 1160.16962890625\n",
      "                , loss2: 0.022193413972854615\n",
      "                , weight: 0.3\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(40000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    elif hgnn_trainer.weight < limit:\n",
    "        hgnn_trainer.weight = limit\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    if loss_1 < 1150 and loss_2 < 0.01:\n",
    "        break\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1274., 1275., 1275.], device='cuda:1', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
