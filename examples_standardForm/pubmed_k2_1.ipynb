{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 2\n",
    "\n",
    "\n",
    "weight = 50\n",
    "limit = 0.3\n",
    "sub = 0.022\n",
    "\n",
    "lr = 4e-3\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3824, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.1}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers131\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": True, \"drop_rate\": 0}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer42\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer421\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer422\"] = {\"in_channels\":16, \"out_channels\":8, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer423\"] = {\"in_channels\":8, \"out_channels\":2, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device, weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的标准.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "\n",
    "    X_ = outs.t().unsqueeze(-1)\n",
    "    H_ = H.unsqueeze(0)\n",
    "    xweight = H.sum(dim=0)\n",
    "    mid = X_.mul(H_)\n",
    "    sum = (mid * (1 / xweight)).sum()\n",
    "    sub = (mid + (1 - H)).prod(dim=1).sum()\n",
    "    loss_1 = sum - sub\n",
    "\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.05, inplace=False)\n",
       "  (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.05, inplace=False)\n",
       "  (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.05, inplace=False)\n",
       "  (9): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (13): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (17): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 33499.8625\n",
      "                , loss1: 537.16572265625\n",
      "                , loss2: 6653.39296875\n",
      "                , weight: 49.978\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 279048.9\n",
      "                , loss1: 5388.326953125\n",
      "                , loss2: 10402.3625\n",
      "                , weight: 49.75800000000002\n",
      "=================================\n",
      "in 20 epoch, average loss: 255408.3\n",
      "                , loss1: 5142.16640625\n",
      "                , loss2: 161.027978515625\n",
      "                , weight: 49.53800000000003\n",
      "=================================\n",
      "in 30 epoch, average loss: 245172.175\n",
      "                , loss1: 4959.50703125\n",
      "                , loss2: 86.12325439453124\n",
      "                , weight: 49.31800000000005\n",
      "=================================\n",
      "in 40 epoch, average loss: 235636.625\n",
      "                , loss1: 4789.271875\n",
      "                , loss2: 15.181535339355468\n",
      "                , weight: 49.09800000000006\n",
      "=================================\n",
      "in 50 epoch, average loss: 222261.65\n",
      "                , loss1: 4537.641796875\n",
      "                , loss2: 16.302291870117188\n",
      "                , weight: 48.87800000000008\n",
      "=================================\n",
      "in 60 epoch, average loss: 206991.775\n",
      "                , loss1: 4242.1453125\n",
      "                , loss2: 151.7797119140625\n",
      "                , weight: 48.658000000000094\n",
      "=================================\n",
      "in 70 epoch, average loss: 189617.675\n",
      "                , loss1: 3902.842578125\n",
      "                , loss2: 179.9584716796875\n",
      "                , weight: 48.43800000000011\n",
      "=================================\n",
      "in 80 epoch, average loss: 173273.875\n",
      "                , loss1: 3577.07109375\n",
      "                , loss2: 434.770556640625\n",
      "                , weight: 48.218000000000124\n",
      "=================================\n",
      "in 90 epoch, average loss: 155432.475\n",
      "                , loss1: 3229.76953125\n",
      "                , loss2: 82.39296875\n",
      "                , weight: 47.99800000000014\n",
      "=================================\n",
      "in 100 epoch, average loss: 130137.3\n",
      "                , loss1: 2711.9791015625\n",
      "                , loss2: 286.0658935546875\n",
      "                , weight: 47.778000000000155\n",
      "=================================\n",
      "in 110 epoch, average loss: 104488.7375\n",
      "                , loss1: 2188.7154296875\n",
      "                , loss2: 171.53240966796875\n",
      "                , weight: 47.55800000000017\n",
      "=================================\n",
      "in 120 epoch, average loss: 81300.08125\n",
      "                , loss1: 1710.129296875\n",
      "                , loss2: 169.746728515625\n",
      "                , weight: 47.338000000000186\n",
      "=================================\n",
      "in 130 epoch, average loss: 67455.1625\n",
      "                , loss1: 1425.4333984375\n",
      "                , loss2: 146.29947509765626\n",
      "                , weight: 47.1180000000002\n",
      "=================================\n",
      "in 140 epoch, average loss: 59244.5125\n",
      "                , loss1: 1258.246484375\n",
      "                , loss2: 108.49334716796875\n",
      "                , weight: 46.898000000000216\n",
      "=================================\n",
      "in 150 epoch, average loss: 54391.45\n",
      "                , loss1: 1160.998828125\n",
      "                , loss2: 81.72359619140624\n",
      "                , weight: 46.67800000000023\n",
      "=================================\n",
      "in 160 epoch, average loss: 50964.371875\n",
      "                , loss1: 1093.375390625\n",
      "                , loss2: 59.40933837890625\n",
      "                , weight: 46.45800000000025\n",
      "=================================\n",
      "in 170 epoch, average loss: 48623.7375\n",
      "                , loss1: 1047.795703125\n",
      "                , loss2: 71.411376953125\n",
      "                , weight: 46.23800000000026\n",
      "=================================\n",
      "in 180 epoch, average loss: 47044.675\n",
      "                , loss1: 1014.68642578125\n",
      "                , loss2: 250.0589111328125\n",
      "                , weight: 46.01800000000028\n",
      "=================================\n",
      "in 190 epoch, average loss: 45571.159375\n",
      "                , loss1: 990.87578125\n",
      "                , loss2: 92.39445190429687\n",
      "                , weight: 45.79800000000029\n",
      "=================================\n",
      "in 200 epoch, average loss: 44603.46875\n",
      "                , loss1: 974.621875\n",
      "                , loss2: 85.34666137695312\n",
      "                , weight: 45.57800000000031\n",
      "=================================\n",
      "in 210 epoch, average loss: 43444.16875\n",
      "                , loss1: 955.29306640625\n",
      "                , loss2: 19.125196838378905\n",
      "                , weight: 45.358000000000324\n",
      "=================================\n",
      "in 220 epoch, average loss: 42747.778125\n",
      "                , loss1: 943.991796875\n",
      "                , loss2: 44.29090270996094\n",
      "                , weight: 45.13800000000034\n",
      "=================================\n",
      "in 230 epoch, average loss: 42145.540625\n",
      "                , loss1: 934.5556640625\n",
      "                , loss2: 74.43840942382812\n",
      "                , weight: 44.918000000000355\n",
      "=================================\n",
      "in 240 epoch, average loss: 41482.64375\n",
      "                , loss1: 924.68984375\n",
      "                , loss2: 59.30579833984375\n",
      "                , weight: 44.69800000000037\n",
      "=================================\n",
      "in 250 epoch, average loss: 41076.153125\n",
      "                , loss1: 919.461328125\n",
      "                , loss2: 89.026513671875\n",
      "                , weight: 44.478000000000385\n",
      "=================================\n",
      "in 260 epoch, average loss: 40462.359375\n",
      "                , loss1: 911.35078125\n",
      "                , loss2: 37.409420776367185\n",
      "                , weight: 44.2580000000004\n",
      "=================================\n",
      "in 270 epoch, average loss: 39865.2125\n",
      "                , loss1: 902.51298828125\n",
      "                , loss2: 30.90532531738281\n",
      "                , weight: 44.038000000000416\n",
      "=================================\n",
      "in 280 epoch, average loss: 39438.48125\n",
      "                , loss1: 897.7390625\n",
      "                , loss2: 12.432136535644531\n",
      "                , weight: 43.81800000000043\n",
      "=================================\n",
      "in 290 epoch, average loss: 39089.05625\n",
      "                , loss1: 894.3431640625\n",
      "                , loss2: 8.785512542724609\n",
      "                , weight: 43.59800000000045\n",
      "=================================\n",
      "in 300 epoch, average loss: 38755.775\n",
      "                , loss1: 891.09921875\n",
      "                , loss2: 13.517330932617188\n",
      "                , weight: 43.37800000000046\n",
      "=================================\n",
      "in 310 epoch, average loss: 38391.965625\n",
      "                , loss1: 887.1947265625\n",
      "                , loss2: 14.451612854003907\n",
      "                , weight: 43.15800000000048\n",
      "=================================\n",
      "in 320 epoch, average loss: 38059.534375\n",
      "                , loss1: 883.64931640625\n",
      "                , loss2: 29.913961791992186\n",
      "                , weight: 42.93800000000049\n",
      "=================================\n",
      "in 330 epoch, average loss: 37620.921875\n",
      "                , loss1: 878.326171875\n",
      "                , loss2: 13.650083923339844\n",
      "                , weight: 42.71800000000051\n",
      "=================================\n",
      "in 340 epoch, average loss: 37344.35625\n",
      "                , loss1: 876.3888671875\n",
      "                , loss2: 12.695024108886718\n",
      "                , weight: 42.49800000000052\n",
      "=================================\n",
      "in 350 epoch, average loss: 36989.990625\n",
      "                , loss1: 872.7970703125\n",
      "                , loss2: 3.409346008300781\n",
      "                , weight: 42.27800000000054\n",
      "=================================\n",
      "in 360 epoch, average loss: 36749.321875\n",
      "                , loss1: 871.5369140625\n",
      "                , loss2: 7.9142333984375\n",
      "                , weight: 42.058000000000554\n",
      "=================================\n",
      "in 370 epoch, average loss: 36462.821875\n",
      "                , loss1: 869.1578125\n",
      "                , loss2: 12.870626831054688\n",
      "                , weight: 41.83800000000057\n",
      "=================================\n",
      "in 380 epoch, average loss: 36280.553125\n",
      "                , loss1: 869.6046875\n",
      "                , loss2: 3.1980300903320313\n",
      "                , weight: 41.618000000000585\n",
      "=================================\n",
      "in 390 epoch, average loss: 35984.81875\n",
      "                , loss1: 867.00078125\n",
      "                , loss2: 6.767733764648438\n",
      "                , weight: 41.3980000000006\n",
      "=================================\n",
      "in 400 epoch, average loss: 35740.784375\n",
      "                , loss1: 865.6962890625\n",
      "                , loss2: 7.3994804382324215\n",
      "                , weight: 41.178000000000615\n",
      "=================================\n",
      "in 410 epoch, average loss: 35468.271875\n",
      "                , loss1: 863.777734375\n",
      "                , loss2: 4.1432334899902346\n",
      "                , weight: 40.95800000000063\n",
      "=================================\n",
      "in 420 epoch, average loss: 35202.221875\n",
      "                , loss1: 861.6263671875\n",
      "                , loss2: 15.901571655273438\n",
      "                , weight: 40.738000000000646\n",
      "=================================\n",
      "in 430 epoch, average loss: 34918.3875\n",
      "                , loss1: 859.5419921875\n",
      "                , loss2: 6.311302185058594\n",
      "                , weight: 40.51800000000066\n",
      "=================================\n",
      "in 440 epoch, average loss: 34721.890625\n",
      "                , loss1: 859.415625\n",
      "                , loss2: 4.133869171142578\n",
      "                , weight: 40.29800000000068\n",
      "=================================\n",
      "in 450 epoch, average loss: 34460.521875\n",
      "                , loss1: 857.494140625\n",
      "                , loss2: 8.927789306640625\n",
      "                , weight: 40.07800000000069\n",
      "=================================\n",
      "in 460 epoch, average loss: 34261.025\n",
      "                , loss1: 857.38212890625\n",
      "                , loss2: 2.6786651611328125\n",
      "                , weight: 39.85800000000071\n",
      "=================================\n",
      "in 470 epoch, average loss: 34049.73125\n",
      "                , loss1: 856.71875\n",
      "                , loss2: 6.327130126953125\n",
      "                , weight: 39.63800000000072\n",
      "=================================\n",
      "in 480 epoch, average loss: 33857.65625\n",
      "                , loss1: 856.7171875\n",
      "                , loss2: 2.7005186080932617\n",
      "                , weight: 39.41800000000074\n",
      "=================================\n",
      "in 490 epoch, average loss: 33558.34375\n",
      "                , loss1: 853.8380859375\n",
      "                , loss2: 5.097836303710937\n",
      "                , weight: 39.198000000000754\n",
      "=================================\n",
      "in 500 epoch, average loss: 33361.184375\n",
      "                , loss1: 853.56728515625\n",
      "                , loss2: 6.3155170440673825\n",
      "                , weight: 38.97800000000077\n",
      "=================================\n",
      "in 510 epoch, average loss: 33179.0375\n",
      "                , loss1: 853.83740234375\n",
      "                , loss2: 1.508432388305664\n",
      "                , weight: 38.758000000000784\n",
      "=================================\n",
      "in 520 epoch, average loss: 32921.33125\n",
      "                , loss1: 851.9978515625\n",
      "                , loss2: 2.6654571533203124\n",
      "                , weight: 38.5380000000008\n",
      "=================================\n",
      "in 530 epoch, average loss: 32774.54375\n",
      "                , loss1: 853.0337890625\n",
      "                , loss2: 3.4663658142089844\n",
      "                , weight: 38.318000000000815\n",
      "=================================\n",
      "in 540 epoch, average loss: 32543.20625\n",
      "                , loss1: 851.876953125\n",
      "                , loss2: 4.031447219848633\n",
      "                , weight: 38.09800000000083\n",
      "=================================\n",
      "in 550 epoch, average loss: 32319.21875\n",
      "                , loss1: 850.976953125\n",
      "                , loss2: 1.6063188552856444\n",
      "                , weight: 37.878000000000846\n",
      "=================================\n",
      "in 560 epoch, average loss: 32207.603125\n",
      "                , loss1: 852.75302734375\n",
      "                , loss2: 10.303605651855468\n",
      "                , weight: 37.65800000000086\n",
      "=================================\n",
      "in 570 epoch, average loss: 38967.671875\n",
      "                , loss1: 887.3\n",
      "                , loss2: 5663.559375\n",
      "                , weight: 37.438000000000876\n",
      "=================================\n",
      "in 580 epoch, average loss: 37314.5125\n",
      "                , loss1: 938.36689453125\n",
      "                , loss2: 2298.6296875\n",
      "                , weight: 37.21800000000089\n",
      "=================================\n",
      "in 590 epoch, average loss: 33908.08125\n",
      "                , loss1: 908.4970703125\n",
      "                , loss2: 205.360400390625\n",
      "                , weight: 36.99800000000091\n",
      "=================================\n",
      "in 600 epoch, average loss: 33147.88125\n",
      "                , loss1: 896.75595703125\n",
      "                , loss2: 77.934814453125\n",
      "                , weight: 36.77800000000092\n",
      "=================================\n",
      "in 610 epoch, average loss: 32764.221875\n",
      "                , loss1: 892.0435546875\n",
      "                , loss2: 64.666796875\n",
      "                , weight: 36.55800000000094\n",
      "=================================\n",
      "in 620 epoch, average loss: 32306.096875\n",
      "                , loss1: 884.6982421875\n",
      "                , loss2: 70.24329223632813\n",
      "                , weight: 36.33800000000095\n",
      "=================================\n",
      "in 630 epoch, average loss: 31998.29375\n",
      "                , loss1: 882.1955078125\n",
      "                , loss2: 47.71600341796875\n",
      "                , weight: 36.11800000000097\n",
      "=================================\n",
      "in 640 epoch, average loss: 31586.8625\n",
      "                , loss1: 875.898046875\n",
      "                , loss2: 56.997894287109375\n",
      "                , weight: 35.898000000000984\n",
      "=================================\n",
      "in 650 epoch, average loss: 31382.275\n",
      "                , loss1: 875.802734375\n",
      "                , loss2: 48.561553955078125\n",
      "                , weight: 35.678000000001\n",
      "=================================\n",
      "in 660 epoch, average loss: 31024.740625\n",
      "                , loss1: 871.4775390625\n",
      "                , loss2: 37.552984619140624\n",
      "                , weight: 35.458000000001014\n",
      "=================================\n",
      "in 670 epoch, average loss: 30833.265625\n",
      "                , loss1: 871.376171875\n",
      "                , loss2: 41.526177978515626\n",
      "                , weight: 35.23800000000103\n",
      "=================================\n",
      "in 680 epoch, average loss: 30540.034375\n",
      "                , loss1: 868.381640625\n",
      "                , loss2: 45.04922790527344\n",
      "                , weight: 35.018000000001045\n",
      "=================================\n",
      "in 690 epoch, average loss: 30238.609375\n",
      "                , loss1: 865.3677734375\n",
      "                , loss2: 39.82264404296875\n",
      "                , weight: 34.79800000000106\n",
      "=================================\n",
      "in 700 epoch, average loss: 29978.540625\n",
      "                , loss1: 863.55458984375\n",
      "                , loss2: 33.061807250976564\n",
      "                , weight: 34.578000000001076\n",
      "=================================\n",
      "in 710 epoch, average loss: 29694.896875\n",
      "                , loss1: 860.89658203125\n",
      "                , loss2: 30.911514282226562\n",
      "                , weight: 34.35800000000109\n",
      "=================================\n",
      "in 720 epoch, average loss: 29438.9\n",
      "                , loss1: 859.4248046875\n",
      "                , loss2: 14.705123901367188\n",
      "                , weight: 34.13800000000111\n",
      "=================================\n",
      "in 730 epoch, average loss: 29229.496875\n",
      "                , loss1: 858.404296875\n",
      "                , loss2: 29.151605224609376\n",
      "                , weight: 33.91800000000112\n",
      "=================================\n",
      "in 740 epoch, average loss: 28940.00625\n",
      "                , loss1: 855.58134765625\n",
      "                , loss2: 23.873614501953124\n",
      "                , weight: 33.69800000000114\n",
      "=================================\n",
      "in 750 epoch, average loss: 28703.815625\n",
      "                , loss1: 854.3751953125\n",
      "                , loss2: 16.44642333984375\n",
      "                , weight: 33.47800000000115\n",
      "=================================\n",
      "in 760 epoch, average loss: 28511.8\n",
      "                , loss1: 854.1150390625\n",
      "                , loss2: 21.028627014160158\n",
      "                , weight: 33.25800000000117\n",
      "=================================\n",
      "in 770 epoch, average loss: 28256.15625\n",
      "                , loss1: 852.3103515625\n",
      "                , loss2: 13.157733154296874\n",
      "                , weight: 33.03800000000118\n",
      "=================================\n",
      "in 780 epoch, average loss: 28078.875\n",
      "                , loss1: 852.38671875\n",
      "                , loss2: 20.78613739013672\n",
      "                , weight: 32.8180000000012\n",
      "=================================\n",
      "in 790 epoch, average loss: 27881.971875\n",
      "                , loss1: 852.15390625\n",
      "                , loss2: 19.050691223144533\n",
      "                , weight: 32.598000000001214\n",
      "=================================\n",
      "in 800 epoch, average loss: 27637.94375\n",
      "                , loss1: 850.356640625\n",
      "                , loss2: 20.940083312988282\n",
      "                , weight: 32.37800000000123\n",
      "=================================\n",
      "in 810 epoch, average loss: 27435.203125\n",
      "                , loss1: 850.2048828125\n",
      "                , loss2: 10.162483978271485\n",
      "                , weight: 32.158000000001245\n",
      "=================================\n",
      "in 820 epoch, average loss: 27233.8875\n",
      "                , loss1: 849.809375\n",
      "                , loss2: 8.56058349609375\n",
      "                , weight: 31.93800000000126\n",
      "=================================\n",
      "in 830 epoch, average loss: 27019.684375\n",
      "                , loss1: 849.08681640625\n",
      "                , loss2: 4.257029342651367\n",
      "                , weight: 31.718000000001275\n",
      "=================================\n",
      "in 840 epoch, average loss: 26822.784375\n",
      "                , loss1: 848.79013671875\n",
      "                , loss2: 3.567036437988281\n",
      "                , weight: 31.49800000000129\n",
      "=================================\n",
      "in 850 epoch, average loss: 26632.165625\n",
      "                , loss1: 848.251171875\n",
      "                , loss2: 16.585768127441405\n",
      "                , weight: 31.278000000001306\n",
      "=================================\n",
      "in 860 epoch, average loss: 26425.7875\n",
      "                , loss1: 847.9162109375\n",
      "                , loss2: 7.250434875488281\n",
      "                , weight: 31.05800000000132\n",
      "=================================\n",
      "in 870 epoch, average loss: 26251.015625\n",
      "                , loss1: 848.3623046875\n",
      "                , loss2: 5.248190307617188\n",
      "                , weight: 30.838000000001337\n",
      "=================================\n",
      "in 880 epoch, average loss: 26022.59375\n",
      "                , loss1: 846.93359375\n",
      "                , loss2: 7.291627502441406\n",
      "                , weight: 30.618000000001352\n",
      "=================================\n",
      "in 890 epoch, average loss: 25833.4453125\n",
      "                , loss1: 846.8490234375\n",
      "                , loss2: 7.089579772949219\n",
      "                , weight: 30.398000000001367\n",
      "=================================\n",
      "in 900 epoch, average loss: 25684.8109375\n",
      "                , loss1: 847.827734375\n",
      "                , loss2: 15.089671325683593\n",
      "                , weight: 30.178000000001383\n",
      "=================================\n",
      "in 910 epoch, average loss: 25553.403125\n",
      "                , loss1: 848.94521484375\n",
      "                , loss2: 36.85600280761719\n",
      "                , weight: 29.958000000001398\n",
      "=================================\n",
      "in 920 epoch, average loss: 25364.3390625\n",
      "                , loss1: 849.7599609375\n",
      "                , loss2: 10.181172180175782\n",
      "                , weight: 29.738000000001414\n",
      "=================================\n",
      "in 930 epoch, average loss: 25071.284375\n",
      "                , loss1: 846.2623046875\n",
      "                , loss2: 7.499400329589844\n",
      "                , weight: 29.51800000000143\n",
      "=================================\n",
      "in 940 epoch, average loss: 24905.5203125\n",
      "                , loss1: 846.913671875\n",
      "                , loss2: 8.718707275390624\n",
      "                , weight: 29.298000000001444\n",
      "=================================\n",
      "in 950 epoch, average loss: 24642.5046875\n",
      "                , loss1: 844.496484375\n",
      "                , loss2: 2.6458473205566406\n",
      "                , weight: 29.07800000000146\n",
      "=================================\n",
      "in 960 epoch, average loss: 24452.25\n",
      "                , loss1: 844.226953125\n",
      "                , loss2: 5.972486877441407\n",
      "                , weight: 28.858000000001475\n",
      "=================================\n",
      "in 970 epoch, average loss: 24255.915625\n",
      "                , loss1: 843.6978515625\n",
      "                , loss2: 10.529460144042968\n",
      "                , weight: 28.63800000000149\n",
      "=================================\n",
      "in 980 epoch, average loss: 24059.403125\n",
      "                , loss1: 843.384375\n",
      "                , loss2: 8.578939819335938\n",
      "                , weight: 28.418000000001506\n",
      "=================================\n",
      "in 990 epoch, average loss: 23877.1484375\n",
      "                , loss1: 843.651953125\n",
      "                , loss2: 4.402238464355468\n",
      "                , weight: 28.19800000000152\n",
      "=================================\n",
      "in 1000 epoch, average loss: 23668.4515625\n",
      "                , loss1: 842.810546875\n",
      "                , loss2: 4.781666564941406\n",
      "                , weight: 27.978000000001536\n",
      "=================================\n",
      "in 1010 epoch, average loss: 23496.26875\n",
      "                , loss1: 843.1107421875\n",
      "                , loss2: 9.75652618408203\n",
      "                , weight: 27.75800000000155\n",
      "=================================\n",
      "in 1020 epoch, average loss: 23313.30625\n",
      "                , loss1: 842.7966796875\n",
      "                , loss2: 20.940928649902343\n",
      "                , weight: 27.538000000001567\n",
      "=================================\n",
      "in 1030 epoch, average loss: 23121.21875\n",
      "                , loss1: 843.0091796875\n",
      "                , loss2: 8.436785125732422\n",
      "                , weight: 27.318000000001582\n",
      "=================================\n",
      "in 1040 epoch, average loss: 22921.9390625\n",
      "                , loss1: 842.5724609375\n",
      "                , loss2: 6.467488098144531\n",
      "                , weight: 27.098000000001598\n",
      "=================================\n",
      "in 1050 epoch, average loss: 22721.834375\n",
      "                , loss1: 842.0869140625\n",
      "                , loss2: 4.848675537109375\n",
      "                , weight: 26.878000000001613\n",
      "=================================\n",
      "in 1060 epoch, average loss: 22537.5359375\n",
      "                , loss1: 842.0478515625\n",
      "                , loss2: 6.88245849609375\n",
      "                , weight: 26.65800000000163\n",
      "=================================\n",
      "in 1070 epoch, average loss: 22354.1671875\n",
      "                , loss1: 842.13193359375\n",
      "                , loss2: 6.466197204589844\n",
      "                , weight: 26.438000000001644\n",
      "=================================\n",
      "in 1080 epoch, average loss: 22156.2234375\n",
      "                , loss1: 841.482421875\n",
      "                , loss2: 10.931275939941406\n",
      "                , weight: 26.21800000000166\n",
      "=================================\n",
      "in 1090 epoch, average loss: 21983.3921875\n",
      "                , loss1: 842.151171875\n",
      "                , loss2: 5.777473068237304\n",
      "                , weight: 25.998000000001674\n",
      "=================================\n",
      "in 1100 epoch, average loss: 21780.5140625\n",
      "                , loss1: 841.421875\n",
      "                , loss2: 7.038557434082032\n",
      "                , weight: 25.77800000000169\n",
      "=================================\n",
      "in 1110 epoch, average loss: 21590.3890625\n",
      "                , loss1: 841.37080078125\n",
      "                , loss2: 3.351161575317383\n",
      "                , weight: 25.558000000001705\n",
      "=================================\n",
      "in 1120 epoch, average loss: 21407.8609375\n",
      "                , loss1: 841.38515625\n",
      "                , loss2: 5.568210601806641\n",
      "                , weight: 25.33800000000172\n",
      "=================================\n",
      "in 1130 epoch, average loss: 21213.640625\n",
      "                , loss1: 841.12783203125\n",
      "                , loss2: 2.922529411315918\n",
      "                , weight: 25.118000000001736\n",
      "=================================\n",
      "in 1140 epoch, average loss: 21033.00625\n",
      "                , loss1: 841.37626953125\n",
      "                , loss2: 1.14512939453125\n",
      "                , weight: 24.89800000000175\n",
      "=================================\n",
      "in 1150 epoch, average loss: 20836.7125\n",
      "                , loss1: 840.74619140625\n",
      "                , loss2: 5.549310302734375\n",
      "                , weight: 24.678000000001767\n",
      "=================================\n",
      "in 1160 epoch, average loss: 20683.775\n",
      "                , loss1: 841.346875\n",
      "                , loss2: 22.819256591796876\n",
      "                , weight: 24.458000000001782\n",
      "=================================\n",
      "in 1170 epoch, average loss: 20488.66875\n",
      "                , loss1: 841.2640625\n",
      "                , loss2: 14.813090515136718\n",
      "                , weight: 24.238000000001797\n",
      "=================================\n",
      "in 1180 epoch, average loss: 20277.9640625\n",
      "                , loss1: 840.6935546875\n",
      "                , loss2: 2.952850341796875\n",
      "                , weight: 24.018000000001813\n",
      "=================================\n",
      "in 1190 epoch, average loss: 20093.2078125\n",
      "                , loss1: 840.7150390625\n",
      "                , loss2: 2.6441156387329103\n",
      "                , weight: 23.798000000001828\n",
      "=================================\n",
      "in 1200 epoch, average loss: 19906.6578125\n",
      "                , loss1: 840.601171875\n",
      "                , loss2: 3.768595504760742\n",
      "                , weight: 23.578000000001843\n",
      "=================================\n",
      "in 1210 epoch, average loss: 19724.18125\n",
      "                , loss1: 840.73203125\n",
      "                , loss2: 3.1287715911865233\n",
      "                , weight: 23.35800000000186\n",
      "=================================\n",
      "in 1220 epoch, average loss: 19514.5421875\n",
      "                , loss1: 839.7171875\n",
      "                , loss2: 2.039093780517578\n",
      "                , weight: 23.138000000001874\n",
      "=================================\n",
      "in 1230 epoch, average loss: 19338.15\n",
      "                , loss1: 840.11669921875\n",
      "                , loss2: 1.1689814567565917\n",
      "                , weight: 22.91800000000189\n",
      "=================================\n",
      "in 1240 epoch, average loss: 19143.7671875\n",
      "                , loss1: 839.6392578125\n",
      "                , loss2: 2.4808324813842773\n",
      "                , weight: 22.698000000001905\n",
      "=================================\n",
      "in 1250 epoch, average loss: 18972.1453125\n",
      "                , loss1: 840.13076171875\n",
      "                , loss2: 4.508792114257813\n",
      "                , weight: 22.47800000000192\n",
      "=================================\n",
      "in 1260 epoch, average loss: 18804.0625\n",
      "                , loss1: 840.7873046875\n",
      "                , loss2: 6.5443603515625\n",
      "                , weight: 22.258000000001935\n",
      "=================================\n",
      "in 1270 epoch, average loss: 18599.3203125\n",
      "                , loss1: 839.98515625\n",
      "                , loss2: 4.551344299316407\n",
      "                , weight: 22.03800000000195\n",
      "=================================\n",
      "in 1280 epoch, average loss: 18415.0359375\n",
      "                , loss1: 840.1095703125\n",
      "                , loss2: 2.3198089599609375\n",
      "                , weight: 21.818000000001966\n",
      "=================================\n",
      "in 1290 epoch, average loss: 18219.490625\n",
      "                , loss1: 839.652734375\n",
      "                , loss2: 1.518269920349121\n",
      "                , weight: 21.59800000000198\n",
      "=================================\n",
      "in 1300 epoch, average loss: 18037.7234375\n",
      "                , loss1: 839.806640625\n",
      "                , loss2: 1.2080677032470704\n",
      "                , weight: 21.378000000001997\n",
      "=================================\n",
      "in 1310 epoch, average loss: 17846.5\n",
      "                , loss1: 839.3556640625\n",
      "                , loss2: 4.305820083618164\n",
      "                , weight: 21.158000000002012\n",
      "=================================\n",
      "in 1320 epoch, average loss: 17668.5875\n",
      "                , loss1: 839.65615234375\n",
      "                , loss2: 4.769568634033203\n",
      "                , weight: 20.938000000002027\n",
      "=================================\n",
      "in 1330 epoch, average loss: 17481.134375\n",
      "                , loss1: 839.6083984375\n",
      "                , loss2: 3.011532783508301\n",
      "                , weight: 20.718000000002043\n",
      "=================================\n",
      "in 1340 epoch, average loss: 17287.0796875\n",
      "                , loss1: 839.0765625\n",
      "                , loss2: 4.613927841186523\n",
      "                , weight: 20.498000000002058\n",
      "=================================\n",
      "in 1350 epoch, average loss: 17114.065625\n",
      "                , loss1: 839.80703125\n",
      "                , loss2: 1.2919609069824218\n",
      "                , weight: 20.278000000002073\n",
      "=================================\n",
      "in 1360 epoch, average loss: 16912.95625\n",
      "                , loss1: 838.9810546875\n",
      "                , loss2: 1.6090957641601562\n",
      "                , weight: 20.05800000000209\n",
      "=================================\n",
      "in 1370 epoch, average loss: 16749.5671875\n",
      "                , loss1: 840.0740234375\n",
      "                , loss2: 1.0547844886779785\n",
      "                , weight: 19.838000000002104\n",
      "=================================\n",
      "in 1380 epoch, average loss: 16546.8078125\n",
      "                , loss1: 839.1779296875\n",
      "                , loss2: 0.7575538158416748\n",
      "                , weight: 19.61800000000212\n",
      "=================================\n",
      "in 1390 epoch, average loss: 16369.4046875\n",
      "                , loss1: 839.5263671875\n",
      "                , loss2: 1.1751201629638672\n",
      "                , weight: 19.398000000002135\n",
      "=================================\n",
      "in 1400 epoch, average loss: 16176.3921875\n",
      "                , loss1: 839.066015625\n",
      "                , loss2: 1.706675148010254\n",
      "                , weight: 19.17800000000215\n",
      "=================================\n",
      "in 1410 epoch, average loss: 15995.48125\n",
      "                , loss1: 839.2658203125\n",
      "                , loss2: 1.6233539581298828\n",
      "                , weight: 18.958000000002166\n",
      "=================================\n",
      "in 1420 epoch, average loss: 15805.5734375\n",
      "                , loss1: 838.7447265625\n",
      "                , loss2: 6.174396514892578\n",
      "                , weight: 18.73800000000218\n",
      "=================================\n",
      "in 1430 epoch, average loss: 15630.4015625\n",
      "                , loss1: 839.3259765625\n",
      "                , loss2: 4.615161895751953\n",
      "                , weight: 18.518000000002196\n",
      "=================================\n",
      "in 1440 epoch, average loss: 15443.19375\n",
      "                , loss1: 839.36083984375\n",
      "                , loss2: 1.4459750175476074\n",
      "                , weight: 18.29800000000221\n",
      "=================================\n",
      "in 1450 epoch, average loss: 15258.7296875\n",
      "                , loss1: 839.3408203125\n",
      "                , loss2: 2.0423179626464845\n",
      "                , weight: 18.078000000002227\n",
      "=================================\n",
      "in 1460 epoch, average loss: 15083.5265625\n",
      "                , loss1: 839.57841796875\n",
      "                , loss2: 7.232605743408203\n",
      "                , weight: 17.858000000002242\n",
      "=================================\n",
      "in 1470 epoch, average loss: 14894.453125\n",
      "                , loss1: 839.55849609375\n",
      "                , loss2: 3.247903060913086\n",
      "                , weight: 17.638000000002258\n",
      "=================================\n",
      "in 1480 epoch, average loss: 14707.01875\n",
      "                , loss1: 839.520703125\n",
      "                , loss2: 1.121255111694336\n",
      "                , weight: 17.418000000002273\n",
      "=================================\n",
      "in 1490 epoch, average loss: 14524.8375\n",
      "                , loss1: 839.62880859375\n",
      "                , loss2: 1.7601545333862305\n",
      "                , weight: 17.19800000000229\n",
      "=================================\n",
      "in 1500 epoch, average loss: 14327.7359375\n",
      "                , loss1: 838.9310546875\n",
      "                , loss2: 1.3472605705261231\n",
      "                , weight: 16.978000000002304\n",
      "=================================\n",
      "in 1510 epoch, average loss: 14141.0125\n",
      "                , loss1: 838.8166015625\n",
      "                , loss2: 1.079559326171875\n",
      "                , weight: 16.75800000000232\n",
      "=================================\n",
      "in 1520 epoch, average loss: 13953.734375\n",
      "                , loss1: 838.66025390625\n",
      "                , loss2: 0.9483860015869141\n",
      "                , weight: 16.538000000002334\n",
      "=================================\n",
      "in 1530 epoch, average loss: 13758.5890625\n",
      "                , loss1: 838.01484375\n",
      "                , loss2: 0.9035202980041503\n",
      "                , weight: 16.31800000000235\n",
      "=================================\n",
      "in 1540 epoch, average loss: 13577.9390625\n",
      "                , loss1: 838.21259765625\n",
      "                , loss2: 1.4252754211425782\n",
      "                , weight: 16.098000000002365\n",
      "=================================\n",
      "in 1550 epoch, average loss: 13415.775\n",
      "                , loss1: 838.94453125\n",
      "                , loss2: 11.95233154296875\n",
      "                , weight: 15.87800000000237\n",
      "=================================\n",
      "in 1560 epoch, average loss: 13229.1484375\n",
      "                , loss1: 839.07763671875\n",
      "                , loss2: 7.773902893066406\n",
      "                , weight: 15.658000000002367\n",
      "=================================\n",
      "in 1570 epoch, average loss: 13038.08125\n",
      "                , loss1: 839.00693359375\n",
      "                , loss2: 2.435232162475586\n",
      "                , weight: 15.438000000002365\n",
      "=================================\n",
      "in 1580 epoch, average loss: 12849.66171875\n",
      "                , loss1: 838.85712890625\n",
      "                , loss2: 0.8684492111206055\n",
      "                , weight: 15.218000000002363\n",
      "=================================\n",
      "in 1590 epoch, average loss: 12651.9765625\n",
      "                , loss1: 837.980859375\n",
      "                , loss2: 0.9633548736572266\n",
      "                , weight: 14.99800000000236\n",
      "=================================\n",
      "in 1600 epoch, average loss: 12480.596875\n",
      "                , loss1: 838.7841796875\n",
      "                , loss2: 2.020376777648926\n",
      "                , weight: 14.778000000002358\n",
      "=================================\n",
      "in 1610 epoch, average loss: 12301.990625\n",
      "                , loss1: 839.1974609375\n",
      "                , loss2: 1.834156608581543\n",
      "                , weight: 14.558000000002355\n",
      "=================================\n",
      "in 1620 epoch, average loss: 12100.26484375\n",
      "                , loss1: 838.05517578125\n",
      "                , loss2: 1.24742431640625\n",
      "                , weight: 14.338000000002353\n",
      "=================================\n",
      "in 1630 epoch, average loss: 11915.51875\n",
      "                , loss1: 838.0033203125\n",
      "                , loss2: 1.6538145065307617\n",
      "                , weight: 14.11800000000235\n",
      "=================================\n",
      "in 1640 epoch, average loss: 11733.54921875\n",
      "                , loss1: 838.1900390625\n",
      "                , loss2: 1.3774887084960938\n",
      "                , weight: 13.898000000002348\n",
      "=================================\n",
      "in 1650 epoch, average loss: 11544.5453125\n",
      "                , loss1: 837.7158203125\n",
      "                , loss2: 3.3105632781982424\n",
      "                , weight: 13.678000000002346\n",
      "=================================\n",
      "in 1660 epoch, average loss: 11360.36875\n",
      "                , loss1: 837.803125\n",
      "                , loss2: 2.2821985244750977\n",
      "                , weight: 13.458000000002343\n",
      "=================================\n",
      "in 1670 epoch, average loss: 11173.7875\n",
      "                , loss1: 837.649609375\n",
      "                , loss2: 2.0387439727783203\n",
      "                , weight: 13.23800000000234\n",
      "=================================\n",
      "in 1680 epoch, average loss: 10986.98046875\n",
      "                , loss1: 837.55634765625\n",
      "                , loss2: 0.7398710250854492\n",
      "                , weight: 13.018000000002338\n",
      "=================================\n",
      "in 1690 epoch, average loss: 10796.98671875\n",
      "                , loss1: 837.103515625\n",
      "                , loss2: 0.8610179901123047\n",
      "                , weight: 12.798000000002336\n",
      "=================================\n",
      "in 1700 epoch, average loss: 10619.27109375\n",
      "                , loss1: 837.57734375\n",
      "                , loss2: 1.2836479187011718\n",
      "                , weight: 12.578000000002334\n",
      "=================================\n",
      "in 1710 epoch, average loss: 10436.58984375\n",
      "                , loss1: 837.541796875\n",
      "                , loss2: 3.312748336791992\n",
      "                , weight: 12.358000000002331\n",
      "=================================\n",
      "in 1720 epoch, average loss: 10256.98046875\n",
      "                , loss1: 838.0728515625\n",
      "                , loss2: 1.4788165092468262\n",
      "                , weight: 12.138000000002329\n",
      "=================================\n",
      "in 1730 epoch, average loss: 10071.16015625\n",
      "                , loss1: 837.80498046875\n",
      "                , loss2: 3.2684364318847656\n",
      "                , weight: 11.918000000002326\n",
      "=================================\n",
      "in 1740 epoch, average loss: 9880.61640625\n",
      "                , loss1: 837.28134765625\n",
      "                , loss2: 3.226537322998047\n",
      "                , weight: 11.698000000002324\n",
      "=================================\n",
      "in 1750 epoch, average loss: 9702.8265625\n",
      "                , loss1: 837.8123046875\n",
      "                , loss2: 3.474070739746094\n",
      "                , weight: 11.478000000002321\n",
      "=================================\n",
      "in 1760 epoch, average loss: 9516.79296875\n",
      "                , loss1: 837.6671875\n",
      "                , loss2: 3.3818428039550783\n",
      "                , weight: 11.258000000002319\n",
      "=================================\n",
      "in 1770 epoch, average loss: 9334.81875\n",
      "                , loss1: 837.97578125\n",
      "                , loss2: 2.321625328063965\n",
      "                , weight: 11.038000000002317\n",
      "=================================\n",
      "in 1780 epoch, average loss: 9143.34453125\n",
      "                , loss1: 837.4408203125\n",
      "                , loss2: 1.0078824996948241\n",
      "                , weight: 10.818000000002314\n",
      "=================================\n",
      "in 1790 epoch, average loss: 8950.55234375\n",
      "                , loss1: 836.66962890625\n",
      "                , loss2: 0.6982688903808594\n",
      "                , weight: 10.598000000002312\n",
      "=================================\n",
      "in 1800 epoch, average loss: 8771.62265625\n",
      "                , loss1: 837.1603515625\n",
      "                , loss2: 0.7093186855316163\n",
      "                , weight: 10.37800000000231\n",
      "=================================\n",
      "in 1810 epoch, average loss: 8588.5453125\n",
      "                , loss1: 837.305078125\n",
      "                , loss2: 0.3176755905151367\n",
      "                , weight: 10.158000000002307\n",
      "=================================\n",
      "in 1820 epoch, average loss: 8400.29375\n",
      "                , loss1: 836.8453125\n",
      "                , loss2: 0.8822415351867676\n",
      "                , weight: 9.938000000002305\n",
      "=================================\n",
      "in 1830 epoch, average loss: 8218.325\n",
      "                , loss1: 836.9919921875\n",
      "                , loss2: 1.5744283676147461\n",
      "                , weight: 9.718000000002302\n",
      "=================================\n",
      "in 1840 epoch, average loss: 8038.18203125\n",
      "                , loss1: 837.47060546875\n",
      "                , loss2: 1.0162058830261231\n",
      "                , weight: 9.4980000000023\n",
      "=================================\n",
      "in 1850 epoch, average loss: 7856.675\n",
      "                , loss1: 837.80546875\n",
      "                , loss2: 0.5099163055419922\n",
      "                , weight: 9.278000000002297\n",
      "=================================\n",
      "in 1860 epoch, average loss: 7662.62890625\n",
      "                , loss1: 836.75634765625\n",
      "                , loss2: 0.4615313529968262\n",
      "                , weight: 9.058000000002295\n",
      "=================================\n",
      "in 1870 epoch, average loss: 7477.63671875\n",
      "                , loss1: 836.62314453125\n",
      "                , loss2: 0.7259109020233154\n",
      "                , weight: 8.838000000002292\n",
      "=================================\n",
      "in 1880 epoch, average loss: 7291.38203125\n",
      "                , loss1: 836.43359375\n",
      "                , loss2: 0.1978590965270996\n",
      "                , weight: 8.61800000000229\n",
      "=================================\n",
      "in 1890 epoch, average loss: 7111.6296875\n",
      "                , loss1: 836.89970703125\n",
      "                , loss2: 0.49817981719970705\n",
      "                , weight: 8.398000000002288\n",
      "=================================\n",
      "in 1900 epoch, average loss: 6935.565625\n",
      "                , loss1: 837.9\n",
      "                , loss2: 0.3493642807006836\n",
      "                , weight: 8.178000000002285\n",
      "=================================\n",
      "in 1910 epoch, average loss: 6743.1578125\n",
      "                , loss1: 836.8576171875\n",
      "                , loss2: 0.5796772480010987\n",
      "                , weight: 7.958000000002283\n",
      "=================================\n",
      "in 1920 epoch, average loss: 6566.940625\n",
      "                , loss1: 837.88671875\n",
      "                , loss2: 0.44074177742004395\n",
      "                , weight: 7.73800000000228\n",
      "=================================\n",
      "in 1930 epoch, average loss: 6383.3078125\n",
      "                , loss1: 837.6359375\n",
      "                , loss2: 2.9825002670288088\n",
      "                , weight: 7.518000000002278\n",
      "=================================\n",
      "in 1940 epoch, average loss: 6191.48671875\n",
      "                , loss1: 836.9349609375\n",
      "                , loss2: 0.6876172542572021\n",
      "                , weight: 7.2980000000022756\n",
      "=================================\n",
      "in 1950 epoch, average loss: 6013.213671875\n",
      "                , loss1: 837.57158203125\n",
      "                , loss2: 1.9668880462646485\n",
      "                , weight: 7.078000000002273\n",
      "=================================\n",
      "in 1960 epoch, average loss: 5828.681640625\n",
      "                , loss1: 837.45703125\n",
      "                , loss2: 2.471907043457031\n",
      "                , weight: 6.858000000002271\n",
      "=================================\n",
      "in 1970 epoch, average loss: 5638.05859375\n",
      "                , loss1: 836.7775390625\n",
      "                , loss2: 0.6945215225219726\n",
      "                , weight: 6.638000000002268\n",
      "=================================\n",
      "in 1980 epoch, average loss: 5451.5328125\n",
      "                , loss1: 836.4232421875\n",
      "                , loss2: 0.5644543170928955\n",
      "                , weight: 6.418000000002266\n",
      "=================================\n",
      "in 1990 epoch, average loss: 5269.3\n",
      "                , loss1: 836.725\n",
      "                , loss2: 0.43319411277770997\n",
      "                , weight: 6.1980000000022635\n",
      "=================================\n",
      "in 2000 epoch, average loss: 5086.479296875\n",
      "                , loss1: 836.93388671875\n",
      "                , loss2: 0.44958343505859377\n",
      "                , weight: 5.978000000002261\n",
      "=================================\n",
      "in 2010 epoch, average loss: 4909.166015625\n",
      "                , loss1: 837.9091796875\n",
      "                , loss2: 1.4790732383728027\n",
      "                , weight: 5.758000000002259\n",
      "=================================\n",
      "in 2020 epoch, average loss: 4716.826953125\n",
      "                , loss1: 836.6083984375\n",
      "                , loss2: 0.8485400199890136\n",
      "                , weight: 5.538000000002256\n",
      "=================================\n",
      "in 2030 epoch, average loss: 4542.188671875\n",
      "                , loss1: 838.451953125\n",
      "                , loss2: 0.17757854461669922\n",
      "                , weight: 5.318000000002254\n",
      "=================================\n",
      "in 2040 epoch, average loss: 4348.732421875\n",
      "                , loss1: 836.6759765625\n",
      "                , loss2: 0.527266263961792\n",
      "                , weight: 5.098000000002251\n",
      "=================================\n",
      "in 2050 epoch, average loss: 4163.962109375\n",
      "                , loss1: 836.5078125\n",
      "                , loss2: 0.657982587814331\n",
      "                , weight: 4.878000000002249\n",
      "=================================\n",
      "in 2060 epoch, average loss: 3983.48828125\n",
      "                , loss1: 837.1216796875\n",
      "                , loss2: 1.2822015762329102\n",
      "                , weight: 4.658000000002247\n",
      "=================================\n",
      "in 2070 epoch, average loss: 3796.146484375\n",
      "                , loss1: 836.65986328125\n",
      "                , loss2: 0.22947947978973388\n",
      "                , weight: 4.438000000002244\n",
      "=================================\n",
      "in 2080 epoch, average loss: 3616.33125\n",
      "                , loss1: 837.2052734375\n",
      "                , loss2: 2.1642385482788087\n",
      "                , weight: 4.218000000002242\n",
      "=================================\n",
      "in 2090 epoch, average loss: 3429.18125\n",
      "                , loss1: 836.69453125\n",
      "                , loss2: 1.2520622253417968\n",
      "                , weight: 3.9980000000022398\n",
      "=================================\n",
      "in 2100 epoch, average loss: 3242.840625\n",
      "                , loss1: 836.3166015625\n",
      "                , loss2: 0.4494227409362793\n",
      "                , weight: 3.778000000002242\n",
      "=================================\n",
      "in 2110 epoch, average loss: 3060.080859375\n",
      "                , loss1: 836.569140625\n",
      "                , loss2: 0.7479358673095703\n",
      "                , weight: 3.558000000002244\n",
      "=================================\n",
      "in 2120 epoch, average loss: 2875.301171875\n",
      "                , loss1: 836.47119140625\n",
      "                , loss2: 0.34748005867004395\n",
      "                , weight: 3.338000000002246\n",
      "=================================\n",
      "in 2130 epoch, average loss: 2691.9865234375\n",
      "                , loss1: 836.607421875\n",
      "                , loss2: 0.6464327335357666\n",
      "                , weight: 3.118000000002248\n",
      "=================================\n",
      "in 2140 epoch, average loss: 2507.5240234375\n",
      "                , loss1: 836.573828125\n",
      "                , loss2: 0.2989038944244385\n",
      "                , weight: 2.89800000000225\n",
      "=================================\n",
      "in 2150 epoch, average loss: 2324.1556640625\n",
      "                , loss1: 836.6962890625\n",
      "                , loss2: 0.6411079883575439\n",
      "                , weight: 2.678000000002252\n",
      "=================================\n",
      "in 2160 epoch, average loss: 2138.2435546875\n",
      "                , loss1: 836.1615234375\n",
      "                , loss2: 0.1949763059616089\n",
      "                , weight: 2.458000000002254\n",
      "=================================\n",
      "in 2170 epoch, average loss: 1956.32109375\n",
      "                , loss1: 836.8380859375\n",
      "                , loss2: 0.5770780563354492\n",
      "                , weight: 2.238000000002256\n",
      "=================================\n",
      "in 2180 epoch, average loss: 1773.553125\n",
      "                , loss1: 837.633203125\n",
      "                , loss2: 0.3639516830444336\n",
      "                , weight: 2.018000000002258\n",
      "=================================\n",
      "in 2190 epoch, average loss: 1587.5458984375\n",
      "                , loss1: 836.51337890625\n",
      "                , loss2: 0.6718389511108398\n",
      "                , weight: 1.7980000000022578\n",
      "=================================\n",
      "in 2200 epoch, average loss: 1403.175390625\n",
      "                , loss1: 836.47978515625\n",
      "                , loss2: 0.4044963836669922\n",
      "                , weight: 1.5780000000022576\n",
      "=================================\n",
      "in 2210 epoch, average loss: 1219.40234375\n",
      "                , loss1: 836.7052734375\n",
      "                , loss2: 0.3442393779754639\n",
      "                , weight: 1.3580000000022574\n",
      "=================================\n",
      "in 2220 epoch, average loss: 1035.05810546875\n",
      "                , loss1: 836.4037109375\n",
      "                , loss2: 0.4017051696777344\n",
      "                , weight: 1.1380000000022572\n",
      "=================================\n",
      "in 2230 epoch, average loss: 850.97353515625\n",
      "                , loss1: 836.5837890625\n",
      "                , loss2: 0.18446974754333495\n",
      "                , weight: 0.918000000002257\n",
      "=================================\n",
      "in 2240 epoch, average loss: 668.008935546875\n",
      "                , loss1: 836.973828125\n",
      "                , loss2: 0.9412223815917968\n",
      "                , weight: 0.6980000000022568\n",
      "=================================\n",
      "in 2250 epoch, average loss: 483.64443359375\n",
      "                , loss1: 836.5302734375\n",
      "                , loss2: 0.9561346054077149\n",
      "                , weight: 0.4780000000022566\n",
      "=================================\n",
      "in 2260 epoch, average loss: 303.208740234375\n",
      "                , loss1: 837.05078125\n",
      "                , loss2: 0.8776608467102051\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2270 epoch, average loss: 251.3182373046875\n",
      "                , loss1: 836.666015625\n",
      "                , loss2: 0.31841633319854734\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2280 epoch, average loss: 250.979833984375\n",
      "                , loss1: 836.440625\n",
      "                , loss2: 0.04765617847442627\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2290 epoch, average loss: 251.2111572265625\n",
      "                , loss1: 836.718359375\n",
      "                , loss2: 0.19562890529632568\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2300 epoch, average loss: 251.5220947265625\n",
      "                , loss1: 837.1732421875\n",
      "                , loss2: 0.3700879573822021\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2310 epoch, average loss: 251.23037109375\n",
      "                , loss1: 836.87177734375\n",
      "                , loss2: 0.16883665323257446\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2320 epoch, average loss: 251.0349609375\n",
      "                , loss1: 836.643359375\n",
      "                , loss2: 0.041925960779190065\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2330 epoch, average loss: 251.4959228515625\n",
      "                , loss1: 836.973828125\n",
      "                , loss2: 0.4037624359130859\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2340 epoch, average loss: 251.89208984375\n",
      "                , loss1: 837.91015625\n",
      "                , loss2: 0.5190558433532715\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2350 epoch, average loss: 251.1760009765625\n",
      "                , loss1: 836.6716796875\n",
      "                , loss2: 0.1745044231414795\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2360 epoch, average loss: 251.0873291015625\n",
      "                , loss1: 836.57060546875\n",
      "                , loss2: 0.1161429762840271\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2370 epoch, average loss: 251.10947265625\n",
      "                , loss1: 836.6044921875\n",
      "                , loss2: 0.12811487913131714\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2380 epoch, average loss: 251.1425537109375\n",
      "                , loss1: 836.64130859375\n",
      "                , loss2: 0.15015411376953125\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2390 epoch, average loss: 251.0139404296875\n",
      "                , loss1: 836.28671875\n",
      "                , loss2: 0.12791335582733154\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2400 epoch, average loss: 251.109326171875\n",
      "                , loss1: 836.74658203125\n",
      "                , loss2: 0.08535673022270203\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2410 epoch, average loss: 251.1656494140625\n",
      "                , loss1: 836.61982421875\n",
      "                , loss2: 0.17967629432678223\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2420 epoch, average loss: 251.372119140625\n",
      "                , loss1: 836.70703125\n",
      "                , loss2: 0.36000006198883056\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2430 epoch, average loss: 251.144775390625\n",
      "                , loss1: 836.631640625\n",
      "                , loss2: 0.1552333950996399\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2440 epoch, average loss: 251.10556640625\n",
      "                , loss1: 836.3705078125\n",
      "                , loss2: 0.19439382553100587\n",
      "                , weight: 0.3\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m<\u001b[39m limit:\n\u001b[1;32m      8\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m----> 9\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     11\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[146], line 30\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode | 设置为训练模式\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[146], line 22\u001b[0m, in \u001b[0;36mTrainer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 22\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     24\u001b[0m         X \u001b[38;5;241m=\u001b[39m layer(X)\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/Hyper-Graph-Partition/examples_standardForm/../hgp/models.py:174\u001b[0m, in \u001b[0;36mHGNNP.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    ``X`` (``torch.Tensor``): Input vertex feature matrix. Size :math:`(N, C_{in})`.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    ``hg`` (``dhg.Hypergraph``): The hypergraph structure that contains :math:`N` vertices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 174\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/nn/convs/hypergraphs/hgnnp_conv.py:62\u001b[0m, in \u001b[0;36mHGNNPConv.forward\u001b[0;34m(self, X, hg)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward function.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    X (``torch.Tensor``): Input vertex feature matrix. Size :math:`(|\\mathcal{V}|, C_{in})`.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    hg (``dhg.Hypergraph``): The hypergraph structure that contains :math:`|\\mathcal{V}|` vertices.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta(X)\n\u001b[0;32m---> 62\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mhg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m     64\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(X)\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1657\u001b[0m, in \u001b[0;36mHypergraph.v2v\u001b[0;34m(self, X, aggr, drop_rate, v2e_aggr, v2e_weight, v2e_drop_rate, e_weight, e2v_aggr, e2v_weight, e2v_drop_rate)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     e2v_drop_rate \u001b[38;5;241m=\u001b[39m drop_rate\n\u001b[1;32m   1656\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(X, v2e_aggr, v2e_weight, e_weight, drop_rate\u001b[38;5;241m=\u001b[39mv2e_drop_rate)\n\u001b[0;32m-> 1657\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_aggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me2v_drop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1595\u001b[0m, in \u001b[0;36mHypergraph.e2v\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21me2v\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, aggr: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, e2v_weight: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, drop_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1586\u001b[0m ):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Message passing of ``hyperedges to vertices``. The combination of ``e2v_aggregation`` and ``e2v_update``.\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m        ``drop_rate`` (``float``): Dropout rate. Randomly dropout the connections in incidence matrix with probability ``drop_rate``. Default: ``0.0``.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1595\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me2v_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me2v_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1596\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2v_update(X)\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/dhg/structure/hypergraphs/hypergraph.py:1466\u001b[0m, in \u001b[0;36mHypergraph.e2v_aggregation\u001b[0;34m(self, X, aggr, e2v_weight, drop_rate)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1466\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_v_neg_1, X)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(40000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    elif hgnn_trainer.weight < limit:\n",
    "        hgnn_trainer.weight = limit\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1912., 1912.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
