{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # 添加项目根目录到路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import hgp\n",
    "from hgp.models import HGNNP,CHGNN\n",
    "from hgp.function import StraightThroughEstimator\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\t\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hgp.models import ParameterDict\n",
    "\n",
    "# fmt: off\n",
    "h_hyper_prmts = ParameterDict()\n",
    "l_hyper_prmts = ParameterDict()\n",
    "\n",
    "partitions = 2\n",
    "\n",
    "\n",
    "weight = 50\n",
    "limit = 0.3\n",
    "sub = 0.022\n",
    "\n",
    "lr = 4e-3\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 3724, \"out_channels\": 2048, \"use_bn\": False, \"drop_rate\": 0.8}\n",
    "h_hyper_prmts[\"convlayers1\"] = {\"in_channels\": 2048, \"out_channels\": 1024, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers12\"] = {\"in_channels\": 1024, \"out_channels\": 512, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers14\"] = {\"in_channels\": 512, \"out_channels\": 256, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers13\"] = {\"in_channels\": 256, \"out_channels\": 128, \"use_bn\": False, \"drop_rate\": 0}\n",
    "h_hyper_prmts[\"convlayers131\"] = {\"in_channels\": 128, \"out_channels\": 64, \"use_bn\": False, \"drop_rate\": 0}\n",
    "\n",
    "\n",
    "l_hyper_prmts[\"linerlayer1\"] = {\"in_channels\":list(h_hyper_prmts.values())[-1][\"out_channels\"], \"out_channels\":64, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer42\"] = {\"in_channels\":64, \"out_channels\":32, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer421\"] = {\"in_channels\":32, \"out_channels\":16, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer422\"] = {\"in_channels\":16, \"out_channels\":8, \"use_bn\":False, \"drop_rate\":0.05}\n",
    "l_hyper_prmts[\"linerlayer423\"] = {\"in_channels\":8, \"out_channels\":2, \"use_bn\":True, \"drop_rate\":0.05}\n",
    "\n",
    "hyper = {\n",
    "    \"h_hyper_prmts\": h_hyper_prmts,\n",
    "    \"l_hyper_prmts\":l_hyper_prmts,\n",
    "    \"init_features_dim\":list(h_hyper_prmts.values())[0][\"in_channels\"],\n",
    "    \"partitions\":partitions\n",
    "}\n",
    "\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bs_matrix(outs, hg, device, weight):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    对于超图的损失函数的标准.\n",
    "    \n",
    "    Args:\n",
    "        ``outs``(`torch.nn.Module`):  模型的输出. Size :math:`(N, nums_classes)`.   \n",
    "        ``hg``(`Hypergraph`):  超图对象.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    H = hg.H.to_dense().to(device)\n",
    "    outs = outs.to(device)\n",
    "\n",
    "    X_ = outs.t().unsqueeze(-1)\n",
    "    H_ = H.unsqueeze(0)\n",
    "    xweight = H.sum(dim=0)\n",
    "    mid = X_.mul(H_)\n",
    "    sum = (mid * (1 / xweight)).sum()\n",
    "    sub = (mid + (1 - H)).prod(dim=1).sum()\n",
    "    loss_1 = sum - sub\n",
    "\n",
    "    loss_2 = torch.var(torch.sum(outs, dim=0)).to(device)\n",
    "    loss = weight * loss_1 + loss_2\n",
    "\n",
    "    return loss, loss_1, loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用于训练的类Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    # fmt: off\n",
    "    r\"\"\"\n",
    "    用于承担训练的类.\n",
    "    ---\n",
    "    Args:\n",
    "        ``net``: (``torch.nn.Module``): 网络模型.  \n",
    "        ``X``: (``torch.Tensor``): 作为输入的顶点特征矩阵. Size :math:`(N, C_{in})`.  \n",
    "        ``hg``: (``dhg.Hypergraph``): 包含 :math:`N` 个顶点的超图结构.  \n",
    "    \"\"\"\n",
    "    # fmt: on\n",
    "    def __init__(self, net, X, hg, optimizer):\n",
    "        super().__init__()\n",
    "        self.X: torch.Tensor = X.to(DEVICE)\n",
    "        self.hg = hg.to(DEVICE)\n",
    "        self.de = self.hg.H.to_dense().sum(dim=0).to(\"cpu\").to(DEVICE)\n",
    "        self.optimizer: torch.optim.Optimizer = optimizer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(net.to(DEVICE))\n",
    "        self.weight = 200\n",
    "    def forward(self, X):\n",
    "        X = self.layers[0](X, self.hg)\n",
    "        for layer in self.layers[1:]:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "    def run(self, epoch):\n",
    "        self.train()  # train mode | 设置为训练模式\n",
    "        self.optimizer.zero_grad()\n",
    "        outs = self.forward(self.X)\n",
    "        loss, loss_1, loss_2 = loss_bs_matrix(outs, self.hg, device=DEVICE,weight=self.weight)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), loss_1.item(), loss_2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7523, 3824)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hgp.utils\n",
    "G = hgp.utils.from_pickle_to_hypergraph(\"../data/pubmed\")\n",
    "edges, _ = G.e\n",
    "G.num_e,G.num_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): HGNNP(\n",
       "    (layers): ModuleList(\n",
       "      (0): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (4): HGNNPConv(\n",
       "        (act): ReLU(inplace=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (theta): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.05, inplace=False)\n",
       "  (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.05, inplace=False)\n",
       "  (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.05, inplace=False)\n",
       "  (9): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (13): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.05, inplace=False)\n",
       "  (16): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (17): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(size=(G.num_v, hyper[\"init_features_dim\"]))\n",
    "# X = torch.eye(hyper[\"init_features_dim\"])\n",
    "net = HGNNP(hyper[\"h_hyper_prmts\"]).to(DEVICE)\n",
    "hgnn_trainer = Trainer(net=net, X=X, hg=G, optimizer=None).to(DEVICE)\n",
    "for (k,v) in hyper[\"l_hyper_prmts\"].items():\n",
    "    hgnn_trainer.layers.append(nn.BatchNorm1d(num_features=v[\"in_channels\"]).to(DEVICE)) if v[\"use_bn\"] else None\n",
    "    hgnn_trainer.layers.append(nn.ReLU().to(DEVICE))\n",
    "    if v[\"drop_rate\"] > 0:\n",
    "        hgnn_trainer.layers.append(nn.Dropout(v[\"drop_rate\"]))\n",
    "    hgnn_trainer.layers.append(nn.Linear(in_features=v[\"in_channels\"],out_features=v[\"out_channels\"],device=DEVICE))\n",
    "hgnn_trainer.layers.append(nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgnn_trainer.layers\n",
    "# for n,p in hgnn_trainer.named_parameters():\n",
    "#     print(n,p)\n",
    "hgnn_trainer.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 0 epoch, average loss: 33499.8625\n",
      "                , loss1: 537.16572265625\n",
      "                , loss2: 6653.39296875\n",
      "                , weight: 49.978\n",
      "=================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 10 epoch, average loss: 279048.7\n",
      "                , loss1: 5388.33203125\n",
      "                , loss2: 10401.88828125\n",
      "                , weight: 49.75800000000002\n",
      "=================================\n",
      "in 20 epoch, average loss: 255402.95\n",
      "                , loss1: 5142.054296875\n",
      "                , loss2: 161.22298583984374\n",
      "                , weight: 49.53800000000003\n",
      "=================================\n",
      "in 30 epoch, average loss: 245164.325\n",
      "                , loss1: 4959.337109375\n",
      "                , loss2: 86.66370239257813\n",
      "                , weight: 49.31800000000005\n",
      "=================================\n",
      "in 40 epoch, average loss: 235634.5\n",
      "                , loss1: 4789.228515625\n",
      "                , loss2: 15.2060546875\n",
      "                , weight: 49.09800000000006\n",
      "=================================\n",
      "in 50 epoch, average loss: 222282.2\n",
      "                , loss1: 4538.0734375\n",
      "                , loss2: 15.658999633789062\n",
      "                , weight: 48.87800000000008\n",
      "=================================\n",
      "in 60 epoch, average loss: 206987.4375\n",
      "                , loss1: 4241.9890625\n",
      "                , loss2: 155.0848388671875\n",
      "                , weight: 48.658000000000094\n",
      "=================================\n",
      "in 70 epoch, average loss: 189575.35\n",
      "                , loss1: 3902.10625\n",
      "                , loss2: 173.28931884765626\n",
      "                , weight: 48.43800000000011\n",
      "=================================\n",
      "in 80 epoch, average loss: 173137.825\n",
      "                , loss1: 3574.364453125\n",
      "                , loss2: 429.523486328125\n",
      "                , weight: 48.218000000000124\n",
      "=================================\n",
      "in 90 epoch, average loss: 154982.0875\n",
      "                , loss1: 3220.76484375\n",
      "                , loss2: 64.80406494140625\n",
      "                , weight: 47.99800000000014\n",
      "=================================\n",
      "in 100 epoch, average loss: 128793.575\n",
      "                , loss1: 2685.95390625\n",
      "                , loss2: 187.993505859375\n",
      "                , weight: 47.778000000000155\n",
      "=================================\n",
      "in 110 epoch, average loss: 104685.8625\n",
      "                , loss1: 2177.665625\n",
      "                , loss2: 896.26787109375\n",
      "                , weight: 47.55800000000017\n",
      "=================================\n",
      "in 120 epoch, average loss: 83881.21875\n",
      "                , loss1: 1761.85859375\n",
      "                , loss2: 296.7588623046875\n",
      "                , weight: 47.338000000000186\n",
      "=================================\n",
      "in 130 epoch, average loss: 68434.775\n",
      "                , loss1: 1447.593359375\n",
      "                , loss2: 79.5826416015625\n",
      "                , weight: 47.1180000000002\n",
      "=================================\n",
      "in 140 epoch, average loss: 60763.2375\n",
      "                , loss1: 1290.22001953125\n",
      "                , loss2: 124.4999755859375\n",
      "                , weight: 46.898000000000216\n",
      "=================================\n",
      "in 150 epoch, average loss: 55529.18125\n",
      "                , loss1: 1185.4431640625\n",
      "                , loss2: 75.45484619140625\n",
      "                , weight: 46.67800000000023\n",
      "=================================\n",
      "in 160 epoch, average loss: 51522.68125\n",
      "                , loss1: 1105.33662109375\n",
      "                , loss2: 60.94393310546875\n",
      "                , weight: 46.45800000000025\n",
      "=================================\n",
      "in 170 epoch, average loss: 49124.640625\n",
      "                , loss1: 1057.71328125\n",
      "                , loss2: 112.6398193359375\n",
      "                , weight: 46.23800000000026\n",
      "=================================\n",
      "in 180 epoch, average loss: 47180.83125\n",
      "                , loss1: 1021.9421875\n",
      "                , loss2: 51.454437255859375\n",
      "                , weight: 46.01800000000028\n",
      "=================================\n",
      "in 190 epoch, average loss: 45678.70625\n",
      "                , loss1: 994.6466796875\n",
      "                , loss2: 26.971124267578126\n",
      "                , weight: 45.79800000000029\n",
      "=================================\n",
      "in 200 epoch, average loss: 44819.21875\n",
      "                , loss1: 980.28154296875\n",
      "                , loss2: 42.58614501953125\n",
      "                , weight: 45.57800000000031\n",
      "=================================\n",
      "in 210 epoch, average loss: 43676.459375\n",
      "                , loss1: 960.14892578125\n",
      "                , loss2: 30.71721496582031\n",
      "                , weight: 45.358000000000324\n",
      "=================================\n",
      "in 220 epoch, average loss: 42981.490625\n",
      "                , loss1: 948.362109375\n",
      "                , loss2: 80.34029541015624\n",
      "                , weight: 45.13800000000034\n",
      "=================================\n",
      "in 230 epoch, average loss: 42317.425\n",
      "                , loss1: 938.3419921875\n",
      "                , loss2: 75.91705322265625\n",
      "                , weight: 44.918000000000355\n",
      "=================================\n",
      "in 240 epoch, average loss: 41609.69375\n",
      "                , loss1: 928.0080078125\n",
      "                , loss2: 37.727987670898436\n",
      "                , weight: 44.69800000000037\n",
      "=================================\n",
      "in 250 epoch, average loss: 41206.58125\n",
      "                , loss1: 923.532421875\n",
      "                , loss2: 37.94801025390625\n",
      "                , weight: 44.478000000000385\n",
      "=================================\n",
      "in 260 epoch, average loss: 40551.99375\n",
      "                , loss1: 914.0044921875\n",
      "                , loss2: 9.357866668701172\n",
      "                , weight: 44.2580000000004\n",
      "=================================\n",
      "in 270 epoch, average loss: 40006.246875\n",
      "                , loss1: 905.7560546875\n",
      "                , loss2: 28.832861328125\n",
      "                , weight: 44.038000000000416\n",
      "=================================\n",
      "in 280 epoch, average loss: 39599.278125\n",
      "                , loss1: 900.3888671875\n",
      "                , loss2: 56.8666259765625\n",
      "                , weight: 43.81800000000043\n",
      "=================================\n",
      "in 290 epoch, average loss: 39279.865625\n",
      "                , loss1: 897.5712890625\n",
      "                , loss2: 58.570404052734375\n",
      "                , weight: 43.59800000000045\n",
      "=================================\n",
      "in 300 epoch, average loss: 38887.89375\n",
      "                , loss1: 893.485546875\n",
      "                , loss2: 41.840371704101564\n",
      "                , weight: 43.37800000000046\n",
      "=================================\n",
      "in 310 epoch, average loss: 38419.8\n",
      "                , loss1: 887.9791015625\n",
      "                , loss2: 8.414022064208984\n",
      "                , weight: 43.15800000000048\n",
      "=================================\n",
      "in 320 epoch, average loss: 38149.365625\n",
      "                , loss1: 886.159375\n",
      "                , loss2: 11.70418930053711\n",
      "                , weight: 42.93800000000049\n",
      "=================================\n",
      "in 330 epoch, average loss: 37736.38125\n",
      "                , loss1: 880.883984375\n",
      "                , loss2: 19.52896728515625\n",
      "                , weight: 42.71800000000051\n",
      "=================================\n",
      "in 340 epoch, average loss: 37471.1875\n",
      "                , loss1: 879.4033203125\n",
      "                , loss2: 11.104608154296875\n",
      "                , weight: 42.49800000000052\n",
      "=================================\n",
      "in 350 epoch, average loss: 37110.81875\n",
      "                , loss1: 875.2712890625\n",
      "                , loss2: 19.40377197265625\n",
      "                , weight: 42.27800000000054\n",
      "=================================\n",
      "in 360 epoch, average loss: 36859.8125\n",
      "                , loss1: 873.78671875\n",
      "                , loss2: 23.52808074951172\n",
      "                , weight: 42.058000000000554\n",
      "=================================\n",
      "in 370 epoch, average loss: 36547.940625\n",
      "                , loss1: 871.3091796875\n",
      "                , loss2: 7.720506286621093\n",
      "                , weight: 41.83800000000057\n",
      "=================================\n",
      "in 380 epoch, average loss: 36327.74375\n",
      "                , loss1: 870.5162109375\n",
      "                , loss2: 12.374605560302735\n",
      "                , weight: 41.618000000000585\n",
      "=================================\n",
      "in 390 epoch, average loss: 36042.734375\n",
      "                , loss1: 868.0353515625\n",
      "                , loss2: 21.762484741210937\n",
      "                , weight: 41.3980000000006\n",
      "=================================\n",
      "in 400 epoch, average loss: 35800.265625\n",
      "                , loss1: 867.19736328125\n",
      "                , loss2: 4.913211441040039\n",
      "                , weight: 41.178000000000615\n",
      "=================================\n",
      "in 410 epoch, average loss: 35477.009375\n",
      "                , loss1: 863.9015625\n",
      "                , loss2: 7.776798248291016\n",
      "                , weight: 40.95800000000063\n",
      "=================================\n",
      "in 420 epoch, average loss: 35191.1375\n",
      "                , loss1: 861.4236328125\n",
      "                , loss2: 13.133126831054687\n",
      "                , weight: 40.738000000000646\n",
      "=================================\n",
      "in 430 epoch, average loss: 34921.625\n",
      "                , loss1: 859.6599609375\n",
      "                , loss2: 4.735939025878906\n",
      "                , weight: 40.51800000000066\n",
      "=================================\n",
      "in 440 epoch, average loss: 34718.678125\n",
      "                , loss1: 859.3654296875\n",
      "                , loss2: 2.94789981842041\n",
      "                , weight: 40.29800000000068\n",
      "=================================\n",
      "in 450 epoch, average loss: 34456.159375\n",
      "                , loss1: 857.5251953125\n",
      "                , loss2: 3.3410430908203126\n",
      "                , weight: 40.07800000000069\n",
      "=================================\n",
      "in 460 epoch, average loss: 34287.259375\n",
      "                , loss1: 857.853515625\n",
      "                , loss2: 10.044860076904296\n",
      "                , weight: 39.85800000000071\n",
      "=================================\n",
      "in 470 epoch, average loss: 34061.975\n",
      "                , loss1: 856.952734375\n",
      "                , loss2: 9.283671569824218\n",
      "                , weight: 39.63800000000072\n",
      "=================================\n",
      "in 480 epoch, average loss: 33870.59375\n",
      "                , loss1: 856.97314453125\n",
      "                , loss2: 5.556313323974609\n",
      "                , weight: 39.41800000000074\n",
      "=================================\n",
      "in 490 epoch, average loss: 33603.65\n",
      "                , loss1: 854.5373046875\n",
      "                , loss2: 22.943702697753906\n",
      "                , weight: 39.198000000000754\n",
      "=================================\n",
      "in 500 epoch, average loss: 33473.078125\n",
      "                , loss1: 855.2951171875\n",
      "                , loss2: 50.76480407714844\n",
      "                , weight: 38.97800000000077\n",
      "=================================\n",
      "in 510 epoch, average loss: 34025.296875\n",
      "                , loss1: 865.401171875\n",
      "                , loss2: 398.722900390625\n",
      "                , weight: 38.758000000000784\n",
      "=================================\n",
      "in 520 epoch, average loss: 33649.646875\n",
      "                , loss1: 865.99375\n",
      "                , loss2: 190.183154296875\n",
      "                , weight: 38.5380000000008\n",
      "=================================\n",
      "in 530 epoch, average loss: 33219.340625\n",
      "                , loss1: 863.401171875\n",
      "                , loss2: 49.894570922851564\n",
      "                , weight: 38.318000000000815\n",
      "=================================\n",
      "in 540 epoch, average loss: 32804.778125\n",
      "                , loss1: 858.20068359375\n",
      "                , loss2: 24.035308837890625\n",
      "                , weight: 38.09800000000083\n",
      "=================================\n",
      "in 550 epoch, average loss: 32465.675\n",
      "                , loss1: 854.70361328125\n",
      "                , loss2: 6.489286804199219\n",
      "                , weight: 37.878000000000846\n",
      "=================================\n",
      "in 560 epoch, average loss: 32199.296875\n",
      "                , loss1: 852.7421875\n",
      "                , loss2: 2.2774782180786133\n",
      "                , weight: 37.65800000000086\n",
      "=================================\n",
      "in 570 epoch, average loss: 31948.73125\n",
      "                , loss1: 851.059765625\n",
      "                , loss2: 2.521990966796875\n",
      "                , weight: 37.438000000000876\n",
      "=================================\n",
      "in 580 epoch, average loss: 31738.359375\n",
      "                , loss1: 850.4189453125\n",
      "                , loss2: 3.2755764007568358\n",
      "                , weight: 37.21800000000089\n",
      "=================================\n",
      "in 590 epoch, average loss: 31526.175\n",
      "                , loss1: 849.772265625\n",
      "                , loss2: 2.1509496688842775\n",
      "                , weight: 36.99800000000091\n",
      "=================================\n",
      "in 600 epoch, average loss: 31300.415625\n",
      "                , loss1: 848.66982421875\n",
      "                , loss2: 4.011995697021485\n",
      "                , weight: 36.77800000000092\n",
      "=================================\n",
      "in 610 epoch, average loss: 31120.959375\n",
      "                , loss1: 848.8947265625\n",
      "                , loss2: 3.05181884765625\n",
      "                , weight: 36.55800000000094\n",
      "=================================\n",
      "in 620 epoch, average loss: 30947.234375\n",
      "                , loss1: 849.2732421875\n",
      "                , loss2: 2.2533634185791014\n",
      "                , weight: 36.33800000000095\n",
      "=================================\n",
      "in 630 epoch, average loss: 30724.1875\n",
      "                , loss1: 848.3169921875\n",
      "                , loss2: 0.6787904262542724\n",
      "                , weight: 36.11800000000097\n",
      "=================================\n",
      "in 640 epoch, average loss: 30485.125\n",
      "                , loss1: 846.7958984375\n",
      "                , loss2: 2.964707946777344\n",
      "                , weight: 35.898000000000984\n",
      "=================================\n",
      "in 650 epoch, average loss: 30283.55625\n",
      "                , loss1: 846.3828125\n",
      "                , loss2: 2.506115531921387\n",
      "                , weight: 35.678000000001\n",
      "=================================\n",
      "in 660 epoch, average loss: 30092.171875\n",
      "                , loss1: 846.1744140625\n",
      "                , loss2: 4.710988235473633\n",
      "                , weight: 35.458000000001014\n",
      "=================================\n",
      "in 670 epoch, average loss: 29929.0125\n",
      "                , loss1: 846.8158203125\n",
      "                , loss2: 5.147629165649414\n",
      "                , weight: 35.23800000000103\n",
      "=================================\n",
      "in 680 epoch, average loss: 29671.71875\n",
      "                , loss1: 844.8689453125\n",
      "                , loss2: 2.472040367126465\n",
      "                , weight: 35.018000000001045\n",
      "=================================\n",
      "in 690 epoch, average loss: 29474.94375\n",
      "                , loss1: 844.6177734375\n",
      "                , loss2: 0.3017071485519409\n",
      "                , weight: 34.79800000000106\n",
      "=================================\n",
      "in 700 epoch, average loss: 29297.378125\n",
      "                , loss1: 844.8330078125\n",
      "                , loss2: 1.086295986175537\n",
      "                , weight: 34.578000000001076\n",
      "=================================\n",
      "in 710 epoch, average loss: 29092.528125\n",
      "                , loss1: 844.17158203125\n",
      "                , loss2: 4.895271301269531\n",
      "                , weight: 34.35800000000109\n",
      "=================================\n",
      "in 720 epoch, average loss: 28884.159375\n",
      "                , loss1: 843.515234375\n",
      "                , loss2: 4.696105575561523\n",
      "                , weight: 34.13800000000111\n",
      "=================================\n",
      "in 730 epoch, average loss: 28691.103125\n",
      "                , loss1: 843.41015625\n",
      "                , loss2: 0.8406100273132324\n",
      "                , weight: 33.91800000000112\n",
      "=================================\n",
      "in 740 epoch, average loss: 28492.85625\n",
      "                , loss1: 843.012890625\n",
      "                , loss2: 1.5440277099609374\n",
      "                , weight: 33.69800000000114\n",
      "=================================\n",
      "in 750 epoch, average loss: 28303.140625\n",
      "                , loss1: 842.8595703125\n",
      "                , loss2: 2.440670204162598\n",
      "                , weight: 33.47800000000115\n",
      "=================================\n",
      "in 760 epoch, average loss: 28116.0625\n",
      "                , loss1: 842.80234375\n",
      "                , loss2: 2.629034233093262\n",
      "                , weight: 33.25800000000117\n",
      "=================================\n",
      "in 770 epoch, average loss: 27895.934375\n",
      "                , loss1: 841.77890625\n",
      "                , loss2: 1.9298919677734374\n",
      "                , weight: 33.03800000000118\n",
      "=================================\n",
      "in 780 epoch, average loss: 27710.334375\n",
      "                , loss1: 841.7423828125\n",
      "                , loss2: 2.6622238159179688\n",
      "                , weight: 32.8180000000012\n",
      "=================================\n",
      "in 790 epoch, average loss: 27514.415625\n",
      "                , loss1: 841.4732421875\n",
      "                , loss2: 0.7391254425048828\n",
      "                , weight: 32.598000000001214\n",
      "=================================\n",
      "in 800 epoch, average loss: 27306.90625\n",
      "                , loss1: 840.7748046875\n",
      "                , loss2: 1.080186367034912\n",
      "                , weight: 32.37800000000123\n",
      "=================================\n",
      "in 810 epoch, average loss: 27133.778125\n",
      "                , loss1: 841.081640625\n",
      "                , loss2: 3.0412033081054686\n",
      "                , weight: 32.158000000001245\n",
      "=================================\n",
      "in 820 epoch, average loss: 26951.6\n",
      "                , loss1: 841.1134765625\n",
      "                , loss2: 4.836971664428711\n",
      "                , weight: 31.93800000000126\n",
      "=================================\n",
      "in 830 epoch, average loss: 26740.36875\n",
      "                , loss1: 840.3494140625\n",
      "                , loss2: 2.9245458602905274\n",
      "                , weight: 31.718000000001275\n",
      "=================================\n",
      "in 840 epoch, average loss: 26541.528125\n",
      "                , loss1: 839.9306640625\n",
      "                , loss2: 2.237508010864258\n",
      "                , weight: 31.49800000000129\n",
      "=================================\n",
      "in 850 epoch, average loss: 26338.109375\n",
      "                , loss1: 839.369140625\n",
      "                , loss2: 1.2197134971618653\n",
      "                , weight: 31.278000000001306\n",
      "=================================\n",
      "in 860 epoch, average loss: 26169.334375\n",
      "                , loss1: 839.83447265625\n",
      "                , loss2: 2.611470413208008\n",
      "                , weight: 31.05800000000132\n",
      "=================================\n",
      "in 870 epoch, average loss: 25992.0953125\n",
      "                , loss1: 840.05478515625\n",
      "                , loss2: 3.3318401336669923\n",
      "                , weight: 30.838000000001337\n",
      "=================================\n",
      "in 880 epoch, average loss: 25785.915625\n",
      "                , loss1: 839.290625\n",
      "                , loss2: 5.381843566894531\n",
      "                , weight: 30.618000000001352\n",
      "=================================\n",
      "in 890 epoch, average loss: 25631.9109375\n",
      "                , loss1: 840.1080078125\n",
      "                , loss2: 11.144361877441407\n",
      "                , weight: 30.398000000001367\n",
      "=================================\n",
      "in 900 epoch, average loss: 25455.1328125\n",
      "                , loss1: 840.64267578125\n",
      "                , loss2: 2.9204427719116213\n",
      "                , weight: 30.178000000001383\n",
      "=================================\n",
      "in 910 epoch, average loss: 25238.21875\n",
      "                , loss1: 839.6107421875\n",
      "                , loss2: 2.0414384841918944\n",
      "                , weight: 29.958000000001398\n",
      "=================================\n",
      "in 920 epoch, average loss: 25048.471875\n",
      "                , loss1: 839.43037109375\n",
      "                , loss2: 2.414206123352051\n",
      "                , weight: 29.738000000001414\n",
      "=================================\n",
      "in 930 epoch, average loss: 24828.259375\n",
      "                , loss1: 838.2740234375\n",
      "                , loss2: 1.1268879890441894\n",
      "                , weight: 29.51800000000143\n",
      "=================================\n",
      "in 940 epoch, average loss: 24685.103125\n",
      "                , loss1: 839.528515625\n",
      "                , loss2: 5.444466018676758\n",
      "                , weight: 29.298000000001444\n",
      "=================================\n",
      "in 950 epoch, average loss: 24457.578125\n",
      "                , loss1: 838.037890625\n",
      "                , loss2: 6.149057769775391\n",
      "                , weight: 29.07800000000146\n",
      "=================================\n",
      "in 960 epoch, average loss: 24269.4828125\n",
      "                , loss1: 838.0498046875\n",
      "                , loss2: 2.0641178131103515\n",
      "                , weight: 28.858000000001475\n",
      "=================================\n",
      "in 970 epoch, average loss: 24070.2921875\n",
      "                , loss1: 837.547265625\n",
      "                , loss2: 1.6652952194213868\n",
      "                , weight: 28.63800000000149\n",
      "=================================\n",
      "in 980 epoch, average loss: 23865.128125\n",
      "                , loss1: 836.825390625\n",
      "                , loss2: 1.348433494567871\n",
      "                , weight: 28.418000000001506\n",
      "=================================\n",
      "in 990 epoch, average loss: 23708.7625\n",
      "                , loss1: 837.8076171875\n",
      "                , loss2: 1.3517568588256836\n",
      "                , weight: 28.19800000000152\n",
      "=================================\n",
      "in 1000 epoch, average loss: 23524.01875\n",
      "                , loss1: 837.7068359375\n",
      "                , loss2: 3.655043029785156\n",
      "                , weight: 27.978000000001536\n",
      "=================================\n",
      "in 1010 epoch, average loss: 23330.6296875\n",
      "                , loss1: 837.4236328125\n",
      "                , loss2: 2.5110645294189453\n",
      "                , weight: 27.75800000000155\n",
      "=================================\n",
      "in 1020 epoch, average loss: 23130.9109375\n",
      "                , loss1: 836.926171875\n",
      "                , loss2: 0.7686193466186524\n",
      "                , weight: 27.538000000001567\n",
      "=================================\n",
      "in 1030 epoch, average loss: 22969.38125\n",
      "                , loss1: 837.71640625\n",
      "                , loss2: 1.734351348876953\n",
      "                , weight: 27.318000000001582\n",
      "=================================\n",
      "in 1040 epoch, average loss: 22769.1953125\n",
      "                , loss1: 837.1482421875\n",
      "                , loss2: 1.2545158386230468\n",
      "                , weight: 27.098000000001598\n",
      "=================================\n",
      "in 1050 epoch, average loss: 22577.1234375\n",
      "                , loss1: 836.86318359375\n",
      "                , loss2: 1.0671825408935547\n",
      "                , weight: 26.878000000001613\n",
      "=================================\n",
      "in 1060 epoch, average loss: 22384.0578125\n",
      "                , loss1: 836.48671875\n",
      "                , loss2: 2.171835517883301\n",
      "                , weight: 26.65800000000163\n",
      "=================================\n",
      "in 1070 epoch, average loss: 22200.08125\n",
      "                , loss1: 836.5361328125\n",
      "                , loss2: 0.9124276161193847\n",
      "                , weight: 26.438000000001644\n",
      "=================================\n",
      "in 1080 epoch, average loss: 22008.271875\n",
      "                , loss1: 836.2103515625\n",
      "                , loss2: 1.7311120986938477\n",
      "                , weight: 26.21800000000166\n",
      "=================================\n",
      "in 1090 epoch, average loss: 21840.5546875\n",
      "                , loss1: 836.8154296875\n",
      "                , loss2: 2.1905290603637697\n",
      "                , weight: 25.998000000001674\n",
      "=================================\n",
      "in 1100 epoch, average loss: 21644.8953125\n",
      "                , loss1: 836.40068359375\n",
      "                , loss2: 1.326188087463379\n",
      "                , weight: 25.77800000000169\n",
      "=================================\n",
      "in 1110 epoch, average loss: 21457.7296875\n",
      "                , loss1: 836.3150390625\n",
      "                , loss2: 0.38856024742126466\n",
      "                , weight: 25.558000000001705\n",
      "=================================\n",
      "in 1120 epoch, average loss: 21281.6875\n",
      "                , loss1: 836.57255859375\n",
      "                , loss2: 1.8022659301757813\n",
      "                , weight: 25.33800000000172\n",
      "=================================\n",
      "in 1130 epoch, average loss: 21100.1171875\n",
      "                , loss1: 836.6466796875\n",
      "                , loss2: 2.389950180053711\n",
      "                , weight: 25.118000000001736\n",
      "=================================\n",
      "in 1140 epoch, average loss: 20899.809375\n",
      "                , loss1: 836.039453125\n",
      "                , loss2: 1.3589786529541015\n",
      "                , weight: 24.89800000000175\n",
      "=================================\n",
      "in 1150 epoch, average loss: 20696.6\n",
      "                , loss1: 835.2154296875\n",
      "                , loss2: 2.4391212463378906\n",
      "                , weight: 24.678000000001767\n",
      "=================================\n",
      "in 1160 epoch, average loss: 20520.0453125\n",
      "                , loss1: 835.54970703125\n",
      "                , loss2: 1.4464980125427247\n",
      "                , weight: 24.458000000001782\n",
      "=================================\n",
      "in 1170 epoch, average loss: 20320.521875\n",
      "                , loss1: 834.9263671875\n",
      "                , loss2: 0.9221553802490234\n",
      "                , weight: 24.238000000001797\n",
      "=================================\n",
      "in 1180 epoch, average loss: 20131.2671875\n",
      "                , loss1: 834.726171875\n",
      "                , loss2: 0.17103453874588012\n",
      "                , weight: 24.018000000001813\n",
      "=================================\n",
      "in 1190 epoch, average loss: 19958.0875\n",
      "                , loss1: 835.1306640625\n",
      "                , loss2: 0.9709028244018555\n",
      "                , weight: 23.798000000001828\n",
      "=================================\n",
      "in 1200 epoch, average loss: 19775.515625\n",
      "                , loss1: 835.1365234375\n",
      "                , loss2: 2.005750846862793\n",
      "                , weight: 23.578000000001843\n",
      "=================================\n",
      "in 1210 epoch, average loss: 19601.878125\n",
      "                , loss1: 835.5783203125\n",
      "                , loss2: 1.7136581420898438\n",
      "                , weight: 23.35800000000186\n",
      "=================================\n",
      "in 1220 epoch, average loss: 19392.3359375\n",
      "                , loss1: 834.5064453125\n",
      "                , loss2: 0.9189027786254883\n",
      "                , weight: 23.138000000001874\n",
      "=================================\n",
      "in 1230 epoch, average loss: 19214.9328125\n",
      "                , loss1: 834.71875\n",
      "                , loss2: 2.2119014739990233\n",
      "                , weight: 22.91800000000189\n",
      "=================================\n",
      "in 1240 epoch, average loss: 19020.5578125\n",
      "                , loss1: 834.2701171875\n",
      "                , loss2: 1.668718147277832\n",
      "                , weight: 22.698000000001905\n",
      "=================================\n",
      "in 1250 epoch, average loss: 18842.7796875\n",
      "                , loss1: 834.58759765625\n",
      "                , loss2: 0.3019396781921387\n",
      "                , weight: 22.47800000000192\n",
      "=================================\n",
      "in 1260 epoch, average loss: 18669.3578125\n",
      "                , loss1: 835.02041015625\n",
      "                , loss2: 0.7802140712738037\n",
      "                , weight: 22.258000000001935\n",
      "=================================\n",
      "in 1270 epoch, average loss: 18479.9640625\n",
      "                , loss1: 834.74833984375\n",
      "                , loss2: 1.1196239471435547\n",
      "                , weight: 22.03800000000195\n",
      "=================================\n",
      "in 1280 epoch, average loss: 18306.7015625\n",
      "                , loss1: 835.20703125\n",
      "                , loss2: 1.432475471496582\n",
      "                , weight: 21.818000000001966\n",
      "=================================\n",
      "in 1290 epoch, average loss: 18108.0234375\n",
      "                , loss1: 834.52451171875\n",
      "                , loss2: 1.3165614128112793\n",
      "                , weight: 21.59800000000198\n",
      "=================================\n",
      "in 1300 epoch, average loss: 17927.8421875\n",
      "                , loss1: 834.70458984375\n",
      "                , loss2: 0.900173282623291\n",
      "                , weight: 21.378000000001997\n",
      "=================================\n",
      "in 1310 epoch, average loss: 17731.00625\n",
      "                , loss1: 834.0837890625\n",
      "                , loss2: 0.8849038124084473\n",
      "                , weight: 21.158000000002012\n",
      "=================================\n",
      "in 1320 epoch, average loss: 17560.4140625\n",
      "                , loss1: 834.5734375\n",
      "                , loss2: 3.515625\n",
      "                , weight: 20.938000000002027\n",
      "=================================\n",
      "in 1330 epoch, average loss: 17369.91875\n",
      "                , loss1: 834.39013671875\n",
      "                , loss2: 0.42449097633361815\n",
      "                , weight: 20.718000000002043\n",
      "=================================\n",
      "in 1340 epoch, average loss: 17174.79375\n",
      "                , loss1: 833.7853515625\n",
      "                , loss2: 1.315132236480713\n",
      "                , weight: 20.498000000002058\n",
      "=================================\n",
      "in 1350 epoch, average loss: 16993.9296875\n",
      "                , loss1: 833.937109375\n",
      "                , loss2: 0.7890478134155273\n",
      "                , weight: 20.278000000002073\n",
      "=================================\n",
      "in 1360 epoch, average loss: 16809.721875\n",
      "                , loss1: 833.90390625\n",
      "                , loss2: 0.724608325958252\n",
      "                , weight: 20.05800000000209\n",
      "=================================\n",
      "in 1370 epoch, average loss: 16634.228125\n",
      "                , loss1: 834.2833984375\n",
      "                , loss2: 1.1515104293823242\n",
      "                , weight: 19.838000000002104\n",
      "=================================\n",
      "in 1380 epoch, average loss: 16436.6359375\n",
      "                , loss1: 833.5767578125\n",
      "                , loss2: 1.0021310806274415\n",
      "                , weight: 19.61800000000212\n",
      "=================================\n",
      "in 1390 epoch, average loss: 16263.334375\n",
      "                , loss1: 834.0958984375\n",
      "                , loss2: 0.9902956008911132\n",
      "                , weight: 19.398000000002135\n",
      "=================================\n",
      "in 1400 epoch, average loss: 16076.8859375\n",
      "                , loss1: 833.92734375\n",
      "                , loss2: 1.2661425590515136\n",
      "                , weight: 19.17800000000215\n",
      "=================================\n",
      "in 1410 epoch, average loss: 15884.059375\n",
      "                , loss1: 833.44287109375\n",
      "                , loss2: 1.1224662780761718\n",
      "                , weight: 18.958000000002166\n",
      "=================================\n",
      "in 1420 epoch, average loss: 15699.053125\n",
      "                , loss1: 833.37158203125\n",
      "                , loss2: 0.8438106536865234\n",
      "                , weight: 18.73800000000218\n",
      "=================================\n",
      "in 1430 epoch, average loss: 15521.8046875\n",
      "                , loss1: 833.71982421875\n",
      "                , loss2: 0.39817032814025877\n",
      "                , weight: 18.518000000002196\n",
      "=================================\n",
      "in 1440 epoch, average loss: 15341.9171875\n",
      "                , loss1: 833.9025390625\n",
      "                , loss2: 0.5850790977478028\n",
      "                , weight: 18.29800000000221\n",
      "=================================\n",
      "in 1450 epoch, average loss: 15166.1671875\n",
      "                , loss1: 834.255078125\n",
      "                , loss2: 1.9258655548095702\n",
      "                , weight: 18.078000000002227\n",
      "=================================\n",
      "in 1460 epoch, average loss: 14974.546875\n",
      "                , loss1: 833.8015625\n",
      "                , loss2: 1.9813453674316406\n",
      "                , weight: 17.858000000002242\n",
      "=================================\n",
      "in 1470 epoch, average loss: 14788.11875\n",
      "                , loss1: 833.721875\n",
      "                , loss2: 0.4085371494293213\n",
      "                , weight: 17.638000000002258\n",
      "=================================\n",
      "in 1480 epoch, average loss: 14603.0\n",
      "                , loss1: 833.63427734375\n",
      "                , loss2: 0.21094150543212892\n",
      "                , weight: 17.418000000002273\n",
      "=================================\n",
      "in 1490 epoch, average loss: 14427.49375\n",
      "                , loss1: 834.0798828125\n",
      "                , loss2: 0.3935982704162598\n",
      "                , weight: 17.19800000000229\n",
      "=================================\n",
      "in 1500 epoch, average loss: 14238.43125\n",
      "                , loss1: 833.7478515625\n",
      "                , loss2: 0.5497353076934814\n",
      "                , weight: 16.978000000002304\n",
      "=================================\n",
      "in 1510 epoch, average loss: 14051.9546875\n",
      "                , loss1: 833.56875\n",
      "                , loss2: 0.49106926918029786\n",
      "                , weight: 16.75800000000232\n",
      "=================================\n",
      "in 1520 epoch, average loss: 13864.3828125\n",
      "                , loss1: 833.3138671875\n",
      "                , loss2: 0.5306591033935547\n",
      "                , weight: 16.538000000002334\n",
      "=================================\n",
      "in 1530 epoch, average loss: 13677.496875\n",
      "                , loss1: 833.1166015625\n",
      "                , loss2: 0.2294783115386963\n",
      "                , weight: 16.31800000000235\n",
      "=================================\n",
      "in 1540 epoch, average loss: 13497.790625\n",
      "                , loss1: 833.31943359375\n",
      "                , loss2: 0.5312866687774658\n",
      "                , weight: 16.098000000002365\n",
      "=================================\n",
      "in 1550 epoch, average loss: 13313.6015625\n",
      "                , loss1: 833.27529296875\n",
      "                , loss2: 0.3317265033721924\n",
      "                , weight: 15.87800000000237\n",
      "=================================\n",
      "in 1560 epoch, average loss: 13123.9\n",
      "                , loss1: 832.8712890625\n",
      "                , loss2: 0.33944485187530515\n",
      "                , weight: 15.658000000002367\n",
      "=================================\n",
      "in 1570 epoch, average loss: 12941.74609375\n",
      "                , loss1: 832.9296875\n",
      "                , loss2: 0.5294272899627686\n",
      "                , weight: 15.438000000002365\n",
      "=================================\n",
      "in 1580 epoch, average loss: 12764.49375\n",
      "                , loss1: 833.2841796875\n",
      "                , loss2: 1.0608270645141602\n",
      "                , weight: 15.218000000002363\n",
      "=================================\n",
      "in 1590 epoch, average loss: 12573.7328125\n",
      "                , loss1: 832.830859375\n",
      "                , loss2: 0.47644357681274413\n",
      "                , weight: 14.99800000000236\n",
      "=================================\n",
      "in 1600 epoch, average loss: 12398.19140625\n",
      "                , loss1: 833.30419921875\n",
      "                , loss2: 1.1342925071716308\n",
      "                , weight: 14.778000000002358\n",
      "=================================\n",
      "in 1610 epoch, average loss: 12232.259375\n",
      "                , loss1: 834.464453125\n",
      "                , loss2: 1.4618881225585938\n",
      "                , weight: 14.558000000002355\n",
      "=================================\n",
      "in 1620 epoch, average loss: 12027.178125\n",
      "                , loss1: 833.04169921875\n",
      "                , loss2: 0.5606296062469482\n",
      "                , weight: 14.338000000002353\n",
      "=================================\n",
      "in 1630 epoch, average loss: 11846.0890625\n",
      "                , loss1: 833.2033203125\n",
      "                , loss2: 0.4717761993408203\n",
      "                , weight: 14.11800000000235\n",
      "=================================\n",
      "in 1640 epoch, average loss: 11666.94140625\n",
      "                , loss1: 833.505078125\n",
      "                , loss2: 0.347740912437439\n",
      "                , weight: 13.898000000002348\n",
      "=================================\n",
      "in 1650 epoch, average loss: 11480.8609375\n",
      "                , loss1: 833.2806640625\n",
      "                , loss2: 0.7351916790008545\n",
      "                , weight: 13.678000000002346\n",
      "=================================\n",
      "in 1660 epoch, average loss: 11295.053125\n",
      "                , loss1: 833.1248046875\n",
      "                , loss2: 0.3899496078491211\n",
      "                , weight: 13.458000000002343\n",
      "=================================\n",
      "in 1670 epoch, average loss: 11109.49140625\n",
      "                , loss1: 832.94453125\n",
      "                , loss2: 0.5020942687988281\n",
      "                , weight: 13.23800000000234\n",
      "=================================\n",
      "in 1680 epoch, average loss: 10927.60234375\n",
      "                , loss1: 833.0466796875\n",
      "                , loss2: 0.5230025291442871\n",
      "                , weight: 13.018000000002338\n",
      "=================================\n",
      "in 1690 epoch, average loss: 10740.55078125\n",
      "                , loss1: 832.7740234375\n",
      "                , loss2: 0.2660507678985596\n",
      "                , weight: 12.798000000002336\n",
      "=================================\n",
      "in 1700 epoch, average loss: 10562.0609375\n",
      "                , loss1: 833.1236328125\n",
      "                , loss2: 0.5387379646301269\n",
      "                , weight: 12.578000000002334\n",
      "=================================\n",
      "in 1710 epoch, average loss: 10376.1125\n",
      "                , loss1: 832.880859375\n",
      "                , loss2: 0.8827966690063477\n",
      "                , weight: 12.358000000002331\n",
      "=================================\n",
      "in 1720 epoch, average loss: 10199.984375\n",
      "                , loss1: 833.515625\n",
      "                , loss2: 0.26683673858642576\n",
      "                , weight: 12.138000000002329\n",
      "=================================\n",
      "in 1730 epoch, average loss: 10006.43125\n",
      "                , loss1: 832.65390625\n",
      "                , loss2: 0.4242557525634766\n",
      "                , weight: 11.918000000002326\n",
      "=================================\n",
      "in 1740 epoch, average loss: 9819.70078125\n",
      "                , loss1: 832.38017578125\n",
      "                , loss2: 0.12544984817504884\n",
      "                , weight: 11.698000000002324\n",
      "=================================\n",
      "in 1750 epoch, average loss: 9643.5015625\n",
      "                , loss1: 832.9736328125\n",
      "                , loss2: 0.17194070816040039\n",
      "                , weight: 11.478000000002321\n",
      "=================================\n",
      "in 1760 epoch, average loss: 9459.8921875\n",
      "                , loss1: 832.926171875\n",
      "                , loss2: 0.3349787473678589\n",
      "                , weight: 11.258000000002319\n",
      "=================================\n",
      "in 1770 epoch, average loss: 9280.8421875\n",
      "                , loss1: 833.319921875\n",
      "                , loss2: 0.19048333168029785\n",
      "                , weight: 11.038000000002317\n",
      "=================================\n",
      "in 1780 epoch, average loss: 9093.64453125\n",
      "                , loss1: 832.944140625\n",
      "                , loss2: 0.4052883625030518\n",
      "                , weight: 10.818000000002314\n",
      "=================================\n",
      "in 1790 epoch, average loss: 8904.0734375\n",
      "                , loss1: 832.3765625\n",
      "                , loss2: 0.14770429134368895\n",
      "                , weight: 10.598000000002312\n",
      "=================================\n",
      "in 1800 epoch, average loss: 8727.44375\n",
      "                , loss1: 832.9513671875\n",
      "                , loss2: 0.6302581787109375\n",
      "                , weight: 10.37800000000231\n",
      "=================================\n",
      "in 1810 epoch, average loss: 8543.9984375\n",
      "                , loss1: 832.93740234375\n",
      "                , loss2: 0.5681833744049072\n",
      "                , weight: 10.158000000002307\n",
      "=================================\n",
      "in 1820 epoch, average loss: 8357.10390625\n",
      "                , loss1: 832.5583984375\n",
      "                , loss2: 0.7257737159729004\n",
      "                , weight: 9.938000000002305\n",
      "=================================\n",
      "in 1830 epoch, average loss: 8174.48125\n",
      "                , loss1: 832.65390625\n",
      "                , loss2: 0.311854887008667\n",
      "                , weight: 9.718000000002302\n",
      "=================================\n",
      "in 1840 epoch, average loss: 7997.23671875\n",
      "                , loss1: 833.2669921875\n",
      "                , loss2: 0.4139987945556641\n",
      "                , weight: 9.4980000000023\n",
      "=================================\n",
      "in 1850 epoch, average loss: 7815.27265625\n",
      "                , loss1: 833.42158203125\n",
      "                , loss2: 0.22498021125793458\n",
      "                , weight: 9.278000000002297\n",
      "=================================\n",
      "in 1860 epoch, average loss: 7623.74609375\n",
      "                , loss1: 832.5068359375\n",
      "                , loss2: 0.4960776805877686\n",
      "                , weight: 9.058000000002295\n",
      "=================================\n",
      "in 1870 epoch, average loss: 7439.21328125\n",
      "                , loss1: 832.37099609375\n",
      "                , loss2: 0.30549468994140627\n",
      "                , weight: 8.838000000002292\n",
      "=================================\n",
      "in 1880 epoch, average loss: 7255.14375\n",
      "                , loss1: 832.241796875\n",
      "                , loss2: 0.5002805233001709\n",
      "                , weight: 8.61800000000229\n",
      "=================================\n",
      "in 1890 epoch, average loss: 7074.2171875\n",
      "                , loss1: 832.5115234375\n",
      "                , loss2: 0.3726590633392334\n",
      "                , weight: 8.398000000002288\n",
      "=================================\n",
      "in 1900 epoch, average loss: 6901.3375\n",
      "                , loss1: 833.75009765625\n",
      "                , loss2: 0.4591840744018555\n",
      "                , weight: 8.178000000002285\n",
      "=================================\n",
      "in 1910 epoch, average loss: 6706.17421875\n",
      "                , loss1: 832.3109375\n",
      "                , loss2: 0.24930253028869628\n",
      "                , weight: 7.958000000002283\n",
      "=================================\n",
      "in 1920 epoch, average loss: 6530.107421875\n",
      "                , loss1: 833.213671875\n",
      "                , loss2: 0.21884765625\n",
      "                , weight: 7.73800000000228\n",
      "=================================\n",
      "in 1930 epoch, average loss: 6345.6390625\n",
      "                , loss1: 833.03125\n",
      "                , loss2: 0.3974111795425415\n",
      "                , weight: 7.518000000002278\n",
      "=================================\n",
      "in 1940 epoch, average loss: 6156.987109375\n",
      "                , loss1: 832.2865234375\n",
      "                , loss2: 0.5538076877593994\n",
      "                , weight: 7.2980000000022756\n",
      "=================================\n",
      "in 1950 epoch, average loss: 5979.39921875\n",
      "                , loss1: 833.06171875\n",
      "                , loss2: 0.5143172264099121\n",
      "                , weight: 7.078000000002273\n",
      "=================================\n",
      "in 1960 epoch, average loss: 5795.947265625\n",
      "                , loss1: 832.96357421875\n",
      "                , loss2: 0.9999122619628906\n",
      "                , weight: 6.858000000002271\n",
      "=================================\n",
      "in 1970 epoch, average loss: 5610.5828125\n",
      "                , loss1: 832.70390625\n",
      "                , loss2: 0.6594661235809326\n",
      "                , weight: 6.638000000002268\n",
      "=================================\n",
      "in 1980 epoch, average loss: 5424.693359375\n",
      "                , loss1: 832.3265625\n",
      "                , loss2: 0.41887974739074707\n",
      "                , weight: 6.418000000002266\n",
      "=================================\n",
      "in 1990 epoch, average loss: 5243.065234375\n",
      "                , loss1: 832.55244140625\n",
      "                , loss2: 0.4811945915222168\n",
      "                , weight: 6.1980000000022635\n",
      "=================================\n",
      "in 2000 epoch, average loss: 5061.1828125\n",
      "                , loss1: 832.76494140625\n",
      "                , loss2: 0.49747433662414553\n",
      "                , weight: 5.978000000002261\n",
      "=================================\n",
      "in 2010 epoch, average loss: 4884.05625\n",
      "                , loss1: 833.769921875\n",
      "                , loss2: 0.606801176071167\n",
      "                , weight: 5.758000000002259\n",
      "=================================\n",
      "in 2020 epoch, average loss: 4692.161328125\n",
      "                , loss1: 832.34521484375\n",
      "                , loss2: 0.20929512977600098\n",
      "                , weight: 5.538000000002256\n",
      "=================================\n",
      "in 2030 epoch, average loss: 4519.048046875\n",
      "                , loss1: 834.166796875\n",
      "                , loss2: 0.2470644474029541\n",
      "                , weight: 5.318000000002254\n",
      "=================================\n",
      "in 2040 epoch, average loss: 4326.866015625\n",
      "                , loss1: 832.4916015625\n",
      "                , loss2: 0.40235176086425783\n",
      "                , weight: 5.098000000002251\n",
      "=================================\n",
      "in 2050 epoch, average loss: 4142.687109375\n",
      "                , loss1: 832.220703125\n",
      "                , loss2: 0.722345495223999\n",
      "                , weight: 4.878000000002249\n",
      "=================================\n",
      "in 2060 epoch, average loss: 3961.461328125\n",
      "                , loss1: 832.709765625\n",
      "                , loss2: 0.24985089302062988\n",
      "                , weight: 4.658000000002247\n",
      "=================================\n",
      "in 2070 epoch, average loss: 3777.455859375\n",
      "                , loss1: 832.544140625\n",
      "                , loss2: 0.20289857387542726\n",
      "                , weight: 4.438000000002244\n",
      "=================================\n",
      "in 2080 epoch, average loss: 3596.546875\n",
      "                , loss1: 832.9033203125\n",
      "                , loss2: 0.9505505561828613\n",
      "                , weight: 4.218000000002242\n",
      "=================================\n",
      "in 2090 epoch, average loss: 3410.194921875\n",
      "                , loss1: 832.269140625\n",
      "                , loss2: 0.4014455795288086\n",
      "                , weight: 3.9980000000022398\n",
      "=================================\n",
      "in 2100 epoch, average loss: 3227.223828125\n",
      "                , loss1: 832.2064453125\n",
      "                , loss2: 0.7678424835205078\n",
      "                , weight: 3.778000000002242\n",
      "=================================\n",
      "in 2110 epoch, average loss: 3045.0240234375\n",
      "                , loss1: 832.3986328125\n",
      "                , loss2: 0.9611653327941895\n",
      "                , weight: 3.558000000002244\n",
      "=================================\n",
      "in 2120 epoch, average loss: 2860.741796875\n",
      "                , loss1: 832.249609375\n",
      "                , loss2: 0.29573640823364256\n",
      "                , weight: 3.338000000002246\n",
      "=================================\n",
      "in 2130 epoch, average loss: 2678.170703125\n",
      "                , loss1: 832.39580078125\n",
      "                , loss2: 0.3762198925018311\n",
      "                , weight: 3.118000000002248\n",
      "=================================\n",
      "in 2140 epoch, average loss: 2495.01484375\n",
      "                , loss1: 832.44140625\n",
      "                , loss2: 0.16947629451751708\n",
      "                , weight: 2.89800000000225\n",
      "=================================\n",
      "in 2150 epoch, average loss: 2311.7814453125\n",
      "                , loss1: 832.453125\n",
      "                , loss2: 0.060277366638183595\n",
      "                , weight: 2.678000000002252\n",
      "=================================\n",
      "in 2160 epoch, average loss: 2127.4078125\n",
      "                , loss1: 831.98271484375\n",
      "                , loss2: 0.0457491010427475\n",
      "                , weight: 2.458000000002254\n",
      "=================================\n",
      "in 2170 epoch, average loss: 1945.58984375\n",
      "                , loss1: 832.4712890625\n",
      "                , loss2: 0.07289665937423706\n",
      "                , weight: 2.238000000002256\n",
      "=================================\n",
      "in 2180 epoch, average loss: 1764.5033203125\n",
      "                , loss1: 833.3560546875\n",
      "                , loss2: 0.36719489097595215\n",
      "                , weight: 2.018000000002258\n",
      "=================================\n",
      "in 2190 epoch, average loss: 1579.276171875\n",
      "                , loss1: 832.376953125\n",
      "                , loss2: 0.24519522190093995\n",
      "                , weight: 1.7980000000022578\n",
      "=================================\n",
      "in 2200 epoch, average loss: 1396.11044921875\n",
      "                , loss1: 832.3041015625\n",
      "                , loss2: 0.3499589920043945\n",
      "                , weight: 1.5780000000022576\n",
      "=================================\n",
      "in 2210 epoch, average loss: 1213.11884765625\n",
      "                , loss1: 832.5669921875\n",
      "                , loss2: 0.08027515411376954\n",
      "                , weight: 1.3580000000022574\n",
      "=================================\n",
      "in 2220 epoch, average loss: 1029.31025390625\n",
      "                , loss1: 831.92685546875\n",
      "                , loss2: 0.21146597862243652\n",
      "                , weight: 1.1380000000022572\n",
      "=================================\n",
      "in 2230 epoch, average loss: 846.81044921875\n",
      "                , loss1: 832.46103515625\n",
      "                , loss2: 0.21961376667022706\n",
      "                , weight: 0.918000000002257\n",
      "=================================\n",
      "in 2240 epoch, average loss: 663.914208984375\n",
      "                , loss1: 832.7130859375\n",
      "                , loss2: 0.22770791053771972\n",
      "                , weight: 0.6980000000022568\n",
      "=================================\n",
      "in 2250 epoch, average loss: 480.492236328125\n",
      "                , loss1: 832.3361328125\n",
      "                , loss2: 0.22408661842346192\n",
      "                , weight: 0.4780000000022566\n",
      "=================================\n",
      "in 2260 epoch, average loss: 300.7700927734375\n",
      "                , loss1: 832.5478515625\n",
      "                , loss2: 0.060448300838470456\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2270 epoch, average loss: 249.82470703125\n",
      "                , loss1: 832.202734375\n",
      "                , loss2: 0.16389296054840088\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2280 epoch, average loss: 249.7813720703125\n",
      "                , loss1: 832.19140625\n",
      "                , loss2: 0.12396647930145263\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2290 epoch, average loss: 250.114697265625\n",
      "                , loss1: 832.46513671875\n",
      "                , loss2: 0.37512423992156985\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2300 epoch, average loss: 250.193798828125\n",
      "                , loss1: 832.8669921875\n",
      "                , loss2: 0.3337059736251831\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2310 epoch, average loss: 249.7552978515625\n",
      "                , loss1: 832.236328125\n",
      "                , loss2: 0.08437082767486573\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2320 epoch, average loss: 249.7859619140625\n",
      "                , loss1: 832.50625\n",
      "                , loss2: 0.034063109755516054\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2330 epoch, average loss: 249.8535888671875\n",
      "                , loss1: 832.6099609375\n",
      "                , loss2: 0.0705910563468933\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2340 epoch, average loss: 250.38974609375\n",
      "                , loss1: 833.6818359375\n",
      "                , loss2: 0.2852036952972412\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2350 epoch, average loss: 249.892236328125\n",
      "                , loss1: 832.4984375\n",
      "                , loss2: 0.14268308877944946\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2360 epoch, average loss: 249.75634765625\n",
      "                , loss1: 832.3306640625\n",
      "                , loss2: 0.0571378231048584\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2370 epoch, average loss: 249.784326171875\n",
      "                , loss1: 832.358203125\n",
      "                , loss2: 0.07685102224349975\n",
      "                , weight: 0.3\n",
      "=================================\n",
      "in 2380 epoch, average loss: 249.8400390625\n",
      "                , loss1: 832.3396484375\n",
      "                , loss2: 0.1381195902824402\n",
      "                , weight: 0.3\n",
      "=================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m<\u001b[39m limit:\n\u001b[1;32m      8\u001b[0m     hgnn_trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m----> 9\u001b[0m loss,loss_1,loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mhgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m temp_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     11\u001b[0m temp_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_1\n",
      "Cell \u001b[0;32mIn[92], line 32\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX)\n\u001b[1;32m     31\u001b[0m loss, loss_1, loss_2 \u001b[38;5;241m=\u001b[39m loss_bs_matrix(outs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhg, device\u001b[38;5;241m=\u001b[39mDEVICE,weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), loss_1\u001b[38;5;241m.\u001b[39mitem(), loss_2\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/partition/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)\n",
    "optim1 = optim.Adam(hgnn_trainer.parameters(), lr=lr, weight_decay=5e-8)\n",
    "hgnn_trainer.optimizer = optim1\n",
    "for epoch in range(40000):\n",
    "    if hgnn_trainer.weight > limit:\n",
    "        hgnn_trainer.weight = hgnn_trainer.weight - sub\n",
    "    elif hgnn_trainer.weight < limit:\n",
    "        hgnn_trainer.weight = limit\n",
    "    loss,loss_1,loss_2 = hgnn_trainer.run(epoch=epoch)\n",
    "    temp_loss_total += loss\n",
    "    temp_loss1 += loss_1\n",
    "    temp_loss2 += loss_2\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"in {epoch} epoch, average loss: {temp_loss_total.item() / 10}\")\n",
    "        print(f\"                , loss1: {temp_loss1.item() / 10}\")\n",
    "        print(f\"                , loss2: {temp_loss2.item() / 10}\")\n",
    "        print(f\"                , weight: {hgnn_trainer.weight}\")\n",
    "        print(f\"=================================\")\n",
    "        sys.stdout.flush()\n",
    "        temp_loss_total,temp_loss1,temp_loss2 = torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False),torch.zeros(1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgnn_trainer.eval()\n",
    "outs = hgnn_trainer.forward(hgnn_trainer.X)\n",
    "outs_straight = StraightThroughEstimator.apply(outs)\n",
    "G_clone = G.clone()\n",
    "edges, _  = G_clone.e\n",
    "cut = 0\n",
    "for vertices in edges:\n",
    "    if torch.prod(outs_straight[list(vertices)], dim=0).sum() == 0:\n",
    "        cut += 1\n",
    "    else:\n",
    "        G_clone.remove_hyperedges(vertices)\n",
    "assert cut == G_clone.num_e\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1912., 1912.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = outs_straight.sum(dim=0)\n",
    "print(num_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
